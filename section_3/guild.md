Data Engineer Learning Path Wiki

üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢: ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ Data Engineering best practices ‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á production-ready ETL pipeline ‡∏î‡πâ‡∏ß‡∏¢ Apache Airflow


üìö ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

Phase 1: Environment Setup
Phase 2: Dataset Selection
Phase 3: Data Pipeline Architecture
Phase 4: Task Implementation Deep Dive
Phase 5: Production Features
Phase 6: Testing & Deployment


Phase 1: Environment Setup
üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏ö‡∏ô local machine ‡∏î‡πâ‡∏ß‡∏¢ Docker ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô Airflow ‡πÅ‡∏•‡∏∞ PostgreSQL ‡πÑ‡∏î‡πâ
‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤: 1-2 ‡∏ß‡∏±‡∏ô
üõ†Ô∏è ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á

Docker Desktop (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö MacOS)

‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏à‡∏≤‡∏Å: https://www.docker.com/products/docker-desktop
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á: docker --version ‡πÅ‡∏•‡∏∞ docker-compose --version


Git (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö version control)

‡∏°‡∏±‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏ô MacOS
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: git --version


Python 3.10+ (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö code local)

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö: python3 --version




üìÅ Project Structure
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
data-engineering-project/
‚îú‚îÄ‚îÄ docker-compose.yml          # ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö orchestrate services
‚îú‚îÄ‚îÄ Dockerfile                  # Custom image (‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô)
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # Documentation
‚îú‚îÄ‚îÄ .env.example               # Environment variables template
‚îú‚îÄ‚îÄ .gitignore                 # ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á commit
‚îÇ
‚îú‚îÄ‚îÄ dags/                      # Airflow DAG files
‚îÇ   ‚îú‚îÄ‚îÄ marketing_etl_dag.py  # Main DAG
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dag_config.yaml   # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ tasks/                # Task functions (modular)
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ ingestion.py
‚îÇ       ‚îú‚îÄ‚îÄ profiling.py
‚îÇ       ‚îú‚îÄ‚îÄ cleaning.py
‚îÇ       ‚îú‚îÄ‚îÄ validation.py
‚îÇ       ‚îî‚îÄ‚îÄ loading.py
‚îÇ
‚îú‚îÄ‚îÄ plugins/                   # Airflow plugins
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ operators/            # Custom operators
‚îÇ   ‚îî‚îÄ‚îÄ hooks/                # Custom hooks
‚îÇ
‚îú‚îÄ‚îÄ tests/                     # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sample.csv
‚îÇ   ‚îú‚îÄ‚îÄ test_dag_validation.py
‚îÇ   ‚îú‚îÄ‚îÄ test_ingestion.py
‚îÇ   ‚îî‚îÄ‚îÄ test_cleaning.py
‚îÇ
‚îú‚îÄ‚îÄ data/                      # Data directories
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Source CSV files
‚îÇ   ‚îú‚îÄ‚îÄ processed/            # Cleaned Parquet files
‚îÇ   ‚îú‚îÄ‚îÄ reports/              # Profiling reports
‚îÇ   ‚îî‚îÄ‚îÄ archive/              # Archived files
‚îÇ
‚îú‚îÄ‚îÄ logs/                      # Airflow logs (auto-generated)
‚îÇ
‚îú‚îÄ‚îÄ scripts/                   # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup_database.sql
‚îÇ   ‚îî‚îÄ‚îÄ create_tables.sql
‚îÇ
‚îî‚îÄ‚îÄ docs/                      # Documentation
    ‚îú‚îÄ‚îÄ ARCHITECTURE.md
    ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md

üê≥ Docker Compose Setup
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå docker-compose.yml:
yamlversion: '3.8'

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.8.1
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./data:/opt/airflow/data
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:14
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    ports:
      - "5432:5432"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID:-50000}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version

volumes:
  postgres-db-volume:

üìù Requirements.txt
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå requirements.txt:
txt# Core Airflow
apache-airflow==2.8.1

# Data Processing
pandas==2.1.4
numpy==1.24.3

# Data Profiling
ydata-profiling==4.18.0

# Database
sqlalchemy==2.0.23
psycopg2-binary==2.9.9

# File Formats
pyarrow==14.0.1

# Testing
pytest==7.4.3
pytest-cov==4.1.0

üöÄ Setup Instructions
Step 1: Clone/Create Project
bashmkdir data-engineering-project
cd data-engineering-project
Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ï‡∏≤‡∏° structure ‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô
Step 3: ‡πÄ‡∏£‡∏¥‡πà‡∏° Services
bash# Set Airflow UID (MacOS)
echo -e "AIRFLOW_UID=$(id -u)" > .env

# Start services
docker-compose up -d
Step 4: Initialize Airflow Database
bashdocker-compose run airflow-init
Step 5: Create Admin User
bashdocker-compose exec airflow-webserver airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
Step 6: Verify Installation

Airflow UI: http://localhost:8080 (login: admin/admin)
PostgreSQL: localhost:5432 (user: airflow/airflow)


‚úÖ Health Check
‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ services ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥:
bash# ‡∏î‡∏π status ‡∏Ç‡∏≠‡∏á containers
docker-compose ps

# ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏´‡πá‡∏ô services ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ running:
# - postgres
# - airflow-webserver
# - airflow-scheduler

# ‡∏î‡∏π logs
docker-compose logs -f airflow-webserver

üîß Troubleshooting
‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Permission denied
bash# Fix permissions
sudo chown -R $USER:$USER .
‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Port 8080 ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß
bash# ‡∏´‡∏≤ process ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ port
lsof -i :8080
# ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô port ‡πÉ‡∏ô docker-compose.yml
‡∏õ‡∏±‡∏ç‡∏´‡∏≤: Container ‡πÑ‡∏°‡πà healthy
bash# Check logs
docker-compose logs postgres
docker-compose logs airflow-scheduler

Phase 2: Dataset Selection
üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dataset ‡∏à‡∏≤‡∏Å Kaggle ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ù‡∏∂‡∏Å ETL pipeline
‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤: 2-3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

üìä Dataset ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥
ü•á Option 1: E-commerce Customer Behavior
‡∏ä‡∏∑‡πà‡∏≠ Dataset: Online Retail Dataset
Link: https://www.kaggle.com/datasets/lakshmi25npathi/online-retail-dataset
‡∏Ç‡∏ô‡∏≤‡∏î: ~540,000 rows, 8 columns
Columns ‡∏´‡∏•‡∏±‡∏Å:

InvoiceNo: ‡∏£‡∏´‡∏±‡∏™‡πÉ‡∏ö‡πÅ‡∏à‡πâ‡∏á‡∏´‡∏ô‡∏µ‡πâ
StockCode: ‡∏£‡∏´‡∏±‡∏™‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
Description: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏¥‡∏ô‡∏Ñ‡πâ‡∏≤
Quantity: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô
InvoiceDate: ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡∏∑‡πâ‡∏≠
UnitPrice: ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡πà‡∏≠‡∏´‡∏ô‡πà‡∏ß‡∏¢
CustomerID: ‡∏£‡∏´‡∏±‡∏™‡∏•‡∏π‡∏Å‡∏Ñ‡πâ‡∏≤
Country: ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®

‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:

‚úÖ Missing values ‡πÉ‡∏ô CustomerID (~25%)
‚úÖ Missing values ‡πÉ‡∏ô Description (~0.3%)
‚úÖ Negative Quantity (returns)
‚úÖ Zero UnitPrice
‚úÖ Date format ‡∏ï‡πâ‡∏≠‡∏á parse
‚úÖ Duplicate transactions

‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:

‡∏ù‡∏∂‡∏Å handle missing values
‡∏ù‡∏∂‡∏Å business logic (returns, refunds)
‡∏ù‡∏∂‡∏Å date manipulation
‡∏ù‡∏∂‡∏Å aggregation (RFM analysis)


ü•à Option 2: Marketing Campaign Data
‡∏ä‡∏∑‡πà‡∏≠ Dataset: Bank Marketing Dataset
Link: https://www.kaggle.com/datasets/henriqueyamahata/bank-marketing
‡∏Ç‡∏ô‡∏≤‡∏î: ~45,000 rows, 17 columns
Columns ‡∏´‡∏•‡∏±‡∏Å:

age: ‡∏≠‡∏≤‡∏¢‡∏∏
job: ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
marital: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏™‡∏°‡∏£‡∏™
education: ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤
balance: ‡∏¢‡∏≠‡∏î‡πÄ‡∏á‡∏¥‡∏ô‡πÉ‡∏ô‡∏ö‡∏±‡∏ç‡∏ä‡∏µ
campaign: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠
poutcome: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÅ‡∏Ñ‡∏°‡πÄ‡∏õ‡∏ç‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
y: ‡∏™‡∏°‡∏±‡∏Ñ‡∏£ term deposit ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà

‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏ö:

‚úÖ Categorical data ‡∏ï‡πâ‡∏≠‡∏á encode
‚úÖ Outliers ‡πÉ‡∏ô balance, campaign
‚úÖ Imbalanced target (y)
‚úÖ Missing values ‡πÅ‡∏ù‡∏á‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô "unknown"
‚úÖ Duplicate records

‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:

‡∏ù‡∏∂‡∏Å categorical encoding
‡∏ù‡∏∂‡∏Å outlier detection
‡∏ù‡∏∂‡∏Å data validation rules
‡∏ù‡∏∂‡∏Å feature engineering


ü•â Option 3: Sales Transaction Data
‡∏ä‡∏∑‡πà‡∏≠ Dataset: Supermarket Sales
Link: https://www.kaggle.com/datasets/aungpyaeap/supermarket-sales
‡∏Ç‡∏ô‡∏≤‡∏î: ~1,000 rows, 17 columns
‚ö†Ô∏è ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Dataset ‡∏ô‡∏µ‡πâ‡πÄ‡∏•‡πá‡∏Å‡πÑ‡∏õ ‡πÅ‡∏ï‡πà clean ‡∏°‡∏≤‡∏Å ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô

üì• ‡∏ß‡∏¥‡∏ò‡∏µ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î Dataset
Option 1: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡∏ú‡πà‡∏≤‡∏ô Kaggle UI

‡πÑ‡∏õ‡∏ó‡∏µ‡πà link dataset
‡∏Å‡∏î "Download" (‡∏ï‡πâ‡∏≠‡∏á login Kaggle)
‡πÅ‡∏ï‡∏Å‡πÑ‡∏ü‡∏•‡πå zip
‡∏ß‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡πÉ‡∏ô data/raw/

Option 2: ‡πÉ‡∏ä‡πâ Kaggle API
bash# Install Kaggle CLI
pip install kaggle

# Setup API token (‡πÑ‡∏õ‡πÄ‡∏≠‡∏≤‡∏ó‡∏µ‡πà Kaggle Account Settings)
mkdir -p ~/.kaggle
mv kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# Download dataset
kaggle datasets download -d lakshmi25npathi/online-retail-dataset
unzip online-retail-dataset.zip -d data/raw/

üîç Data Exploration (‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)
‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå notebooks/explore.ipynb (optional):
pythonimport pandas as pd

# Load data
df = pd.read_csv('data/raw/online_retail.csv', encoding='ISO-8859-1')

# Basic info
print(f"Shape: {df.shape}")
print(f"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# Missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Duplicates
print(f"\nDuplicates: {df.duplicated().sum()}")

# Data types
print("\nData Types:")
print(df.dtypes)

# Sample rows
df.head()
```

---

## Phase 3: Data Pipeline Architecture

### üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞ flow ‡∏Ç‡∏≠‡∏á ETL pipeline ‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô code

### ‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤: 3-4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

---

### üìê Pipeline Overview
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Raw CSV    ‚îÇ
‚îÇ  (Source)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Task 1: Ingestion  ‚îÇ  ‚Üê ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå, validate, save as Parquet
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Task 2: Profiling  ‚îÇ  ‚Üê ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, generate report
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Task 3: Cleaning   ‚îÇ  ‚Üê ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ missing, duplicates, outliers
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Task 4: Validation ‚îÇ  ‚Üê ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö business rules, data quality
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Task 5: Loading    ‚îÇ  ‚Üê Load ‡πÄ‡∏Ç‡πâ‡∏≤ PostgreSQL + save Parquet
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇPostgreSQL‚îÇ  (Analytics Database)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Parquet  ‚îÇ  (Data Lake)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

üîÑ Task Dependencies
Sequential Flow (‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á):
pythoningestion >> profiling >> cleaning >> validation >> loading
‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•:

Ingestion ‡∏Å‡πà‡∏≠‡∏ô: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞ process ‡πÑ‡∏î‡πâ
Profiling ‡∏´‡∏•‡∏±‡∏á Ingestion: ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏≠‡∏∞‡πÑ‡∏£
Cleaning ‡∏´‡∏•‡∏±‡∏á Profiling: ‡∏£‡∏π‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡πà‡∏≠‡∏¢‡πÅ‡∏Å‡πâ
Validation ‡∏´‡∏•‡∏±‡∏á Cleaning: ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤ clean ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
Loading ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢: ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà clean ‡πÅ‡∏•‡πâ‡∏ß


üóÇÔ∏è Data Flow & Storage
Intermediate Files:
StageFormatLocationPurposeRawCSVdata/raw/Original sourcePost-IngestionParquetdata/processed/ingested.parquetFaster I/OPost-ProfilingHTML + JSONdata/reports/profile.htmlAnalysis reportPost-CleaningParquetdata/processed/cleaned.parquetClean dataPost-ValidationJSONdata/reports/validation.jsonValidation resultsFinalPostgreSQLmarketing_data tableAnalytics readyFinalParquetdata/processed/final/Data lake

üéØ ‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ Parquet ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà CSV?
FeatureCSVParquetRead Speedüêå Slow‚ö° Fast (10-100x)File Sizeüì¶ Largeüì¶ Small (compression)Schema‚ùå No‚úÖ Yes (typed columns)Partial Read‚ùå Read all‚úÖ Read columns neededCompression‚ùå Limited‚úÖ Built-in (snappy, gzip)
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:

CSV: 100 MB ‚Üí ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå 3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
Parquet: 20 MB ‚Üí ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏Ñ‡πà columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ 0.3 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ


üìä Data Schema Design
Target PostgreSQL Table:
sqlCREATE TABLE marketing_data (
    -- Primary Key
    id SERIAL PRIMARY KEY,
    
    -- Customer Information
    customer_id VARCHAR(50),
    age INTEGER,
    gender VARCHAR(10),
    
    -- Transaction
    invoice_no VARCHAR(50),
    invoice_date TIMESTAMP,
    product_code VARCHAR(50),
    product_name VARCHAR(255),
    quantity INTEGER,
    unit_price DECIMAL(10, 2),
    total_amount DECIMAL(10, 2),
    
    -- Location
    country VARCHAR(100),
    
    -- Data Quality Flags
    has_missing_customer BOOLEAN,
    is_return BOOLEAN,
    is_outlier BOOLEAN,
    
    -- Metadata
    loaded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    source_file VARCHAR(255)
);

-- Indexes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query performance
CREATE INDEX idx_customer_id ON marketing_data(customer_id);
CREATE INDEX idx_invoice_date ON marketing_data(invoice_date);
CREATE INDEX idx_country ON marketing_data(country);

üîê Configuration Management
‡πÑ‡∏ü‡∏•‡πå .env.example:
env# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=false

# PostgreSQL
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_HOST=postgres
POSTGRES_PORT=5432

# Paths
DATA_RAW_PATH=/opt/airflow/data/raw
DATA_PROCESSED_PATH=/opt/airflow/data/processed
DATA_REPORTS_PATH=/opt/airflow/data/reports

# Processing
CHUNK_SIZE=10000
PROFILING_MODE=minimal
ENABLE_PROFILING=true
‡πÑ‡∏ü‡∏•‡πå dags/config/dag_config.yaml:
yaml# DAG Configuration
dag:
  schedule_interval: "0 2 * * *"  # Daily at 2 AM
  max_active_runs: 1
  catchup: false
  
# Data Sources
data:
  source_file: "online_retail.csv"
  encoding: "ISO-8859-1"
  date_columns:
    - InvoiceDate
  
# Data Quality Thresholds
quality:
  max_missing_percent: 30.0
  max_duplicate_percent: 5.0
  outlier_method: "iqr"
  outlier_threshold: 1.5

# Cleaning Rules
cleaning:
  fill_methods:
    CustomerID: "flag"  # Flag as missing
    Description: "Unknown"
  drop_columns:
    - unnecessary_col
  
# Validation Rules
validation:
  required_columns:
    - InvoiceNo
    - StockCode
    - Quantity
    - InvoiceDate
  ranges:
    Quantity:
      min: -10000
      max: 10000
    UnitPrice:
      min: 0
      max: 100000

üìù DAG File Structure (Preview)
python# dags/marketing_etl_dag.py

from airflow import DAG
from airflow.decorators import task
from datetime import datetime, timedelta
import yaml

# Load configuration
with open('/opt/airflow/dags/config/dag_config.yaml', 'r') as f:
    config = yaml.safe_load(f)

# Default arguments
default_args = {
    'owner': 'data-engineering',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# DAG definition
with DAG(
    dag_id='marketing_etl_pipeline',
    default_args=default_args,
    description='Marketing Data ETL Pipeline',
    schedule_interval=config['dag']['schedule_interval'],
    start_date=datetime(2024, 1, 1),
    catchup=config['dag']['catchup'],
    tags=['marketing', 'etl', 'production'],
) as dag:
    
    @task
    def ingest_raw_data():
        # Implementation ‡πÉ‡∏ô Phase 4
        pass
    
    @task
    def profile_data():
        pass
    
    @task
    def clean_data():
        pass
    
    @task
    def validate_data():
        pass
    
    @task
    def load_data():
        pass
    
    # Dependencies
    ingestion = ingest_raw_data()
    profiling = profile_data()
    cleaning = clean_data()
    validation = validate_data()
    loading = load_data()
    
    ingestion >> profiling >> cleaning >> validation >> loading
```

---

## Phase 4: Task Implementation Deep Dive

> üéØ **‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏´‡∏±‡∏ß‡πÉ‡∏à‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ** - ‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ó‡∏∏‡∏Å parameter, ‡∏ó‡∏∏‡∏Å function, ‡∏ó‡∏∏‡∏Å‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î code ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

### ‚è±Ô∏è ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤: 10-12 ‡∏ß‡∏±‡∏ô

---

## üì• Task 1: Data Ingestion

### üéØ ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å
```
=============================================================================
TASK 1: DATA INGESTION - ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å CSV ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö
=============================================================================
‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:

‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV ‡∏à‡∏≤‡∏Å source directory
Validate ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå (exists, readable, format)
Load ‡πÄ‡∏Ç‡πâ‡∏≤ Pandas DataFrame ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ encoding
‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Parquet format (optimized ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö processing)
Return metadata ‡∏ú‡πà‡∏≤‡∏ô XCom ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tasks ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

üìä Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

Validate file path existence
Check file permissions
Read CSV with proper encoding
Validate basic schema (columns exist)
Save as Parquet (compressed)
Log statistics (rows, memory, file size)
Return metadata to XCom

‚ö†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:

File encoding issues (UTF-8 vs ISO-8859-1 vs Windows-1252)
Large files ‚Üí memory overflow
File path security (path traversal attacks)
Concurrent file access
Disk space limitations


üìù Complete Implementation
python@task(
    # task_id: ‡∏ä‡∏∑‡πà‡∏≠ unique identifier ‡∏Ç‡∏≠‡∏á task ‡πÉ‡∏ô DAG
    # - ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô string ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÉ‡∏ô DAG
    # - Naming convention: snake_case, descriptive
    # - ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ special characters ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å underscore
    # - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: 3-50 characters
    # - Best practice: ‡πÉ‡∏ä‡πâ verb + noun (e.g., ingest_raw_data, load_customer_data)
    task_id='ingest_raw_data',
    
    # retries: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏£‡∏±‡∏ô task ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡πâ‡∏≤ fail
    # - Type: int
    # - Default: 0 (‡πÑ‡∏°‡πà retry)
    # - Range: 0-10 (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
    # - Use case: 
    #   * File I/O operations: 2-3 retries (transient errors)
    #   * API calls: 3-5 retries (network issues)
    #   * Database operations: 2-3 retries (connection issues)
    # - Best practice: retry ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö transient errors ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    #   ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô logic error (‡πÄ‡∏ä‡πà‡∏ô bad data) ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£ retry
    retries=3,
    
    # retry_delay: ‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡∏Å‡πà‡∏≠‡∏ô retry ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    # - Type: timedelta object
    # - Default: timedelta(seconds=300) = 5 minutes
    # - Best practice:
    #   * Short tasks: timedelta(seconds=30-60)
    #   * Medium tasks: timedelta(minutes=2-5)
    #   * Long tasks: timedelta(minutes=5-15)
    # - ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏∞‡∏ö‡∏ö recover ‡∏à‡∏≤‡∏Å transient issues
    # - Trade-off: delay ‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí pipeline slow, ‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí waste resources
    retry_delay=timedelta(minutes=2),
    
    # execution_timeout: ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà task ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ
    # - Type: timedelta object
    # - Default: None (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)
    # - Use case: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô hanging tasks
    # - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:
    #   * File size 100MB ‚Üí ~30-60 seconds
    #   * File size 1GB ‚Üí ~5-10 minutes
    #   * Add buffer 2-3x ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö slow disk/network
    # - Best practice: ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ average runtime 3-5 ‡πÄ‡∏ó‡πà‡∏≤
    # - ‡∏ñ‡πâ‡∏≤ timeout ‚Üí task fail ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ (‡πÑ‡∏°‡πà retry automatic)
    execution_timeout=timedelta(minutes=10),
    
    # pool: resource pool ‡∏ó‡∏µ‡πà task ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ
    # - Type: string
    # - Default: 'default_pool'
    # - Purpose: ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô tasks ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
    # - Use case:
    #   * 'cpu_intensive': ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tasks ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ CPU ‡∏™‡∏π‡∏á
    #   * 'io_intensive': ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö file/network operations
    #   * 'memory_intensive': ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö tasks ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ RAM ‡πÄ‡∏¢‡∏≠‡∏∞
    # - Best practice: ‡πÅ‡∏¢‡∏Å pool ‡∏ï‡∏≤‡∏° resource type
    # - Configuration: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô Airflow UI ‚Üí Admin ‚Üí Pools
    pool='io_intensive',
    
    # pool_slots: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô slots ‡∏ó‡∏µ‡πà task ‡∏ô‡∏µ‡πâ‡πÉ‡∏ä‡πâ‡∏à‡∏≤‡∏Å pool
    # - Type: int
    # - Default: 1
    # - Range: 1-100
    # - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢: ‡∏ñ‡πâ‡∏≤ pool ‡∏°‡∏µ 10 slots, task ‡πÉ‡∏ä‡πâ 2 slots ‚Üí ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ 5 tasks ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
    # - Use case:
    #   * Small file (< 100MB): 1 slot
    #   * Medium file (100MB-1GB): 2 slots
    #   * Large file (> 1GB): 3-5 slots
    # - Best practice: ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏à‡∏≤‡∏Å resource usage
    # - Trade-off: ‡πÉ‡∏ä‡πâ slots ‡πÄ‡∏¢‡∏≠‡∏∞ ‚Üí tasks ‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏≠‡∏ô‡∏≤‡∏ô
    pool_slots=2,
    
    # retry_exponential_backoff: ‡πÄ‡∏û‡∏¥‡πà‡∏° delay ‡πÅ‡∏ö‡∏ö exponential ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö retry
    # - Type: bool
    # - Default: False
    # - Algorithm: delay = retry_delay * (2 ** retry_number)
    # - Example: retry_delay=60s ‚Üí 60s, 120s, 240s, 480s
    # - Use case: API rate limits, database connection pools
    # - Best practice: ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏î load ‡∏ö‡∏ô external systems
    retry_exponential_backoff=True,
# max_retry_delay: delay ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á retries
# - Type: timedelta object
# - Default: None (‡πÑ‡∏°‡πà‡∏à‡∏≥‡∏Å‡∏±‡∏î)
# - Use case: ‡∏à‡∏≥‡∏Å‡∏±‡∏î exponential backoff ‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
# - Example: retry_delay=1min, exponential=True
#   ‚Üí 1min, 2min, 4min, 8min, 16min ‡∏à‡∏∞ cap ‡∏ó‡∏µ‡πà max_retry_delay
max_retry_delay=timedelta(minutes=15),
)
def ingest_raw_data(
file_path: str = '/opt/airflow/data/raw/online_retail.csv',
# file_path: ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡πà‡∏≤‡∏ô
# - Type: str (absolute path)
# - Format: '/path/to/file.csv' (Unix-style path)
# - Default: path ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô configuration
# - Constraints:
#   * ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô absolute path (‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ '/')
#   * Extension ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô .csv
#   * File size < 10GB (memory limit)
# - Security: ‡∏ï‡πâ‡∏≠‡∏á validate ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô path traversal (../)
# - Best practice: ‡πÄ‡∏Å‡πá‡∏ö path ‡πÉ‡∏ô Airflow Variable ‡∏´‡∏£‡∏∑‡∏≠ config file
encoding: str = 'ISO-8859-1',  
# encoding: character encoding ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå CSV
# - Type: str
# - Common options:
#   * 'utf-8': Standard encoding (most common)
#   * 'ISO-8859-1' (Latin-1): European characters
#   * 'Windows-1252': Windows default
#   * 'utf-8-sig': UTF-8 with BOM
# - Default: 'ISO-8859-1' (for this dataset)
# - ‡∏ó‡∏î‡∏™‡∏≠‡∏ö encoding:
#   ```python
#   import chardet
#   with open(file_path, 'rb') as f:
#       result = chardet.detect(f.read(100000))
#       print(result['encoding'])
#   ```
# - Error handling: ‡∏ñ‡πâ‡∏≤ encoding ‡∏ú‡∏¥‡∏î ‚Üí UnicodeDecodeError
# - Best practice: ‡∏£‡∏∞‡∏ö‡∏∏ encoding ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏ä‡πâ default

chunk_size: int = None,  
# chunk_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà)
# - Type: int or None
# - Default: None (‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå)
# - Range: 1000-100000 rows per chunk
# - Use case: ‡πÑ‡∏ü‡∏•‡πå > 1GB ‚Üí ‡πÉ‡∏ä‡πâ chunking
# - Memory calculation:
#   * 10,000 rows √ó 20 columns √ó 8 bytes ‚âà 1.6 MB
#   * 100,000 rows √ó 20 columns √ó 8 bytes ‚âà 16 MB
# - Best practice:
#   * Available RAM / 4 = safe chunk size
#   * Example: 8GB RAM ‚Üí 2GB chunks ‚Üí ~250,000 rows
# - Trade-off: chunk ‡πÄ‡∏•‡πá‡∏Å ‚Üí I/O overhead, chunk ‡πÉ‡∏´‡∏ç‡πà ‚Üí memory spike

output_format: str = 'parquet',  
# output_format: ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
# - Type: str
# - Options: 'parquet', 'csv', 'feather'
# - Default: 'parquet'
# - Comparison:
#   * Parquet: Columnar, compressed, schema-aware (recommended)
#   * CSV: Human-readable, no compression, no schema
#   * Feather: Fast, binary, but less compression
# - Performance:
#   * Write: CSV (slowest) < Parquet < Feather (fastest)
#   * Read: CSV (slowest) < Parquet < Feather (fastest)
#   * Size: CSV (largest) > Feather > Parquet (smallest)
# - Best practice: Parquet ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö data pipelines
) -> dict:
# Return type: dictionary containing metadata
# - Structure:
#   {
#       'status': str,           # 'success' or 'failed'
#       'file_path': str,        # Input file path
#       'output_path': str,      # Output Parquet path
#       'rows': int,             # Number of rows
#       'columns': int,          # Number of columns
#       'memory_mb': float,      # Memory usage in MB
#       'file_size_mb': float,   # Original file size
#       'compression_ratio': float,  # Original/Compressed size
#       'duration_seconds': float,   # Processing time
#       'timestamp': str         # ISO format timestamp
#   }
# - Purpose: ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ‡∏¢‡∏±‡∏á tasks ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏ú‡πà‡∏≤‡∏ô XCom
# - XCom size limits:
#   * SQLite (dev): 2GB (‡πÅ‡∏ï‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ < 1MB)
#   * PostgreSQL: 1GB (‡πÅ‡∏ï‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ < 1MB)
#   * MySQL: 64KB-16MB (depending on config)
# - Best practice:
#   * Return ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ metadata (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà data ‡πÄ‡∏≠‡∏á)
#   * ‡∏ñ‡πâ‡∏≤ metadata ‡πÉ‡∏´‡∏ç‡πà ‚Üí save ‡πÄ‡∏õ‡πá‡∏ô file ‡πÅ‡∏ó‡∏ô
"""
‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå CSV, validate, ‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô Parquet format

Args:
    file_path: ‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á absolute ‡πÑ‡∏õ‡∏¢‡∏±‡∏á‡πÑ‡∏ü‡∏•‡πå CSV
        Example: '/opt/airflow/data/raw/data.csv'
        
    encoding: character encoding ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
        Example: 'utf-8', 'ISO-8859-1'
        
    chunk_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ï‡πà‡∏≠ chunk (None = ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå)
        Example: 10000 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà
        
    output_format: ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö output file
        Example: 'parquet' (recommended), 'csv'

Returns:
    Dict containing:
    - status: 'success' ‡∏´‡∏£‡∏∑‡∏≠ 'failed'
    - rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
    - columns: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô columns
    - memory_mb: memory usage
    - output_path: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà save
    - compression_ratio: ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î
    - duration_seconds: ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
    
Raises:
    FileNotFoundError: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏û‡∏ö
    PermissionError: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡πå‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    UnicodeDecodeError: ‡∏ñ‡πâ‡∏≤ encoding ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    pd.errors.ParserError: ‡∏ñ‡πâ‡∏≤ CSV format ‡∏ú‡∏¥‡∏î
    MemoryError: ‡∏ñ‡πâ‡∏≤ RAM ‡πÑ‡∏°‡πà‡∏û‡∏≠
"""

import pandas as pd
import logging
import os
from pathlib import Path
from datetime import datetime
import time

# ============================================================
# STEP 1: SETUP LOGGING
# ============================================================
# logging module: Python's built-in logging framework
# - Levels: DEBUG < INFO < WARNING < ERROR < CRITICAL
# - Airflow automatically captures logs from Python's logging
# - Logs are stored in /opt/airflow/logs/dag_id/task_id/execution_date/
# - Best practice: ‡πÉ‡∏ä‡πâ logger ‡πÅ‡∏ó‡∏ô print()
logger = logging.getLogger(__name__)
# __name__: module name (automatic naming)
# Alternative: logger = logging.getLogger('marketing_etl.ingestion')

logger.info("üöÄ [STEP 1/7] Starting data ingestion task")
# Emoji ‡πÉ‡∏ô logs: ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏´‡∏≤ logs ‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ (‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏°‡∏±‡∏î‡∏£‡∏∞‡∏ß‡∏±‡∏á)
# Pattern: [STEP X/Y] ‡πÄ‡∏û‡∏∑‡πà‡∏≠ track progress

start_time = time.time()
# time.time(): Unix timestamp (seconds since 1970-01-01)
# ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì duration

# ============================================================
# STEP 2: VALIDATE FILE PATH
# ============================================================
logger.info(f"üîç [STEP 2/7] Validating file path: {file_path}")

# Path validation - Security check
# ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô path traversal attacks (e.g., '../../../etc/passwd')
file_path_obj = Path(file_path)
# Path object from pathlib: modern way to handle file paths
# Benefits: 
# - OS-independent (works on Windows/Linux/Mac)
# - Rich methods (.exists(), .is_file(), .stat())
# - Safer than string manipulation

# Security check 1: Resolve to absolute path
try:
    resolved_path = file_path_obj.resolve(strict=True)
    # resolve(): convert to absolute path
    # strict=True: raise error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á
    # ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á resolve:
    # - '../data.csv' ‚Üí '/opt/airflow/data/data.csv'
    # - Prevent path traversal: '../../etc/passwd'
except FileNotFoundError:
    error_msg = f"‚ùå File not found: {file_path}"
    logger.error(error_msg)
    # logger.error(): log level ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö errors ‡∏ó‡∏µ‡πà task fail
    # Airflow ‡∏à‡∏∞ capture error ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô UI
    raise FileNotFoundError(error_msg)
    # raise: throw exception ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ task fail
    # Airflow ‡∏à‡∏∞ retry task ‡∏ï‡∏≤‡∏° retries parameter

# Security check 2: Verify file is within allowed directory
allowed_base = Path('/opt/airflow/data/raw').resolve()
# allowed_base: directory ‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏î‡πâ
# Best practice: hardcode allowed paths ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô config

if not str(resolved_path).startswith(str(allowed_base)):
    error_msg = f"‚ùå File path outside allowed directory: {resolved_path}"
    logger.error(error_msg)
    raise PermissionError(error_msg)
    # PermissionError: exception ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö security violations

# Security check 3: Verify it's a file (not directory)
if not resolved_path.is_file():
    # .is_file(): returns True ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô file (not directory, symlink)
    error_msg = f"‚ùå Path is not a file: {resolved_path}"
    logger.error(error_msg)
    raise ValueError(error_msg)

# Check file extension
if resolved_path.suffix.lower() != '.csv':
    # .suffix: file extension (e.g., '.csv', '.txt')
    # .lower(): case-insensitive comparison
    error_msg = f"‚ùå Invalid file extension: {resolved_path.suffix}"
    logger.error(error_msg)
    raise ValueError(error_msg)

# Check file size (prevent loading huge files)
file_size_bytes = resolved_path.stat().st_size
# .stat(): file metadata (size, timestamps, permissions)
# .st_size: file size in bytes

file_size_mb = file_size_bytes / (1024 * 1024)
# Convert bytes to MB: bytes / 1024 / 1024
# Alternative: file_size_mb = file_size_bytes / 1048576

max_file_size_mb = 10240  # 10 GB limit
# Best practice: ‡∏ï‡∏±‡πâ‡∏á limit ‡∏ï‡∏≤‡∏° available RAM
# Rule of thumb: file size < RAM / 4

if file_size_mb > max_file_size_mb:
    error_msg = f"‚ùå File too large: {file_size_mb:.2f}MB (max: {max_file_size_mb}MB)"
    logger.error(error_msg)
    raise ValueError(error_msg)

logger.info(f"‚úÖ File validation passed: {file_size_mb:.2f}MB")

# ============================================================
# STEP 3: READ CSV FILE
# ============================================================
logger.info(f"üìñ [STEP 3/7] Reading CSV file...")

try:
    # pd.read_csv(): Pandas function to read CSV files
    # ‡∏°‡∏µ parameters ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50 ‡∏ï‡∏±‡∏ß - ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    
    df = pd.read_csv(
        filepath_or_buffer=str(resolved_path),
        # filepath_or_buffer: path to file, URL, or file-like object
        # - Type: str, Path object, or file handle
        # - ‡∏ï‡πâ‡∏≠‡∏á convert Path object ‡πÄ‡∏õ‡πá‡∏ô str
        # - ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö: local files, URLs, S3 (with s3fs), GCS (with gcsfs)
        
        encoding=encoding,
        # encoding: character encoding
        # - ‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ default
        # - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÅ‡∏ô‡πà‡πÉ‡∏à: ‡πÉ‡∏ä‡πâ chardet library detect ‡∏Å‡πà‡∏≠‡∏ô
        
        encoding_errors='strict',
        # encoding_errors: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ encoding errors
        # - Options:
        #   * 'strict': raise UnicodeDecodeError (recommended)
        #   * 'ignore': skip invalid characters (data loss!)
        #   * 'replace': replace with ÔøΩ (visible data loss)
        # - Best practice: 'strict' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        
        low_memory=False,
        # low_memory: ‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡∏≠‡πà‡∏≤‡∏ô‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤
        # - Default: True
        # - ‡∏ñ‡πâ‡∏≤ True: Pandas ‡∏à‡∏∞ infer dtype ‡πÇ‡∏î‡∏¢‡∏î‡∏π‡πÅ‡∏Ñ‡πà chunk ‡πÅ‡∏£‡∏Å
        #   ‚Üí ‡∏≠‡∏≤‡∏à‡πÑ‡∏î‡πâ dtype ‡∏ú‡∏¥‡∏î (mixed types)
        # - ‡∏ñ‡πâ‡∏≤ False: ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠ infer dtype ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        # - Best practice: 
        #   * File < 1GB: low_memory=False
        #   * File > 1GB: low_memory=True + specify dtype manually
        
        na_values=['', 'NA', 'N/A', 'null', 'NULL', 'None', 'nan', 'NaN'],
        # na_values: ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô missing/null
        # - Type: list, dict, or scalar
        # - Default: ['', '#N/A', '#N/A N/A', '#NA', '-1.#IND', ...]
        # - Best practice: ‡πÄ‡∏û‡∏¥‡πà‡∏° domain-specific null values
        #   * Example: '999', 'Unknown', 'N.A.', '--'
        # - Dict format: {'column': ['specific', 'null', 'values']}
        
        keep_default_na=True,
        # keep_default_na: ‡πÉ‡∏ä‡πâ default na_values ‡∏Ç‡∏≠‡∏á Pandas ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # - Default: True
        # - ‡∏ñ‡πâ‡∏≤ False: ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô na_values parameter
        # - Use case: ‡∏ñ‡πâ‡∏≤ 'NA' ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á (e.g., state code for Nebraska)
        
        skipinitialspace=True,
        # skipinitialspace: ‡∏ï‡∏±‡∏î leading whitespace ‡∏´‡∏•‡∏±‡∏á delimiter
        # - Default: False
        # - Example: 'a, b, c' ‚Üí ['a', 'b', 'c'] (not ['a', ' b', ' c'])
        # - Best practice: True ‡πÄ‡∏û‡∏∑‡πà‡∏≠ normalize data
        
        skip_blank_lines=True,
        # skip_blank_lines: ‡∏Ç‡πâ‡∏≤‡∏° empty rows
        # - Default: True
        # - ‡∏ñ‡πâ‡∏≤ False: empty rows ‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô rows ‡∏Ç‡∏≠‡∏á NaN
        # - Best practice: True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏≠‡∏á data
        
        parse_dates=False,
        # parse_dates: ‡πÅ‡∏õ‡∏•‡∏á columns ‡πÄ‡∏õ‡πá‡∏ô datetime
        # - Type: bool, list of columns, or dict
        # - Examples:
        #   * parse_dates=True: ‡∏•‡∏≠‡∏á parse ‡∏ó‡∏∏‡∏Å column (slow!)
        #   * parse_dates=['date_col1', 'date_col2']
        #   * parse_dates={'datetime': ['date', 'time']} (combine columns)
        # - Best practice: 
        #   * ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô ingestion (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ format ‡∏≠‡∏≤‡∏à‡πÑ‡∏°‡πà consistent)
        #   * Parse ‡πÉ‡∏ô cleaning step ‡πÅ‡∏ó‡∏ô (‡∏°‡∏µ control ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤)
        
        infer_datetime_format=False,
        # infer_datetime_format: ‡πÉ‡∏´‡πâ Pandas ‡∏•‡∏≠‡∏á detect format
        # - Default: False (deprecated in Pandas 2.0)
        # - ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥: ‡∏£‡∏∞‡∏ö‡∏∏ format ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô pd.to_datetime() ‡πÅ‡∏ó‡∏ô
        
        dayfirst=False,
        # dayfirst: ‡∏ñ‡πâ‡∏≤‡∏ß‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡πÉ‡∏ô ambiguous dates
        # - Default: False (MM/DD/YYYY - US format)
        # - True: DD/MM/YYYY (European format)
        # - Example: '01/02/2024'
        #   * dayfirst=False ‚Üí Jan 2, 2024
        #   * dayfirst=True ‚Üí Feb 1, 2024
        # - Best practice: ‡∏£‡∏∞‡∏ö‡∏∏ format ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÉ‡∏ô pd.to_datetime() ‡πÅ‡∏ó‡∏ô
        
        dtype=None,
        # dtype: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î data type ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ column
        # - Type: dict or None
        # - Example: {'col1': 'int64', 'col2': 'float64', 'col3': 'string'}
        # - Benefits:
        #   * Faster reading (no type inference)
        #   * Prevent type errors (e.g., mixed types)
        #   * Save memory (e.g., int32 vs int64)
        # - Best practice:
        #   * None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ingestion (‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô)
        #   * Specify dtype ‡πÉ‡∏ô production (consistency)
        
        chunksize=chunk_size,
        # chunksize: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ï‡πà‡∏≠ chunk
        # - ‡∏ñ‡πâ‡∏≤ None: ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡πÑ‡∏ü‡∏•‡πå
        # - ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏∏: return iterator of DataFrames
        # - Use case: ‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô RAM
        # - Example usage:
        #   ```python
        #   for chunk in pd.read_csv(file, chunksize=10000):
        #       process(chunk)
        #       save(chunk)
        #   ```
        
        on_bad_lines='error',
        # on_bad_lines: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ rows ‡∏ó‡∏µ‡πà malformed
        # - Options:
        #   * 'error': raise error (recommended for quality)
        #   * 'warn': skip row ‡πÅ‡∏•‡∏∞ log warning
        #   * 'skip': skip row ‡πÅ‡∏ö‡∏ö‡πÄ‡∏á‡∏µ‡∏¢‡∏ö‡πÜ (‡∏≠‡∏±‡∏ô‡∏ï‡∏£‡∏≤‡∏¢!)
        # - Best practice: 'error' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤
        #   * ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏î error ‚Üí ‡πÑ‡∏õ‡∏ï‡∏£‡∏ß‡∏à source data
        
        memory_map=True,
        # memory_map: ‡πÉ‡∏ä‡πâ memory-mapped file I/O
        # - Default: False
        # - Benefits: faster ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà
        # - ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô: map file ‡πÄ‡∏Ç‡πâ‡∏≤ RAM ‡πÅ‡∏ö‡∏ö lazy (load ‡∏ï‡∏≠‡∏ô‡πÉ‡∏ä‡πâ)
        # - Trade-off: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏±‡∏ö local files, ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö network files
        
        verbose=False,
        # verbose: ‡πÅ‡∏™‡∏î‡∏á progress ‡∏Ç‡∏ì‡∏∞‡∏≠‡πà‡∏≤‡∏ô
        # - Default: False
        # - ‡∏ñ‡πâ‡∏≤ True: print diagnostic info
        # - Best practice: False (‡πÉ‡∏ä‡πâ logger ‡πÅ‡∏ó‡∏ô)
    )
    
    logger.info(f"‚úÖ CSV loaded successfully: {len(df):,} rows √ó {len(df.columns)} columns")
    # f-string formatting:
    # - {len(df):,}: format number with comma separator
    # - Example: 125483 ‚Üí 125,483
    
except UnicodeDecodeError as e:
    # UnicodeDecodeError: encoding ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    # - Common causes:
    #   * Wrong encoding specified
    #   * File corrupted
    #   * Mixed encodings in file
    error_msg = f"‚ùå Encoding error: {str(e)}\nTry different encoding (utf-8, ISO-8859-1, Windows-1252)"
    logger.error(error_msg)
    raise UnicodeDecodeError(e.encoding, e.object, e.start, e.end, error_msg)
    # Re-raise with additional context
    
except pd.errors.ParserError as e:
    # ParserError: CSV format ‡∏ú‡∏¥‡∏î
    # - Common causes:
    #   * Mixed delimiters (comma vs tab)
    #   * Unquoted delimiters inside fields
    #   * Inconsistent number of columns
    #   * Malformed rows
    error_msg = f"‚ùå CSV parsing error: {str(e)}\nCheck file format and delimiters"
    logger.error(error_msg)
    raise pd.errors.ParserError(error_msg)
    
except MemoryError as e:
    # MemoryError: RAM ‡πÑ‡∏°‡πà‡∏û‡∏≠
    # - Solutions:
    #   * Use chunksize parameter
    #   * Use Dask instead of Pandas
    #   * Increase Docker memory limit
    #   * Use sampling for development
    error_msg = f"‚ùå Out of memory: {str(e)}\nTry using chunksize parameter or increase RAM"
    logger.error(error_msg)
    raise MemoryError(error_msg)
    
except Exception as e:
    # Catch-all for unexpected errors
    # - Always log unexpected errors
    # - Re-raise to fail task
    error_msg = f"‚ùå Unexpected error during CSV read: {str(e)}"
    logger.error(error_msg)
    raise Exception(error_msg) from e
    # raise ... from e: chain exceptions (preserve stack trace)

# ============================================================
# STEP 4: BASIC SCHEMA VALIDATION
# ============================================================
logger.info(f"üîç [STEP 4/7] Validating basic schema...")

# Check ‡∏ß‡πà‡∏≤ DataFrame ‡πÑ‡∏°‡πà empty
if df.empty:
    # df.empty: True ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ rows
    error_msg = "‚ùå DataFrame is empty (0 rows)"
    logger.error(error_msg)
    raise ValueError(error_msg)

# Check ‡∏ß‡πà‡∏≤‡∏°‡∏µ columns
if len(df.columns) == 0:
    error_msg = "‚ùå DataFrame has no columns"
    logger.error(error_msg)
    raise ValueError(error_msg)

# Log column names and types
logger.info("üìã Columns detected:")
for col in df.columns:
    # df.dtypes: Series of data types
    # - dtype: NumPy/Pandas data type
    # - Common types:
    #   * int64: 64-bit integer
    #   * float64: 64-bit float
    #   * object: string or mixed types
    #   * datetime64[ns]: datetime with nanosecond precision
    #   * bool: boolean
    #   * category: categorical (efficient for repeated values)
    logger.info(f"  - {col}: {df[col].dtype}")

logger.info("‚úÖ Schema validation passed")

# ============================================================
# STEP 5: COLLECT METADATA
# ============================================================
logger.info(f"üìä [STEP 5/7] Collecting metadata...")

# Memory usage
memory_usage_bytes = df.memory_usage(deep=True).sum()
# df.memory_usage(): memory ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠ column
# - deep=True: ‡∏£‡∏ß‡∏° object overhead (strings, etc.)
# - deep=False: ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ pointer size (underestimate)
# - .sum(): total memory
# - Best practice: always use deep=True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö accurate measurement

memory_usage_mb = memory_usage_bytes / (1024 * 1024)

# Row and column count
num_rows, num_cols = df.shape
# df.shape: tuple (rows, columns)
# - Alternative: len(df), len(df.columns)

logger.info(f"  - Rows: {num_rows:,}")
logger.info(f"  - Columns: {num_cols}")
logger.info(f"  - Memory: {memory_usage_mb:.2f} MB")
logger.info(f"  - File size: {file_size_mb:.2f} MB")

# ============================================================
# STEP 6: SAVE AS PARQUET
# ============================================================
logger.info(f"üíæ [STEP 6/7] Saving as Parquet...")

# Create output directory if not exists
output_dir = Path('/opt/airflow/data/processed')
output_dir.mkdir(parents=True, exist_ok=True)
# mkdir(): create directory
# - parents=True: create parent directories ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
# - exist_ok=True: ‡πÑ‡∏°‡πà raise error ‡∏ñ‡πâ‡∏≤ directory ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß
# - Best practice: always create directories before writing

# Generate output filename with timestamp
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
# strftime(): format datetime as string
# - %Y: year (4 digits)
# - %m: month (01-12)
# - %d: day (01-31)
# - %H: hour (00-23)
# - %M: minute (00-59)
# - %S: second (00-59)
# - Result: '20241224_143022'

output_filename = f'ingested_{timestamp}.parquet'
output_path = output_dir / output_filename
# Path operator /: join paths
# - Same as: output_path = output_dir.joinpath(output_filename)

try:
    df.to_parquet(
        path=str(output_path),
        # path: output file path
        # - ‡∏ï‡πâ‡∏≠‡∏á convert Path object ‡πÄ‡∏õ‡πá‡∏ô str
        
        engine='pyarrow',
        # engine: library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö write Parquet
        # - Options: 'auto', 'pyarrow', 'fastparquet'
        # - pyarrow: ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤, stable, recommended
        # - fastparquet: alternative, ‡∏ö‡∏≤‡∏á features ‡πÑ‡∏°‡πà‡∏°‡∏µ
        # - Best practice: always specify 'pyarrow'
        
        compression='snappy',
        # compression: algorithm ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î
        # - Options:
        #   * 'snappy': fast, moderate compression (recommended)
        #   * 'gzip': slower, better compression
        #   * 'lz4': fastest, light compression
        #   * 'zstd': balanced, best compression ratio
        #   * 'brotli': slowest, highest compression
        #   * None: no compression
        # - Comparison (100MB file):
        #   * snappy: 30MB, 2s write, 0.5s read
        #   * gzip: 25MB, 5s write, 1s read
        #   * lz4: 35MB, 1s write, 0.3s read
        #   * zstd: 28MB, 3s write, 0.7s read
        # - Best practice:
        #   * Default: snappy (balanced)
        #   * Storage-critical: zstd or gzip
        #   * Speed-critical: lz4
        
        index=False,
        # index: ‡πÄ‡∏Å‡πá‡∏ö DataFrame index ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        # - Default: None (auto-detect)
        # - False: ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡πá‡∏ö index (recommended)
        # - True: ‡πÄ‡∏Å‡πá‡∏ö index as column
        # - Best practice: False (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ index ‡∏°‡∏±‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢)
        
        partition_cols=None,
        # partition_cols: ‡πÅ‡∏ö‡πà‡∏á files ‡∏ï‡∏≤‡∏° column values
        # - Type: list of column names or None
        # - Example: partition_cols=['year', 'month']
        #   ‚Üí Creates directory structure:
        #     year=2024/month=01/data.parquet
        #     year=2024/month=02/data.parquet
        # - Benefits:
        #   * Query efficiency (read ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ partitions ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        #   * Parallel processing
        # - Best practice:
        #   * Partition by columns ‡∏ó‡∏µ‡πà query ‡∏ö‡πà‡∏≠‡∏¢ (date, region)
        #   * ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£ partition ‡∏ñ‡πâ‡∏≤ data ‡∏ô‡πâ‡∏≠‡∏¢ (< 1GB)
        #   * Avoid high cardinality columns (e.g., user_id)
        # - ‡πÉ‡∏ô ingestion step: ‡πÑ‡∏°‡πà partition (partition ‡πÉ‡∏ô loading step)
        
        row_group_size=None,
        # row_group_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ï‡πà‡∏≠ row group
        # - Default: None (auto-calculate ~64MB per group)
        # - Row group: unit of parallelism ‡πÉ‡∏ô Parquet
        # - Small row groups: 
        #   * Better parallelism
        #   * More file overhead
        # - Large row groups:
        #   * Lessoverhead
#   * Less parallelism
# - Best practice: ‡πÉ‡∏ä‡πâ default (‡πÉ‡∏´‡πâ PyArrow ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì)
)
    # Get output file size
    output_size_bytes = output_path.stat().st_size
    output_size_mb = output_size_bytes / (1024 * 1024)
    
    # Calculate compression ratio
    compression_ratio = file_size_mb / output_size_mb if output_size_mb > 0 else 0
    # Compression ratio: original size / compressed size
    # - Example: 100MB ‚Üí 30MB = 3.33x compression
    # - Typical Parquet compression: 2-5x ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV
    
    logger.info(f"‚úÖ Parquet saved: {output_size_mb:.2f} MB ({compression_ratio:.2f}x compression)")
    
except Exception as e:
    error_msg = f"‚ùå Error saving Parquet: {str(e)}"
    logger.error(error_msg)
    raise Exception(error_msg) from e

# ============================================================
# STEP 7: RETURN METADATA (via XCom)
# ============================================================
logger.info(f"üéØ [STEP 7/7] Task completed successfully")

# Calculate duration
end_time = time.time()
duration_seconds = end_time - start_time

# Create metadata dictionary
# XCom (Cross-Communication): Airflow's mechanism ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ä‡∏£‡πå data ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á tasks
# - How it works:
#   * Task returns value ‚Üí Airflow stores in XCom table
#   * Next task pulls value from XCom
# - Storage:
#   * SQLite (dev): xcom table in airflow.db
#   * PostgreSQL/MySQL (prod): xcom table
# - Size limits:
#   * SQLite: 2GB (but not recommended > 1MB)
#   * PostgreSQL: 1GB (but not recommended > 1MB)
#   * MySQL: 64KB default, max 16MB
# - Best practices:
#   * Return ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ metadata (paths, counts, flags)
#   * ‡∏´‡πâ‡∏≤‡∏° return DataFrame, large objects
#   * ‡∏ñ‡πâ‡∏≤ data ‡πÉ‡∏´‡∏ç‡πà ‚Üí save file ‡πÅ‡∏•‡∏∞ return path
# - Alternative approaches:
#   * File-based: save metadata.json, return path
#   * Database: save to metadata table, return ID
#   * Object storage: S3/GCS, return URI

metadata = {
    'status': 'success',
    # status: task execution status
    # - Values: 'success', 'failed', 'partial'
    # - ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö conditional branching ‡πÉ‡∏ô DAG
    
    'file_path': str(resolved_path),
    # file_path: input file path
    # - ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠ audit trail
    # - ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô error reporting
    
    'output_path': str(output_path),
    # output_path: Parquet file path
    # - Tasks ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏à‡∏∞‡πÉ‡∏ä‡πâ path ‡∏ô‡∏µ‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    # - Important: ‡∏ï‡πâ‡∏≠‡∏á return absolute path
    
    'rows': int(num_rows),
    # rows: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows
    # - Type: int (convert from numpy.int64)
    # - ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö validation, monitoring
    
    'columns': int(num_cols),
    # columns: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô columns
    # - ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö schema consistency
    
    'memory_mb': round(memory_usage_mb, 2),
    # memory_mb: memory usage
    # - round(x, 2): round to 2 decimal places
    # - ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö monitoring, capacity planning
    
    'file_size_mb': round(file_size_mb, 2),
    # file_size_mb: original file size
    # - ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì compression ratio
    
    'output_size_mb': round(output_size_mb, 2),
    # output_size_mb: Parquet file size
    # - ‡πÉ‡∏ä‡πâ track storage growth
    
    'compression_ratio': round(compression_ratio, 2),
    # compression_ratio: ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ö‡∏µ‡∏ö‡∏≠‡∏±‡∏î
    # - Monitor compression performance
    # - Detect data quality issues (low compression = dirty data)
    
    'duration_seconds': round(duration_seconds, 2),
    # duration_seconds: processing time
    # - ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö performance monitoring
    # - Detect performance degradation
    
    'timestamp': datetime.now().isoformat(),
    # timestamp: ISO 8601 format timestamp
    # - Format: '2024-12-24T14:30:22.123456'
    # - ISO 8601: international standard, machine-readable
    # - Alternative: .strftime('%Y-%m-%d %H:%M:%S')
    
    'columns_list': df.columns.tolist(),
    # columns_list: actual column names
    # - ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö schema validation ‡πÉ‡∏ô tasks ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ
    # - .tolist(): convert Index to Python list
    
    'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
    # dtypes: column data types
    # - dict comprehension: {key: value for ...}
    # - str(dtype): convert numpy dtype to string
    # - ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö type consistency
}

logger.info(f"‚úÖ Metadata prepared for XCom")
logger.info(f"‚è±Ô∏è  Total duration: {duration_seconds:.2f}s")

# Return metadata ‚Üí Airflow stores in XCom automatically
# Access in next task:
# ```python
# @task
# def next_task(metadata):  # Airflow passes XCom value as argument
#     file_path = metadata['output_path']
#     df = pd.read_parquet(file_path)
# ```
return metadata

---

### üéì Key Learnings from Task 1

**1. File I/O Best Practices**:
- Always validate paths (security)
- Handle encoding explicitly
- Check file sizes before loading
- Use Parquet for intermediate storage

**2. Error Handling Strategy**:
- Specific exceptions first (FileNotFoundError, UnicodeDecodeError)
- Generic Exception as fallback
- Always log errors with context
- Re-raise to fail task (enable retry)

**3. Memory Management**:
- Monitor memory usage with `df.memory_usage(deep=True)`
- Use `low_memory=False` for accurate dtypes
- Consider chunking for large files
- Parquet reduces memory footprint 2-5x

**4. XCom Guidelines**:
- Return metadata only (paths, counts, flags)
- Never return DataFrames or large objects
- Keep XCom payloads < 1MB
- Use ISO 8601 for timestamps

**5. Logging Best Practices**:
- Use structured logging with [STEP X/Y]
- Include metrics (rows, size, duration)
- Log at appropriate levels (INFO, WARNING, ERROR)
- Emoji for visual scanning (use sparingly)

---

### ‚úÖ Testing Task 1
```python
# tests/test_ingestion.py
import pytest
from dags.tasks.ingestion import ingest_raw_data

def test_ingest_valid_file():
    """Test ingestion with valid CSV file"""
    metadata = ingest_raw_data(
        file_path='/opt/airflow/tests/fixtures/sample.csv'
    )
    
    assert metadata['status'] == 'success'
    assert metadata['rows'] > 0
    assert Path(metadata['output_path']).exists()

def test_ingest_missing_file():
    """Test ingestion with missing file"""
    with pytest.raises(FileNotFoundError):
        ingest_raw_data(file_path='/nonexistent/file.csv')

def test_ingest_invalid_encoding():
    """Test ingestion with wrong encoding"""
    with pytest.raises(UnicodeDecodeError):
        ingest_raw_data(
            file_path='/opt/airflow/tests/fixtures/sample.csv',
            encoding='ascii'  # Wrong encoding
        )
```

---
üìä Task 2: Data Profiling
üéØ ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å
=============================================================================
TASK 2: DATA PROFILING - ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
=============================================================================
‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:

‡πÇ‡∏´‡∏•‡∏î Parquet file ‡∏à‡∏≤‡∏Å Task 1
‡∏™‡∏£‡πâ‡∏≤‡∏á comprehensive data quality report ‡∏î‡πâ‡∏ß‡∏¢ ydata-profiling
‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå missing values, duplicates, distributions
‡∏™‡∏£‡πâ‡∏≤‡∏á HTML report ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö manual review
Extract key metrics ‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö automated decisions
Flag potential data quality issues

üìä Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

Read Parquet from Task 1 output
Generate ProfileReport with ydata-profiling
Save HTML report for stakeholders
Extract JSON metrics for pipeline logic
Analyze key indicators (missing %, duplicates, correlations)
Flag issues that need attention
Return summary metrics to XCom

‚ö†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:

Profiling ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö large datasets (‡∏≠‡∏≤‡∏à‡∏ñ‡∏∂‡∏á 10-30 ‡∏ô‡∏≤‡∏ó‡∏µ)
Memory intensive (‡πÉ‡∏ä‡πâ RAM ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ data size 2-3 ‡πÄ‡∏ó‡πà‡∏≤)
HTML report size ‡πÉ‡∏´‡∏ç‡πà (10-100 MB) ‚Üí storage management
Correlation calculations expensive (O(n¬≤) columns)
‡∏Ñ‡∏ß‡∏£ skip profiling ‡πÉ‡∏ô CI/CD pipelines (‡∏ä‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ)


üìù Complete Implementation
python@task(
    # task_id: ‡∏ä‡∏∑‡πà‡∏≠ unique identifier ‡∏Ç‡∏≠‡∏á task
    # - Naming: snake_case, verb + noun
    # - Example: profile_data, generate_quality_report
    task_id='profile_data',
    
    # retries: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô retry
    # - Profiling ‡∏Ñ‡∏ß‡∏£ retry ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ingestion
    # - ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡∏ñ‡πâ‡∏≤ profiling fail ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô resource issue (memory, timeout)
    # - Resource issues ‡∏°‡∏±‡∏Å‡πÑ‡∏°‡πà recover ‡∏î‡πâ‡∏ß‡∏¢ retry
    # - Use case:
    #   * Transient errors: 1-2 retries (e.g., temporary file lock)
    #   * Memory errors: 0-1 retries (retry ‡πÑ‡∏°‡πà‡∏ä‡πà‡∏ß‡∏¢)
    # - Best practice: 1 retry ‡∏û‡∏≠, ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á fail ‚Üí investigate
    retries=1,
    
    # retry_delay: ‡πÄ‡∏ß‡∏•‡∏≤‡∏£‡∏≠‡∏Å‡πà‡∏≠‡∏ô retry
    # - Profiling ‡∏Ñ‡∏ß‡∏£‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ ingestion
    # - ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: ‡πÉ‡∏´‡πâ‡πÄ‡∏ß‡∏•‡∏≤ OS reclaim memory
    # - Best practice: 5-10 minutes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö memory-intensive tasks
    retry_delay=timedelta(minutes=5),
    
    # execution_timeout: ‡πÄ‡∏ß‡∏•‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà task ‡∏£‡∏±‡∏ô‡πÑ‡∏î‡πâ
    # - Profiling ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏°‡∏≤‡∏Å
    # - ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:
    #   * Small dataset (< 100K rows): 5-10 minutes
    #   * Medium dataset (100K-1M rows): 10-30 minutes
    #   * Large dataset (> 1M rows): 30-60 minutes
    # - Best practice: ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏Å‡∏û‡∏≠ + monitor actual runtime
    # - ‡∏ñ‡πâ‡∏≤ timeout ‡∏ö‡πà‡∏≠‡∏¢ ‚Üí ‡πÉ‡∏ä‡πâ minimal profiling mode
    execution_timeout=timedelta(minutes=30),
    
    # pool: resource pool ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö task
    # - Profiling ‡πÄ‡∏õ‡πá‡∏ô memory-intensive task
    # - ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ dedicated pool ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏¥‡∏ô resources ‡∏Ç‡∏≠‡∏á tasks ‡∏≠‡∏∑‡πà‡∏ô
    # - Configuration:
    #   * Pool name: 'memory_intensive'
    #   * Slots: 2-3 (‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô profiling tasks ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
    pool='memory_intensive',
    
    # pool_slots: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô slots ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ
    # - Profiling ‡πÉ‡∏ä‡πâ resources ‡πÄ‡∏¢‡∏≠‡∏∞ ‚Üí ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ slots
    # - Small dataset: 1 slot
    # - Medium dataset: 2 slots
    # - Large dataset: 3 slots
    # - Trade-off: slots ‡πÄ‡∏¢‡∏≠‡∏∞ ‚Üí tasks ‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏≠‡∏ô‡∏≤‡∏ô
    pool_slots=2,
    
    # retry_exponential_backoff: ‡πÄ‡∏û‡∏¥‡πà‡∏° delay ‡πÅ‡∏ö‡∏ö exponential
    # - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö profiling: ‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
    # - ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•: profiling fail ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô deterministic (memory, timeout)
    # - ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡∏°‡∏µ transient errors: ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ
    retry_exponential_backoff=False,
)
def profile_data(
    ingestion_metadata: dict,
    # ingestion_metadata: metadata ‡∏à‡∏≤‡∏Å Task 1 (‡∏ú‡πà‡∏≤‡∏ô XCom)
    # - Type: dict
    # - Airflow TaskFlow API ‡∏à‡∏∞ pass XCom value ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    # - Structure: {
    #     'output_path': '/path/to/parquet',
    #     'rows': 125483,
    #     'columns': 8,
    #     ...
    #   }
    # - ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô: ‡∏î‡∏∂‡∏á output_path ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå
    # - Alternative approach (explicit pull):
    #   ```python
    #   from airflow.operators.python import get_current_context
    #   context = get_current_context()
    #   metadata = context['ti'].xcom_pull(task_ids='ingest_raw_data')
    #   ```
    
    profiling_mode: str = 'minimal',
    # profiling_mode: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á profiling
    # - Type: str
    # - Options:
    #   * 'minimal': basic statistics only (fastest)
    #     - Missing values, types, basic stats
    #     - No correlations, no interactions
    #     - Time: ~1-5 minutes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1M rows
    #   
    #   * 'explorative': moderate analysis (balanced)
    #     - Everything in minimal
    #     - + Correlations (Pearson, Spearman, etc.)
    #     - + Missing value patterns
    #     - Time: ~5-15 minutes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1M rows
    #   
    #   * 'full': complete analysis (slowest)
    #     - Everything in explorative
    #     - + Interactions between variables
    #     - + Detailed duplicate analysis
    #     - Time: ~15-60 minutes ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1M rows
    # 
    # - Best practice:
    #   * Development: 'minimal' (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤)
    #   * Production (daily): 'minimal' (performance)
    #   * Ad-hoc analysis: 'explorative' or 'full'
    #   * New dataset: 'full' (‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÅ‡∏£‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à data)
    # 
    # - Trade-off:
    #   * More detail = more time + more memory
    #   * Minimal ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î important patterns
    
    enable_correlations: bool = False,
    # enable_correlations: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation matrix ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # - Type: bool
    # - Default: False (‡πÄ‡∏û‡∏£‡∏≤‡∏∞ expensive)
    # - Correlation types calculated:
    #   * Pearson: linear correlation (-1 to 1)
    #   * Spearman: rank correlation (non-linear)
    #   * Kendall: rank correlation (alternative to Spearman)
    #   * Cram√©r's V: categorical correlation
    # - Computational cost:
    #   * O(n¬≤) where n = number of columns
    #   * 10 columns: 45 correlations (fast)
    #   * 50 columns: 1,225 correlations (slow)
    #   * 100 columns: 4,950 correlations (very slow)
    # - Memory cost:
    #   * Stores full correlation matrices
    #   * 100 columns √ó 100 columns √ó 8 bytes = 80KB per matrix
    #   * √ó 4 types = 320KB (acceptable)
    # - Best practice:
    #   * False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö routine profiling
    #   * True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature selection, EDA
    # - Use case: ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏´‡∏≤ multicollinearity ‡∏Å‡πà‡∏≠‡∏ô modeling
    
    sample_size: int = None,
    # sample_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡∏à‡∏∞ sample ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö profiling
    # - Type: int or None
    # - Default: None (‡πÉ‡∏ä‡πâ data ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
    # - Use case: dataset ‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å (> 5M rows)
    # - Sampling strategy:
    #   * Random sampling: df.sample(n=sample_size)
    #   * Representative: stratified sampling by key columns
    # - Best practice:
    #   * < 1M rows: None (‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
    #   * 1-5M rows: 500K-1M (representative)
    #   * > 5M rows: 1M (statistical validity)
    # - Trade-off:
    #   * Smaller sample = faster profiling
    #   * But: ‡∏≠‡∏≤‡∏à‡∏û‡∏•‡∏≤‡∏î outliers, rare patterns
    # - Statistical note:
    #   * n=1000: margin of error ¬±3%
    #   * n=10000: margin of error ¬±1%
    #   * n=100000: margin of error ¬±0.3%
    
    output_format: str = 'html',
    # output_format: format ‡∏Ç‡∏≠‡∏á profiling report
    # - Type: str
    # - Options:
    #   * 'html': interactive HTML report (recommended)
    #   * 'json': machine-readable JSON
    #   * 'both': ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏±‡πâ‡∏á HTML ‡πÅ‡∏•‡∏∞ JSON
    # - HTML benefits:
    #   * Interactive charts (hover, zoom)
    #   * Easy to share with non-technical stakeholders
    #   * Visual data exploration
    # - JSON benefits:
    #   * Programmatic access to metrics
    #   * Version control friendly (‡∏ñ‡πâ‡∏≤ pretty-printed)
    #   * Can be loaded into dashboards
    # - Best practice: 'both' ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production
    
) -> dict:
    # Return type: dictionary containing profiling results
    # - Structure:
    #   {
    #       'status': str,                    # 'success' or 'failed'
    #       'html_report_path': str,          # Path to HTML report
    #       'json_report_path': str,          # Path to JSON report
    #       'total_rows': int,                # Number of rows profiled
    #       'total_columns': int,             # Number of columns
    #       'missing_cells_pct': float,       # % of missing values
    #       'duplicate_rows': int,            # Number of duplicates
    #       'duplicate_rows_pct': float,      # % duplicates
    #       'columns_with_missing': list,     # Columns with nulls
    #       'high_missing_columns': list,     # Columns with >30% missing
    #       'high_cardinality_columns': list, # Columns with many unique values
    #       'skewed_columns': list,           # Highly skewed distributions
    #       'constant_columns': list,         # Columns with 1 unique value
    #       'flags': dict,                    # Quality flags/warnings
    #       'duration_seconds': float,        # Processing time
    #       'timestamp': str                  # ISO timestamp
    #   }
    
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á data profiling report ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    
    Args:
        ingestion_metadata: metadata ‡∏à‡∏≤‡∏Å ingestion task
            Required keys: 'output_path' (path to Parquet file)
            
        profiling_mode: ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏Ç‡∏≠‡∏á profiling
            Options: 'minimal', 'explorative', 'full'
            
        enable_correlations: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            True = ‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ ‡πÅ‡∏ï‡πà‡πÑ‡∏î‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• relationship ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á columns
            
        sample_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ó‡∏µ‡πà‡∏à‡∏∞ sample (None = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
            Use for large datasets (> 1M rows)
            
        output_format: format ‡∏Ç‡∏≠‡∏á report
            Options: 'html', 'json', 'both'
    
    Returns:
        Dict containing:
        - status: success/failed
        - report paths: HTML and/or JSON
        - summary metrics: missing %, duplicates, etc.
        - quality flags: issues ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á attention
        - processing metadata: duration, timestamp
        
    Raises:
        FileNotFoundError: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå Parquet ‡πÑ‡∏°‡πà‡∏û‡∏ö
        MemoryError: ‡∏ñ‡πâ‡∏≤ RAM ‡πÑ‡∏°‡πà‡∏û‡∏≠‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö profiling
        ValueError: ‡∏ñ‡πâ‡∏≤ profiling_mode ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
    """
    
    import pandas as pd
    import logging
    from ydata_profiling import ProfileReport
    from pathlib import Path
    from datetime import datetime
    import time
    import json
    
    # ============================================================
    # STEP 1: SETUP AND VALIDATION
    # ============================================================
    logger = logging.getLogger(__name__)
    logger.info("üöÄ [STEP 1/8] Starting data profiling task")
    
    start_time = time.time()
    
    # Validate ingestion metadata
    if not ingestion_metadata:
        error_msg = "‚ùå No metadata received from ingestion task"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    if 'output_path' not in ingestion_metadata:
        error_msg = "‚ùå Missing 'output_path' in ingestion metadata"
        logger.error(error_msg)
        raise KeyError(error_msg)
    
    # Validate profiling_mode
    valid_modes = ['minimal', 'explorative', 'full']
    if profiling_mode not in valid_modes:
        error_msg = f"‚ùå Invalid profiling_mode: {profiling_mode}. Must be one of {valid_modes}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    logger.info(f"‚úÖ Validation passed")
    logger.info(f"  - Profiling mode: {profiling_mode}")
    logger.info(f"  - Enable correlations: {enable_correlations}")
    logger.info(f"  - Sample size: {sample_size or 'Full dataset'}")
    
    # ============================================================
    # STEP 2: READ PARQUET FILE
    # ============================================================
    logger.info(f"üìñ [STEP 2/8] Reading Parquet file...")
    
    parquet_path = Path(ingestion_metadata['output_path'])
    
    # Validate file exists
    if not parquet_path.exists():
        error_msg = f"‚ùå Parquet file not found: {parquet_path}"
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        # pd.read_parquet(): read Parquet file into DataFrame
        df = pd.read_parquet(
            path=str(parquet_path),
            # path: file path or URL
            
            engine='pyarrow',
            # engine: backend library ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏≠‡πà‡∏≤‡∏ô Parquet
            # - Options: 'auto', 'pyarrow', 'fastparquet'
            # - Best practice: always use 'pyarrow' (faster, more features)
            
            columns=None,
            # columns: list of columns to read (None = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
            # - Use case: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (save memory)
            # - Example: columns=['col1', 'col2', 'col3']
            # - Best practice: 
            #   * Profiling: ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ complete picture)
            #   * Processing: ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô (optimize memory)
            
            use_nullable_dtypes=False,
            # use_nullable_dtypes: ‡πÉ‡∏ä‡πâ Pandas nullable dtypes ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            # - Default: False
            # - Pandas nullable dtypes (Int64, string, boolean):
            #   * Support NA values properly
            #   * Traditional types: int64 ‡πÑ‡∏°‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö NA (convert to float64)
            # - Example:
            #   * False: int column with NA ‚Üí float64
            #   * True: int column with NA ‚Üí Int64 (capital I)
            # - Best practice:
            #   * False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö backward compatibility
            #   * True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö new projects (cleaner type handling)
        )
        
        logger.info(f"‚úÖ Parquet loaded: {len(df):,} rows √ó {len(df.columns)} columns")
        
    except FileNotFoundError as e:
        error_msg = f"‚ùå File not found: {str(e)}"
        logger.error(error_msg)
        raise
        
    except Exception as e:
        error_msg = f"‚ùå Error reading Parquet: {str(e)}"
        logger.error(error_msg)
        raise Exception(error_msg) from e
    
    # ============================================================
    # STEP 3: SAMPLE DATA (if needed)
    # ============================================================
    original_rows = len(df)
    
    if sample_size and original_rows > sample_size:
        logger.info(f"üé≤ [STEP 3/8] Sampling {sample_size:,} rows from {original_rows:,}...")
        
        # df.sample(): random sampling
        df = df.sample(
            n=sample_size,
            # n: number of rows to sample
            # - Alternative: frac=0.5 (sample 50%)
            
            random_state=42,
            # random_state: seed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reproducibility
            # - Same seed ‚Üí same sample every time
            # - Best practice: always set random_state
            # - Convention: 42 (Hitchhiker's Guide reference)
            
            replace=False,
            # replace: sampling with replacement
            # - False: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ row ‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (default)
            # - True: ‡πÅ‡∏ï‡πà‡∏•‡∏∞ row ‡∏≠‡∏≤‡∏à‡∏ñ‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            # - Best practice: False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö profiling
        )
        
        logger.info(f"‚úÖ Sampled to {len(df):,} rows")
    else:
        logger.info(f"‚ÑπÔ∏è  [STEP 3/8] Using full dataset ({original_rows:,} rows)")
    
    # ============================================================
    # STEP 4: CONFIGURE PROFILING
    # ============================================================
    logger.info(f"‚öôÔ∏è  [STEP 4/8] Configuring ProfileReport...")
    
    # ProfileReport configuration
    # ydata-profiling ‡∏°‡∏µ configuration ‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡∏Å
    # ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á config based on profiling_mode
    
    # Base configuration (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å mode)
    config = {
        # ==============================================
        # DATASET CONFIGURATION
        # ==============================================
        'title': f'Data Quality Report - {datetime.now().strftime("%Y-%m-%d")}',
        # title: ‡∏ä‡∏∑‡πà‡∏≠ report
        # - ‡πÅ‡∏™‡∏î‡∏á‡∏ó‡∏µ‡πà header ‡∏Ç‡∏≠‡∏á HTML report
        
        'dataset': {
            'description': f'Marketing data profiling ({profiling_mode} mode)',
            # description: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ dataset
            
            'url': str(parquet_path),
            # url: path ‡∏´‡∏£‡∏∑‡∏≠ URL ‡∏Ç‡∏≠‡∏á data source
        },
        
        # ==============================================
        # VARIABLES (COLUMNS) CONFIGURATION
        # ==============================================
        'variables': {
            'descriptions': {},
            # descriptions: ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞ column
            # - Format: {'column_name': 'description'}
            # - Best practice: ‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å data dictionary/metadata
        },
        
        # ==============================================
        # CORRELATIONS CONFIGURATION
        # ==============================================
        'correlations': {
            'auto': {
                'calculate': enable_correlations,
                # calculate: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì correlation ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                # - False: skip (save time)
                # - True: calculate all correlation types
                
                'warn_high_correlations': True,
                # warn_high_correlations: ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ high correlation
                # - Threshold: > 0.9 (default)
                # - Use case: detect multicollinearity
            },
            
            'pearson': {
                'calculate': enable_correlations,
                # Pearson correlation: linear relationship
                # - Range: -1 (perfect negative) to +1 (perfect positive)
                # - 0: no linear correlation
                # - Use case: continuous variables
                
                'warn_high_correlations': True,
                'threshold': 0.9,
                # threshold: ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ñ‡∏∑‡∏≠‡∏ß‡πà‡∏≤ "high correlation"
                # - 0.9: very strong correlation
                # - 0.7-0.9: strong correlation
                # - Best practice: 0.9 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö multicollinearity detection
            },
            
            'spearman': {
                'calculate': enable_correlations,
                # Spearman correlation: monotonic relationship
                # - Rank-based (not sensitive to outliers)
                # - Can detect non-linear relationships
                # - Use case: ordinal data, non-linear relationships
                
                'warn_high_correlations': False,
                # Usually less critical than Pearson
            },
            
            'kendall': {
                'calculate': False,
                # Kendall's tau: alternative rank correlation
                # - More robust than Spearman for small samples
                # - Slower to compute
                # - Best practice: skip ‡πÉ‡∏ô production (redundant with Spearman)
                
                'warn_high_correlations': False,
            },
            
            'cramers': {
                'calculate': enable_correlations,
                # Cram√©r's V: association between categorical variables
                # - Range: 0 (no association) to 1 (perfect association)
                # - Based on chi-square test
                # - Use case: categorical columns only
                
                'warn_high_correlations': False,
            },
        },
        
        # ==============================================
        # MISSING VALUES CONFIGURATION
        # ==============================================
        'missing_diagrams': {
            'bar': True,
            # bar chart: missing values per column
            # - Visual: easy to spot columns with many nulls
            
            'matrix': True if profiling_mode != 'minimal' else False,
            # matrix: pattern of missing values across rows
            # - Shows: missing data patterns (MCAR, MAR, MNAR)
            # - Expensive: O(n*m) where n=rows, m=columns
            # - Best practice: skip in minimal mode
            
            'heatmap': True if profiling_mode == 'full' else False,
            # heatmap: correlation of missingness
            # - Shows: columns that tend to be missing together
            # - Very expensive: correlation matrix of null indicators
            # - Use case: understand missing data mechanisms
        },
        
        # ==============================================
        # DUPLICATES CONFIGURATION
        # ==============================================
        'duplicates': {
            'head': 10,
            # head: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô duplicate rows ‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô report
            # - Default: 10
            # - Show examples of duplicates ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö investigation
            
            'key': None,
            # key: columns to use for duplicate detection
            # - None: ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å column
            # - List: ['col1', 'col2'] = duplicates ‡∏ï‡∏≤‡∏° columns ‡∏ô‡∏µ‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            # - Best practice:
            #   * None ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö profiling (comprehensive view)
            #   * Specific columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö business logic
        },
        
        # ==============================================
        # SAMPLES CONFIGURATION
        # ==============================================
        'samples': {
            'head': 10,
            # head: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á
            
            'tail': 10,
            # tail: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÅ‡∏™‡∏î‡∏á
            # - Use case: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ data loading ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏´‡∏°
        },
        
        # ==============================================
        # INTERACTIONS CONFIGURATION
        # ==============================================
        'interactions': {
            'continuous': profiling_mode == 'full',
            # continuous: scatter plots between numeric variables
            # - Very expensive: n choose 2 plots
            # - Example: 10 columns ‚Üí 45 plots
            # - Best practice: ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ full mode
            
            'targets': [],
            # targets: specific columns to analyze interactions with
            # - Empty: analyze all pairs
            # - ['target']: analyze only with target variable
        },
        
        # ==============================================
        # PLOTS CONFIGURATION
        # ==============================================
        'plot': {
            'histogram': {
                'bins': 50,
                # bins: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô bins ‡πÉ‡∏ô histogram
                # - More bins = more detail, but noisier
                # - Best practice: 50 for most cases
                
                'max_bins': 250,
                # max_bins: maximum bins for adaptive binning
            },
            
            'scatter_threshold': 1000,
            # scatter_threshold: max points in scatter plot
            # - ‡∏ñ‡πâ‡∏≤‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ ‚Üí sample or use density plot
            # - Best practice: 1000-5000 (readable without overplotting)
        },
        
        # ==============================================
        # HTML REPORT CONFIGURATION
        # ==============================================
        'html': {
            'style': {
                'theme': 'flatly',
                # theme: Bootstrap theme
                # - Options: 'flatly', 'united', 'simplex', etc.
                # - Best practice: 'flatly' (clean, professional)
                
                'logo': None,
                # logo: path to custom logo image
                # - Use case: company branding
                
                'primary_color': '#337ab7',
                # primary_color: main color ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö charts
            },
            
            'minify_html': True,
            # minify_html: compress HTML file
            # - True: smaller file size
            # - False: readable HTML source (for debugging)
            # - Best practice: True ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö production
        },
        
        # ==============================================
        # PERFORMANCE CONFIGURATION
        # ==============================================
        'pool_size': 4,
        # pool_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô worker processes
        # - Default: 0 (single-threaded)
        # - > 0: parallel processing
        # - Best practice: 4 (balance between speed and memory)
        # - Trade-off: more workers = faster but more memory
        
        'progress_bar': False,
        # progress_bar: ‡πÅ‡∏™‡∏î‡∏á progress bar ‡πÉ‡∏ô console
        # - False: cleaner logs (recommended ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Airflow)
        # - True: useful for interactive sessions
    }
    
    # Mode-specific configuration
    if profiling_mode == 'minimal':
        # Minimal mode: fastest, basic stats only
        config.update({
            'correlations': {
                'auto': {'calculate': False},
                'pearson': {'calculate': False},
                'spearman': {'calculate': False},
                'kendall': {'calculate': False},
                'cramers': {'calculate': False},
            },
            'missing_diagrams': {
                'bar': True,
                'matrix': False,
                'heatmap': False,
            },
            'interactions': {
                'continuous': False,
            },
        })
        logger.info("  - Mode: MINIMAL (fast, basic stats only)")
        
    elif profiling_mode == 'explorative':
        # Explorative mode: balanced
        # config already set appropriately above
        logger.info("  - Mode: EXPLORATIVE (balanced, includes correlations)")
        
    elif profiling_mode == 'full':
        # Full mode: comprehensive analysis
        config.update({
            'missing_diagrams': {
                'bar': True,
                'matrix': True,
                'heatmap': True,
            },
            'interactions': {
                'continuous': True,
            },
        })
        logger.info("  - Mode: FULL (comprehensive, slow)")
    
    logger.info(f"‚úÖ Configuration prepared")
    
    # ============================================================
    # STEP 5: GENERATE PROFILE REPORT
    # ============================================================
    logger.info(f"üî¨ [STEP 5/8] Generating ProfileReport (this may take several minutes)...")
    
    profiling_start = time.time()
    
    try:
        # ProfileReport: main class from ydata-profiling
        profile = ProfileReport(
            df=df,
            # df: Pandas DataFrame to profile
            
            minimal=(profiling_mode == 'minimal'),
            # minimal: shortcut for minimal mode
            # - True: override config to minimal settings
            # - False: use custom config
            # - Best practice: use config parameter ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö full control
            
            explorative=(profiling_mode == 'explorative'),
            # explorative: shortcut for explorative mode
            # - Similar to minimal parameter
            
            dark_mode=False,
            # dark_mode: ‡πÉ‡∏ä‡πâ dark theme ‡πÉ‡∏ô HTML report
            # - True: dark background (easier on eyes)
            # - False: light background (better for printing)
            # - Best practice: False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö professional reports
            
            orange_mode=False,
            # orange_mode: special theme (deprecated)
            # - Best practice: ‡πÉ‡∏ä‡πâ config['html']['style']['theme'] ‡πÅ‡∏ó‡∏ô
            
            config=config,
            # config: custom configuration dict
            # - Override default settings
            # - See config structure above
            
            lazy=False,
            # lazy: defer computation until needed
            # - False: compute everything now (default)
            # - True: compute on-demand (for interactive use)
            # - Best practice: False ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö pipelines
            
            type_schema=None,
            # type_schema: override automatic type detection
            # - None: auto-detect types
            # - Dict: {'column': 'Categorical', 'column2': 'Numeric'}
            # - Types: 'Categorical', 'Numeric', 'DateTime', 'Boolean', 'URL', 'Path', 'File', 'Image'
            # - Use case: fix incorrect type detection
# - Example:
#   * ZIP codes detected as Numeric ‚Üí should be Categorical
#   * IDs detected as Numeric ‚Üí should be Categorical
        tsmode=False,
        # tsmode: time series mode
        # - True: additional time series analyses
        # - False: standard profiling (default)
        # - Use case: time series data with datetime index
        
        sortby=None,
        # sortby: column to sort DataFrame by
        # - None: no sorting (keep original order)
        # - String: column name
        # - Use case: sort by timestamp for time series
        
        sample=None,
        # sample: internal sampling parameter
        # - None: use full DataFrame (we already sampled in Step 3)
        # - Dict: sampling configuration
        # - Best practice: do sampling before ProfileReport
    )
    
    profiling_duration = time.time() - profiling_start
    logger.info(f"‚úÖ ProfileReport generated in {profiling_duration:.2f}s")
    
except MemoryError as e:
    error_msg = f"‚ùå Out of memory during profiling: {str(e)}"
    logger.error(error_msg)
    logger.error("üí° Try: reduce sample_size or use profiling_mode='minimal'")
    raise MemoryError(error_msg)
    
except Exception as e:
    error_msg = f"‚ùå Error generating profile: {str(e)}"
    logger.error(error_msg)
    raise Exception(error_msg) from e

# ============================================================
# STEP 6: SAVE REPORTS
# ============================================================
logger.info(f"üíæ [STEP 6/8] Saving reports...")

# Create reports directory
reports_dir = Path('/opt/airflow/data/reports')
reports_dir.mkdir(parents=True, exist_ok=True)

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

html_report_path = None
json_report_path = None

# Save HTML report
if output_format in ['html', 'both']:
    html_filename = f'profile_report_{timestamp}.html'
    html_report_path = reports_dir / html_filename
    
    try:
        # profile.to_file(): save report as file
        profile.to_file(
            output_file=str(html_report_path),
            # output_file: path to save HTML
            
            silent=True,
            # silent: suppress progress messages
            # - True: no console output (cleaner logs)
            # - False: show progress
        )
        
        html_size_mb = html_report_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ HTML report saved: {html_size_mb:.2f} MB")
        logger.info(f"   Path: {html_report_path}")
        
    except Exception as e:
        error_msg = f"‚ùå Error saving HTML report: {str(e)}"
        logger.error(error_msg)
        # Don't raise - continue with JSON

# Save JSON report
if output_format in ['json', 'both']:
    json_filename = f'profile_report_{timestamp}.json'
    json_report_path = reports_dir / json_filename
    
    try:
        # profile.to_json(): export as JSON string
        json_data = profile.to_json()
        # Returns: JSON string containing all profiling results
        # Structure: nested dict with all statistics
        
        # Save to file
        with open(json_report_path, 'w', encoding='utf-8') as f:
            # Write JSON with pretty printing
            json.dump(
                json.loads(json_data),  # Parse string to dict
                # json.loads(): convert JSON string to Python dict
                
                f,
                indent=2,
                # indent: spaces per indentation level
                # - Makes JSON human-readable
                # - Trade-off: larger file size
                # - Best practice: 2 or 4 spaces
                
                ensure_ascii=False,
                # ensure_ascii: allow non-ASCII characters
                # - False: support UTF-8 (Thai, Chinese, etc.)
                # - True: escape non-ASCII (safe but ugly)
                
                sort_keys=False,
                # sort_keys: alphabetically sort dict keys
                # - False: preserve original order (Python 3.7+)
                # - True: consistent ordering (useful for diffs)
            )
        
        json_size_mb = json_report_path.stat().st_size / (1024 * 1024)
        logger.info(f"‚úÖ JSON report saved: {json_size_mb:.2f} MB")
        logger.info(f"   Path: {json_report_path}")
        
    except Exception as e:
        error_msg = f"‚ùå Error saving JSON report: {str(e)}"
        logger.error(error_msg)
        # Don't raise - we have HTML at least

# ============================================================
# STEP 7: EXTRACT KEY METRICS
# ============================================================
logger.info(f"üìà [STEP 7/8] Extracting key metrics...")

# profile.get_description(): get all statistics as dict
description = profile.get_description()
# Returns: comprehensive dict with all profiling results
# Structure:
# {
#     'analysis': {...},
#     'table': {...},         # Dataset-level stats
#     'variables': {...},     # Column-level stats
#     'correlations': {...},
#     'missing': {...},
#     'alerts': [...],
#     'package': {...}
# }

# Extract dataset-level metrics
table_stats = description.get('table', {})

total_rows = table_stats.get('n', len(df))
# n: number of observations (rows)

total_columns = table_stats.get('n_var', len(df.columns))
# n_var: number of variables (columns)

total_cells = total_rows * total_columns

missing_cells = table_stats.get('n_cells_missing', 0)
# n_cells_missing: total missing values across dataset

missing_cells_pct = (missing_cells / total_cells * 100) if total_cells > 0 else 0

duplicate_rows = table_stats.get('n_duplicates', 0)
# n_duplicates: number of duplicate rows

duplicate_rows_pct = (duplicate_rows / total_rows * 100) if total_rows > 0 else 0

memory_size_bytes = table_stats.get('memory_size', 0)
memory_size_mb = memory_size_bytes / (1024 * 1024)

logger.info(f"  - Total rows: {total_rows:,}")
logger.info(f"  - Total columns: {total_columns}")
logger.info(f"  - Missing cells: {missing_cells:,} ({missing_cells_pct:.2f}%)")
logger.info(f"  - Duplicate rows: {duplicate_rows:,} ({duplicate_rows_pct:.2f}%)")
logger.info(f"  - Memory size: {memory_size_mb:.2f} MB")

# Extract column-level metrics
variables = description.get('variables', {})

columns_with_missing = []
high_missing_columns = []  # > 30% missing
high_cardinality_columns = []  # > 50% unique values
constant_columns = []  # only 1 unique value
skewed_columns = []  # |skewness| > 2

for col_name, col_stats in variables.items():
    # Missing values analysis
    n_missing = col_stats.get('n_missing', 0)
    missing_pct = (n_missing / total_rows * 100) if total_rows > 0 else 0
    
    if n_missing > 0:
        columns_with_missing.append({
            'column': col_name,
            'missing_count': n_missing,
            'missing_pct': round(missing_pct, 2)
        })
    
    if missing_pct > 30:
        high_missing_columns.append(col_name)
    
    # Cardinality analysis
    n_distinct = col_stats.get('n_distinct', 0)
    distinct_pct = (n_distinct / total_rows * 100) if total_rows > 0 else 0
    
    if distinct_pct > 50 and n_distinct > 100:
        # High cardinality: >50% unique AND >100 unique values
        # Note: small datasets naturally have high %
        high_cardinality_columns.append({
            'column': col_name,
            'unique_count': n_distinct,
            'unique_pct': round(distinct_pct, 2)
        })
    
    # Constant columns
    if n_distinct == 1:
        constant_columns.append(col_name)
    
    # Skewness analysis (numeric columns only)
    if col_stats.get('type') in ['Numeric', 'Continuous']:
        skewness = col_stats.get('skewness', 0)
        # Skewness interpretation:
        # - |skewness| < 0.5: fairly symmetric
        # - 0.5 < |skewness| < 1: moderately skewed
        # - |skewness| > 1: highly skewed
        # - |skewness| > 2: extremely skewed
        
        if abs(skewness) > 2:
            skewed_columns.append({
                'column': col_name,
                'skewness': round(skewness, 2)
            })

logger.info(f"  - Columns with missing: {len(columns_with_missing)}")
logger.info(f"  - High missing columns (>30%): {len(high_missing_columns)}")
logger.info(f"  - High cardinality columns: {len(high_cardinality_columns)}")
logger.info(f"  - Constant columns: {len(constant_columns)}")
logger.info(f"  - Skewed columns: {len(skewed_columns)}")

# ============================================================
# STEP 8: GENERATE QUALITY FLAGS
# ============================================================
logger.info(f"üö© [STEP 8/8] Generating quality flags...")

flags = {
    'high_missing_data': missing_cells_pct > 10,
    # Flag if > 10% of cells are missing
    # - Threshold: business-dependent
    # - Action: investigate missing data mechanism
    
    'high_duplicate_rate': duplicate_rows_pct > 5,
    # Flag if > 5% rows are duplicates
    # - May indicate data quality issue or legitimate repeats
    # - Action: verify business logic
    
    'constant_columns_exist': len(constant_columns) > 0,
    # Flag if any columns have only 1 unique value
    # - These columns provide no information
    # - Action: consider dropping
    
    'high_cardinality_issues': len(high_cardinality_columns) > total_columns * 0.2,
    # Flag if > 20% columns have high cardinality
    # - May indicate IDs, timestamps, or data quality issues
    # - Action: review column types
    
    'severely_skewed_columns': len(skewed_columns) > 0,
    # Flag if any columns are extremely skewed
    # - May need transformation (log, sqrt, Box-Cox)
    # - Action: consider transformation in cleaning step
    
    'memory_intensive': memory_size_mb > 1000,
    # Flag if dataset > 1GB
    # - May need optimization (sampling, chunking, Dask)
    # - Action: consider memory optimization strategies
}

# Count total flags
flags_raised = sum(flags.values())

if flags_raised == 0:
    logger.info(f"‚úÖ No quality flags raised - data looks good!")
else:
    logger.warning(f"‚ö†Ô∏è  {flags_raised} quality flag(s) raised:")
    for flag_name, flag_value in flags.items():
        if flag_value:
            logger.warning(f"     - {flag_name}")

# ============================================================
# RETURN METADATA
# ============================================================
end_time = time.time()
total_duration = end_time - start_time

metadata = {
    'status': 'success',
    
    # Report paths
    'html_report_path': str(html_report_path) if html_report_path else None,
    'json_report_path': str(json_report_path) if json_report_path else None,
    
    # Dataset metrics
    'total_rows': total_rows,
    'total_columns': total_columns,
    'memory_mb': round(memory_size_mb, 2),
    
    # Missing data metrics
    'missing_cells': missing_cells,
    'missing_cells_pct': round(missing_cells_pct, 2),
    'columns_with_missing': columns_with_missing,
    'high_missing_columns': high_missing_columns,
    
    # Duplicate metrics
    'duplicate_rows': duplicate_rows,
    'duplicate_rows_pct': round(duplicate_rows_pct, 2),
    
    # Column analysis
    'high_cardinality_columns': high_cardinality_columns,
    'constant_columns': constant_columns,
    'skewed_columns': skewed_columns,
    
    # Quality flags
    'flags': flags,
    'flags_raised_count': flags_raised,
    
    # Processing metadata
    'profiling_mode': profiling_mode,
    'profiling_duration_seconds': round(profiling_duration, 2),
    'total_duration_seconds': round(total_duration, 2),
    'sampled': sample_size is not None and original_rows > sample_size,
    'sample_size': len(df) if sample_size else None,
    'original_rows': original_rows,
    'timestamp': datetime.now().isoformat(),
}

logger.info(f"‚úÖ Profiling completed successfully")
logger.info(f"‚è±Ô∏è  Total duration: {total_duration:.2f}s")

return metadata

---

### üéì Key Learnings from Task 2

**1. ydata-profiling Configuration**:
- **Modes**: minimal (fast) vs explorative (balanced) vs full (comprehensive)
- **Correlations**: Expensive O(n¬≤) - enable only when needed
- **Sampling**: Use for large datasets (> 1M rows)
- **Trade-off**: Detail vs Performance

**2. Profiling Metrics to Watch**:
- **Missing data %**: > 10% = investigate pattern
- **Duplicate rate**: > 5% = verify business logic
- **Constant columns**: No information = consider dropping
- **High cardinality**: May be IDs or dirty data
- **Skewness**: |skewness| > 2 = consider transformation

**3. Performance Optimization**:
- Minimal mode: 5-10x faster than full
- Disable correlations: 3-5x faster
- Sampling: linear speedup with sample ratio
- Parallel processing: pool_size=4 recommended

**4. Memory Management**:
- Profiling uses 2-3x data size in RAM
- Large datasets (> 1GB): use sampling or minimal mode
- Monitor peak memory usage
- Consider chunking for very large data

**5. Output Management**:
- **HTML**: For stakeholders (visual, interactive)
- **JSON**: For automation (machine-readable)
- **Both**: Best practice for production
- Manage report retention (can be 10-100 MB each)

---

### ‚úÖ Testing Task 2
```python
# tests/test_profiling.py
import pytest
from dags.tasks.profiling import profile_data

def test_profile_minimal_mode():
    """Test profiling in minimal mode"""
    ingestion_metadata = {
        'output_path': '/opt/airflow/tests/fixtures/sample.parquet',
        'rows': 1000,
        'columns': 10
    }
    
    metadata = profile_data(
        ingestion_metadata=ingestion_metadata,
        profiling_mode='minimal'
    )
    
    assert metadata['status'] == 'success'
    assert metadata['total_rows'] > 0
    assert metadata['html_report_path'] is not None

def test_profile_with_correlations():
    """Test profiling with correlation analysis"""
    ingestion_metadata = {
        'output_path': '/opt/airflow/tests/fixtures/sample.parquet'
    }
    
    metadata = profile_data(
        ingestion_metadata=ingestion_metadata,
        profiling_mode='explorative',
        enable_correlations=True
    )
    
    assert metadata['status'] == 'success'
    # Verify correlations were calculated (check JSON output)

def test_profile_with_sampling():
    """Test profiling with sampling"""
    ingestion_metadata = {
        'output_path': '/opt/airflow/tests/fixtures/large_sample.parquet'
    }
    
    metadata = profile_data(
        ingestion_metadata=ingestion_metadata,
        sample_size=10000
    )
    
    assert metadata['sampled'] == True
    assert metadata['sample_size'] == 10000
```

---

Data Cleaning
üéØ ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å
=============================================================================
TASK 3: DATA CLEANING - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
=============================================================================
‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:

‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Task 1 (Parquet file)
‡πÉ‡∏ä‡πâ insights ‡∏à‡∏≤‡∏Å Task 2 (Profiling) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à cleaning strategies
Handle missing values (imputation, flagging, removal)
Remove/flag duplicates ‡∏ï‡∏≤‡∏° business logic
Fix data types (dates, numerics, categories)
Standardize formats (dates, strings, phone numbers)
Handle outliers (detect ‡πÅ‡∏•‡∏∞ treat)
Create data quality flags
Log cleaning summary ‡∏û‡∏£‡πâ‡∏≠‡∏° before/after comparison

üìä Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

Load data ‡πÅ‡∏•‡∏∞ profiling metadata
Analyze missing values ‚Üí decide strategy per column
Handle duplicates based on business rules
Fix data types (date parsing, numeric conversion)
Standardize string formats
Detect ‡πÅ‡∏•‡∏∞ treat outliers
Create quality flag columns
Validate cleaning results
Save cleaned data
Return cleaning summary

‚ö†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:

Idempotency: ‡∏£‡∏±‡∏ô cleaning ‡∏´‡∏•‡∏≤‡∏¢‡∏£‡∏≠‡∏ö‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡∏ú‡∏•‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°
Data loss tracking: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏•‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
Business logic: cleaning decisions ‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö business requirements
Reversibility: ‡πÄ‡∏Å‡πá‡∏ö backup ‡∏´‡∏£‡∏∑‡∏≠ flag ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏ö (‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ)
Documentation: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å WHY not just WHAT ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏∏‡∏Å cleaning decision


üìù Complete Implementation
python@task(
    # task_id: ‡∏ä‡∏∑‡πà‡∏≠ unique identifier
    task_id='clean_data',
    
    # retries: cleaning ‡∏Ñ‡∏ß‡∏£ retry ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?
    # - Cleaning operations are deterministic
    # - ‡∏ñ‡πâ‡∏≤ fail ‚Üí ‡∏°‡∏±‡∏Å‡πÄ‡∏õ‡πá‡∏ô logic error, data quality issue
    # - Retry ‡πÑ‡∏°‡πà‡∏ä‡πà‡∏ß‡∏¢ ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ 1-2 retries ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö transient I/O errors
    retries=2,
    
    # retry_delay: delay before retry
    # - Cleaning ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏≠‡∏ô‡∏≤‡∏ô (not resource-intensive like profiling)
    retry_delay=timedelta(minutes=2),
    
    # execution_timeout: cleaning ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô
    # - Depends on: data size, number of transformations
    # - Estimation:
    #   * Small (< 100K rows): 2-5 minutes
    #   * Medium (100K-1M rows): 5-15 minutes
    #   * Large (> 1M rows): 15-30 minutes
    execution_timeout=timedelta(minutes=20),
    
    # pool: cleaning ‡πÄ‡∏õ‡πá‡∏ô CPU-intensive task
    # - ‡πÉ‡∏ä‡πâ CPU pool ‡πÅ‡∏ó‡∏ô memory pool
    pool='cpu_intensive',
    
    # pool_slots: cleaning ‡πÑ‡∏°‡πà‡∏Ñ‡πà‡∏≠‡∏¢ intensive
    pool_slots=1,
)
def clean_data(
    ingestion_metadata: dict,
    # ingestion_metadata: metadata ‡∏à‡∏≤‡∏Å Task 1
    # - Required: output_path to Parquet file
    
    profiling_metadata: dict,
    # profiling_metadata: metadata ‡∏à‡∏≤‡∏Å Task 2
    # - ‡πÉ‡∏ä‡πâ insights ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à cleaning strategies
    # - Example: high_missing_columns, skewed_columns
    
    missing_threshold: float = 50.0,
    # missing_threshold: % missing ‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏à‡∏∞ drop column
    # - Type: float (0-100)
    # - Default: 50.0 (drop columns with > 50% missing)
    # - Best practice:
    #   * Conservative: 30-40% (high quality)
    #   * Moderate: 50-60% (balanced)
    #   * Permissive: 70-80% (keep more data)
    # - Business consideration:
    #   * Important columns: keep even with high missing
    #   * Redundant columns: drop even with low missing
    
    duplicate_strategy: str = 'keep_first',
    # duplicate_strategy: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ duplicates
    # - Type: str
    # - Options:
    #   * 'keep_first': ‡πÄ‡∏Å‡πá‡∏ö occurrence ‡πÅ‡∏£‡∏Å (default)
    #   * 'keep_last': ‡πÄ‡∏Å‡πá‡∏ö occurrence ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
    #   * 'keep_all': ‡πÑ‡∏°‡πà‡∏•‡∏ö, flag only
    #   * 'remove_all': ‡∏•‡∏ö‡∏ó‡∏∏‡∏Å occurrences (‡∏£‡∏ß‡∏°‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å)
    # - Best practice:
    #   * Temporal data: 'keep_first' or 'keep_last' (‡∏ï‡∏≤‡∏° business logic)
    #   * Transactions: investigate before removing
    #   * 'keep_all' + flag: safest approach
    
    date_columns: list = None,
    # date_columns: columns ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô dates
    # - Type: list of column names or None
    # - None: auto-detect (‡∏≠‡∏≤‡∏à‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)
    # - Best practice: specify explicitly
    # - Example: ['InvoiceDate', 'OrderDate', 'ShipDate']
    
    date_format: str = None,
    # date_format: format string ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö parse dates
    # - Type: str or None
    # - None: infer format (slower, less reliable)
    # - Format codes:
    #   * %Y: 4-digit year (2024)
    #   * %y: 2-digit year (24)
    #   * %m: month (01-12)
    #   * %d: day (01-31)
    #   * %H: hour (00-23)
    #   * %M: minute (00-59)
    #   * %S: second (00-59)
    # - Examples:
    #   * '%Y-%m-%d': 2024-12-24
    #   * '%d/%m/%Y': 24/12/2024
    #   * '%Y-%m-%d %H:%M:%S': 2024-12-24 14:30:00
    # - Best practice: specify format for consistency and speed
    
    outlier_method: str = 'iqr',
    # outlier_method: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö outliers
    # - Type: str
    # - Options:
    #   * 'iqr': Interquartile Range (Q1-1.5*IQR, Q3+1.5*IQR)
    #   * 'zscore': Z-score method (|z| > 3)
    #   * 'isolation_forest': ML-based (sklearn)
    #   * 'none': skip outlier detection
    # - Comparison:
    #   * IQR: simple, robust, good for skewed data
    #   * Z-score: assumes normal distribution, sensitive to extremes
    #   * Isolation Forest: sophisticated, handles multiple dimensions
    # - Best practice: 'iqr' for most cases
    
    outlier_action: str = 'flag',
    # outlier_action: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ outliers
    # - Type: str
    # - Options:
    #   * 'flag': ‡πÄ‡∏û‡∏¥‡πà‡∏° boolean column (safest)
    #   * 'cap': cap at min/max thresholds (winsorizing)
    #   * 'remove': ‡∏•‡∏ö rows (data loss!)
    #   * 'transform': log/sqrt transformation
    # - Best practice:
    #   * Default: 'flag' (reversible, transparent)
    #   * Modeling: 'cap' or 'transform'
    #   * Never: 'remove' without investigation
    
    categorical_threshold: int = 50,
    # categorical_threshold: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô unique values ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical
    # - Type: int
    # - ‡∏ñ‡πâ‡∏≤ unique values ‚â§ threshold ‚Üí convert to category dtype
    # - Benefits of category dtype:
    #   * Memory savings (significant for large data)
    #   * Faster operations (group by, merge)
    #   * Explicit value set (validation)
    # - Example:
    #   * 10K rows, 'Country' column (50 countries)
    #   * object: 10K strings = ~200KB
    #   * category: 50 strings + 10K codes = ~30KB
    #   * Savings: ~85%
    # - Best practice:
    #   * Low cardinality (< 50): always use category
    #   * Medium (50-1000): consider case by case
    #   * High (> 1000): usually keep as object
    
) -> dict:
    # Return type: cleaning summary metadata
    # Structure:
    # {
    #     'status': str,
    #     'output_path': str,
    #     'rows_before': int,
    #     'rows_after': int,
    #     'rows_removed': int,
    #     'columns_before': int,
    #     'columns_after': int,
    #     'columns_dropped': list,
    #     'missing_values_filled': dict,
    #     'duplicates_removed': int,
    #     'outliers_detected': dict,
    #     'dtypes_changed': dict,
    #     'cleaning_summary': dict,
    #     'duration_seconds': float,
    #     'timestamp': str
    # }
    
    """
    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° best practices ‡πÅ‡∏•‡∏∞ business rules
    
    Args:
        ingestion_metadata: metadata ‡∏à‡∏≤‡∏Å ingestion task
        profiling_metadata: metadata ‡∏à‡∏≤‡∏Å profiling task
        missing_threshold: % missing ‡∏ó‡∏µ‡πà drop column (0-100)
        duplicate_strategy: 'keep_first', 'keep_last', 'keep_all', 'remove_all'
        date_columns: list ‡∏Ç‡∏≠‡∏á column names ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô dates
        date_format: format string ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö parse dates
        outlier_method: 'iqr', 'zscore', 'isolation_forest', 'none'
        outlier_action: 'flag', 'cap', 'remove', 'transform'
        categorical_threshold: max unique values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö category dtype
    
    Returns:
        Dict containing:
        - status: success/failed
        - output_path: path to cleaned Parquet
        - cleaning statistics: rows/columns before/after
        - transformation details: what changed
        - duration: processing time
        
    Raises:
        FileNotFoundError: ‡∏ñ‡πâ‡∏≤ input Parquet ‡πÑ‡∏°‡πà‡∏û‡∏ö
        ValueError: ‡∏ñ‡πâ‡∏≤ parameters ‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
        KeyError: ‡∏ñ‡πâ‡∏≤ specified columns ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô data
    """
    
    import pandas as pd
    import numpy as np
    import logging
    from pathlib import Path
    from datetime import datetime
    import time
    from scipy import stats
    
    # ============================================================
    # STEP 1: SETUP AND VALIDATION
    # ============================================================
    logger = logging.getLogger(__name__)
    logger.info("üöÄ [STEP 1/10] Starting data cleaning task")
    
    start_time = time.time()
    
    # Validate metadata
    if not ingestion_metadata or 'output_path' not in ingestion_metadata:
        error_msg = "‚ùå Invalid ingestion metadata"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    # Validate parameters
    if not 0 <= missing_threshold <= 100:
        error_msg = f"‚ùå missing_threshold must be 0-100, got {missing_threshold}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    valid_dup_strategies = ['keep_first', 'keep_last', 'keep_all', 'remove_all']
    if duplicate_strategy not in valid_dup_strategies:
        error_msg = f"‚ùå Invalid duplicate_strategy: {duplicate_strategy}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    valid_outlier_methods = ['iqr', 'zscore', 'isolation_forest', 'none']
    if outlier_method not in valid_outlier_methods:
        error_msg = f"‚ùå Invalid outlier_method: {outlier_method}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    valid_outlier_actions = ['flag', 'cap', 'remove', 'transform']
    if outlier_action not in valid_outlier_actions:
        error_msg = f"‚ùå Invalid outlier_action: {outlier_action}"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    logger.info(f"‚úÖ Configuration validated")
    
    # ============================================================
    # STEP 2: LOAD DATA
    # ============================================================
    logger.info(f"üìñ [STEP 2/10] Loading data from Parquet...")
    
    parquet_path = Path(ingestion_metadata['output_path'])
    
    if not parquet_path.exists():
        error_msg = f"‚ùå Parquet file not found: {parquet_path}"
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        df = pd.read_parquet(parquet_path, engine='pyarrow')
        logger.info(f"‚úÖ Data loaded: {len(df):,} rows √ó {len(df.columns)} columns")
        
    except Exception as e:
        error_msg = f"‚ùå Error loading Parquet: {str(e)}"
        logger.error(error_msg)
        raise
    
    # Store original counts for comparison
    rows_before = len(df)
    columns_before = len(df.columns)
    original_columns = df.columns.tolist()
    
    # Initialize tracking dictionaries
    cleaning_summary = {
        'missing_values_handled': {},
        'duplicates_info': {},
        'outliers_info': {},
        'dtypes_changed': {},
        'columns_dropped': [],
        'transformations': []
    }
    
    # ============================================================
    # STEP 3: HANDLE MISSING VALUES
    # ============================================================
    logger.info(f"üîç [STEP 3/10] Handling missing values...")
    
    # Calculate missing percentage per column
    missing_stats = df.isnull().sum()
    # df.isnull(): return DataFrame of boolean (True = missing)
    # .sum(): count True values per column
    # Result: Series with column names and missing counts
    
    missing_pct = (missing_stats / len(df) * 100).round(2)
    # Calculate percentage and round to 2 decimals
    
    # Log columns with missing values
    cols_with_missing = missing_stats[missing_stats > 0]
    if len(cols_with_missing) > 0:
        logger.info(f"  - Found {len(cols_with_missing)} columns with missing values:")
        for col in cols_with_missing.index[:10]:  # Show first 10
            logger.info(f"    * {col}: {missing_stats[col]:,} ({missing_pct[col]:.2f}%)")
        if len(cols_with_missing) > 10:
            logger.info(f"    ... and {len(cols_with_missing) - 10} more")
    else:
        logger.info(f"  - No missing values found!")
    
    # Strategy 1: Drop columns with excessive missing values
    # --------------------------------------------------------
    cols_to_drop = missing_pct[missing_pct > missing_threshold].index.tolist()
    # missing_pct > threshold: boolean Series
    # .index: get column names
    # .tolist(): convert to list
    
    if cols_to_drop:
        logger.info(f"  - Dropping {len(cols_to_drop)} columns with >{missing_threshold}% missing:")
        for col in cols_to_drop:
            logger.info(f"    * {col}: {missing_pct[col]:.2f}% missing")
            cleaning_summary['columns_dropped'].append({
                'column': col,
                'reason': f'>{missing_threshold}% missing',
                'missing_pct': missing_pct[col]
            })
        
        df = df.drop(columns=cols_to_drop)
        # df.drop(): remove columns
        # - columns: list of column names to drop
        # - axis: 0 (rows) or 1 (columns) - use columns= instead
        # - inplace: False (return new DataFrame) or True (modify in place)
        # - Best practice: inplace=False (functional style, safer)
        
        logger.info(f"  - Columns after drop: {len(df.columns)}")
    
    # Strategy 2: Fill numeric columns with median
    # --------------------------------------------------------
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    # df.select_dtypes(): select columns by data type
    # - include: list of types to include
    #   * np.number: all numeric types (int, float)
    #   * 'object': strings
    #   * 'datetime64': datetimes
    #   * 'category': categorical
    # - exclude: list of types to exclude
    # - Returns: DataFrame with selected columns
    # - .columns: get column names only
    
    numeric_cols_with_missing = [col for col in numeric_cols if df[col].isnull().any()]
    # List comprehension: filter numeric columns that have missing values
    # df[col].isnull().any(): True if column has any missing values
    
    if numeric_cols_with_missing:
        logger.info(f"  - Filling {len(numeric_cols_with_missing)} numeric columns with median:")
        
        for col in numeric_cols_with_missing:
            missing_count = df[col].isnull().sum()
            
            # Calculate median (ignoring NaN)
            median_value = df[col].median()
            # df[col].median(): calculate median
            # - Automatically ignores NaN values
            # - Returns: scalar value
            # - Alternative methods:
            #   * .mean(): arithmetic mean (sensitive to outliers)
            #   * .mode(): most frequent value
            #   * Forward fill: .ffill() (time series)
            #   * Backward fill: .bfill() (time series)
            
            # Why median vs mean?
            # - Median: robust to outliers (recommended)
            # - Mean: affected by outliers, can be misleading
            # - Example: [1, 2, 3, 4, 1000]
            #   * Mean: 202 (skewed by 1000)
            #   * Median: 3 (representative)
            
            # Fill missing values
            df[col] = df[col].fillna(median_value)
            # df[col].fillna(): fill missing values
            # - value: scalar, dict, Series, or DataFrame
            # - method: 'ffill' (forward fill), 'bfill' (backward fill)
            # - inplace: False (default) or True
            # - limit: maximum number of consecutive NaNs to fill
            # - Best practice: assign back to column (inplace=False)
            
            logger.info(f"    * {col}: filled {missing_count:,} values with median={median_value:.2f}")
            
            cleaning_summary['missing_values_handled'][col] = {
                'method': 'median',
                'count': int(missing_count),
                'fill_value': float(median_value)
            }
    
    # Strategy 3: Fill categorical columns with mode or 'Unknown'
    # --------------------------------------------------------
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns
    categorical_cols_with_missing = [col for col in categorical_cols if df[col].isnull().any()]
    
    if categorical_cols_with_missing:
        logger.info(f"  - Filling {len(categorical_cols_with_missing)} categorical columns:")
        
        for col in categorical_cols_with_missing:
            missing_count = df[col].isnull().sum()
            
            # Calculate mode (most frequent value)
            mode_values = df[col].mode()
            # df[col].mode(): return Series of most frequent value(s)
            # - Can return multiple values if there's a tie
            # - Returns: Series (even if single value)
            
            if len(mode_values) > 0:
                # Use mode if exists
                fill_value = mode_values[0]
                method = 'mode'
            else:
                # Use 'Unknown' if no mode (shouldn't happen, but safe)
                fill_value = 'Unknown'
                method = 'constant'
            
            df[col] = df[col].fillna(fill_value)
            
            logger.info(f"    * {col}: filled {missing_count:,} values with {method}='{fill_value}'")
            
            cleaning_summary['missing_values_handled'][col] = {
                'method': method,
                'count': int(missing_count),
                'fill_value': str(fill_value)
            }
    
    # Strategy 4: Create missing value flags
    # --------------------------------------------------------
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö columns ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: flag ‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£ fill
    # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ model ‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô imputed
    
    if profiling_metadata and 'high_missing_columns' in profiling_metadata:
        high_missing_cols = profiling_metadata['high_missing_columns']
        # Get columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing ‡∏™‡∏π‡∏á‡∏à‡∏≤‡∏Å profiling
        
        for col in high_missing_cols:
            if col in df.columns:
                # Create flag column
                flag_col_name = f'{col}_was_missing'
                df[flag_col_name] = df[col].isnull().astype(int)
                # .isnull(): return boolean Series
                # .astype(int): convert True‚Üí1, False‚Üí0
                # Reason: boolean flags ‡πÉ‡∏ä‡πâ memory ‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ int
                # ‡πÅ‡∏ï‡πà int ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CSV export ‡πÅ‡∏•‡∏∞ compatibility
                
                logger.info(f"    * Created flag: {flag_col_name}")
                
                cleaning_summary['transformations'].append({
                    'type': 'create_flag',
                    'column': flag_col_name,
                    'source': col
                })
    
    logger.info(f"‚úÖ Missing values handled")
    
    # ============================================================
    # STEP 4: HANDLE DUPLICATES
    # ============================================================
    logger.info(f"üîÑ [STEP 4/10] Handling duplicate rows...")
    
    # Count duplicates
    duplicate_mask = df.duplicated(keep=False)
    # df.duplicated(): identify duplicate rows
    # - subset: columns to consider (None = all columns)
    # - keep: which duplicates to mark
    #   * 'first': mark all except first occurrence (default)
    #   * 'last': mark all except last occurrence
    #   * False: mark all duplicates (including first)
    # - Returns: boolean Series (True = duplicate)
    
    n_duplicates = duplicate_mask.sum()
    duplicate_pct = (n_duplicates / len(df) * 100) if len(df) > 0 else 0
    
    logger.info(f"  - Found {n_duplicates:,} duplicate rows ({duplicate_pct:.2f}%)")
    
    if n_duplicates > 0:
        # Show examples of duplicates
        if n_duplicates <= 10:
            logger.info(f"  - Duplicate examples:")
            duplicate_examples = df[duplicate_mask].head(5)
            for idx, row in duplicate_examples.iterrows():
                logger.info(f"    * Index {idx}: {row.to_dict()}")
        
        # Apply duplicate strategy
        if duplicate_strategy == 'keep_first':
            # Keep first occurrence, remove rest
            df = df.drop_duplicates(keep='first')
            # df.drop_duplicates(): remove duplicate rows
            # - Returns: DataFrame without duplicates
            # - subset: columns to consider
            # - keep: 'first', 'last', or False
            # - inplace: False or True
            
            rows_removed = rows_before - len(df)
            logger.info(f"  - Removed {rows_removed:,} duplicate rows (kept first)")
            
        elif duplicate_strategy == 'keep_last':
            df = df.drop_duplicates(keep='last')
            rows_removed = rows_before - len(df)
            logger.info(f"  - Removed {rows_removed:,} duplicate rows (kept last)")
            
        elif duplicate_strategy == 'keep_all':
            # Don't remove, just flag
            df['is_duplicate'] = duplicate_mask.astype(int)
            rows_removed = 0
            logger.info(f"  - Flagged {n_duplicates:,} duplicates (kept all)")
            
            cleaning_summary['transformations'].append({
                'type': 'create_flag',
                'column': 'is_duplicate',
                'count': int(n_duplicates)
            })
            
        elif duplicate_strategy == 'remove_all':
            # Remove all occurrences (including first)
            df = df[~duplicate_mask]
            # ~duplicate_mask: invert boolean (True‚ÜíFalse, False‚ÜíTrue)
            # df[boolean_series]: filter rows where True
            
            rows_removed = n_duplicates
            logger.info(f"  - Removed {rows_removed:,} rows (all duplicates)")
        
        cleaning_summary['duplicates_info'] = {
            'found': int(n_duplicates),
            'strategy': duplicate_strategy,
            'removed': int(rows_removed) if duplicate_strategy != 'keep_all' else 0
        }
    else:
        logger.info(f"  - No duplicates found")
        cleaning_summary['duplicates_info'] = {
            'found': 0,
            'strategy': duplicate_strategy,
            'removed': 0
        }
    
    logger.info(f"‚úÖ Duplicates handled")
    
    # ============================================================
    # STEP 5: FIX DATA TYPES - DATES
    # ============================================================
    logger.info(f"üìÖ [STEP 5/10] Converting date columns...")
    
    if date_columns is None:
        # Auto-detect date columns
        # Look for columns with 'date', 'time', 'timestamp' in name
        potential_date_cols = [col for col in df.columns 
                              if any(keyword in col.lower() 
                                    for keyword in ['date', 'time', 'timestamp'])]
        date_columns = potential_date_cols
        
        if date_columns:
            logger.info(f"  - Auto-detected {len(date_columns)} potential date columns:")
            for col in date_columns:
                logger.info(f"    * {col}")
    
    if date_columns:
        for col in date_columns:
            if col not in df.columns:
                logger.warning(f"  ‚ö†Ô∏è  Column '{col}' not found in data, skipping")
                continue
            
            # Skip if already datetime
            if pd.api.types.is_datetime64_any_dtype(df[col]):
                # pd.api.types.is_datetime64_any_dtype(): check if datetime
                # - Returns: True if column is datetime type
                # - Alternative: df[col].dtype == 'datetime64[ns]'
                logger.info(f"  - {col}: already datetime, skipping")
                continue
            
            logger.info(f"  - Converting {col} to datetime...")
            
            original_dtype = str(df[col].dtype)
            
            try:
                # pd.to_datetime(): convert to datetime
                df[col] = pd.to_datetime(
                    df[col],
                    # arg: scalar, list, Series, DataFrame
                    
                    format=date_format,
                    # format: strftime format string
                    # - None: infer format (slower)
                    # - String: exact format (faster, more reliable)
                    # - Examples:
                    #   * '%Y-%m-%d': 2024-12-24
                    #   * '%d/%m/%Y %H:%M': 24/12/2024 14:30
                    # - Best practice: specify format if known
                    
                    errors='coerce',
                    # errors: how to handle parsing errors
                    # - 'raise': raise exception on error
                    # - 'coerce': invalid dates ‚Üí NaT (Not a Time)
                    # - 'ignore': return original data on error
                    # - Best practice: 'coerce' (convert what you can, flag rest)
                    
                    dayfirst=False,
                    # dayfirst: interpret first value as day?
                    # - False: MM/DD/YYYY (US format)
                    # - True: DD/MM/YYYY (European format)
                    # - Only affects ambiguous dates (e.g., 01/02/2024)
                    # - Best practice: use format parameter instead
                    
                    infer_datetime_format=True,
                    # infer_datetime_format: auto-detect format
                    # - True: try to infer format (faster if consistent)
                    # - False: parse each value individually
                    # - Deprecated in Pandas 2.0
                    # - Best practice: specify format explicitly
                    
                    utc=False,
                    # utc: convert to UTC timezone
                    # - False: keep timezone-naive (default)
                    # - True: convert to UTC
                    # - Use case: when working with multiple timezones
                )
                
                # Count invalid dates (converted to NaT)
                nat_count = df[col].isnull().sum()
                if nat_count > 0:
                    logger.warning(f"    ‚ö†Ô∏è  {nat_count:,} invalid dates converted to NaT")
                
                logger.info(f"    ‚úÖ Converted: {original_dtype} ‚Üí datetime64[ns]")
                
                cleaning_summary['dtypes_changed'][col] = {
                    'from': original_dtype,
                    'to': 'datetime64[ns]',
                    'invalid_count': int(nat_count)
                }
                
            except Exception as e:
                logger.error(f"    ‚ùå Error converting {col}: {str(e)}")
                logger.error(f"       Sample values: {df[col].head(3).tolist()}")
                # Don't raise - continue with other columns
    else:
        logger.info(f"  - No date columns specified")
    
    logger.info(f"‚úÖ Date conversion completed")
    
    # ============================================================
    # STEP 6: FIX DATA TYPES - NUMERIC COLUMNS
    # ============================================================
    logger.info(f"üî¢ [STEP 6/10] Converting numeric columns...")
    
    # Find object columns that might be numeric
    object_cols = df.select_dtypes(include=['object']).columns
for col in object_cols:
    # Try to convert to numeric
    # Skip if column name suggests it's categorical
    if any(keyword in col.lower() for keyword in ['id', 'code', 'category', 'type', 'name']):
        continue
    
    # Sample a few values to check if numeric
    sample = df[col].dropna().head(100)
    if len(sample) == 0:
        continue
    
    # Try conversion on sample
    try:
        pd.to_numeric(sample, errors='raise')
        # If successful, convert entire column
        
        logger.info(f"  - Converting {col} to numeric...")
        original_dtype = str(df[col].dtype)
        
        df[col] = pd.to_numeric(
            df[col],
            # arg: scalar, list, Series
            
            errors='coerce',
            # errors: how to handle non-numeric values
            # - 'raise': raise exception
            # - 'coerce': non-numeric ‚Üí NaN
            # - 'ignore': return original if error
            # - Best practice: 'coerce' (convert what you can)
            
            downcast=None,
            # downcast: convert to smaller dtype
            # - None: no downcasting (default)
            # - 'integer': try int8, int16, int32, int64
            # - 'signed': try signed integers
            # - 'unsigned': try unsigned integers
            # - 'float': try float32, float64
            # - Benefits: memory savings
            # - Trade-off: potential overflow
            # - Best practice: None (safety over memory)
        )
        
        # Count non-numeric values
        nan_count = df[col].isnull().sum()
        if nan_count > 0:
            logger.warning(f"    ‚ö†Ô∏è  {nan_count:,} non-numeric values converted to NaN")
        
        logger.info(f"    ‚úÖ Converted: {original_dtype} ‚Üí {df[col].dtype}")
        
        cleaning_summary['dtypes_changed'][col] = {
            'from': original_dtype,
            'to': str(df[col].dtype),
            'invalid_count': int(nan_count)
        }
        
    except:
        # Not numeric, skip
        pass

logger.info(f"‚úÖ Numeric conversion completed")

# ============================================================
# STEP 7: OPTIMIZE CATEGORICAL COLUMNS
# ============================================================
logger.info(f"üè∑Ô∏è  [STEP 7/10] Optimizing categorical columns...")

# Find low-cardinality object columns
object_cols = df.select_dtypes(include=['object']).columns

categorical_converted = 0

for col in object_cols:
    n_unique = df[col].nunique()
    # df[col].nunique(): count unique values
    # - dropna: True (default) - exclude NaN from count
    # - Returns: int
    
    if n_unique <= categorical_threshold and n_unique > 1:
        # Convert to category if:
        # - Unique values ‚â§ threshold
        # - More than 1 unique value (not constant)
        
        memory_before = df[col].memory_usage(deep=True)
        # memory_usage(deep=True): actual memory including strings
        # - deep=False: only pointer size
        # - deep=True: full object size (accurate)
        
        df[col] = df[col].astype('category')
        # .astype('category'): convert to categorical dtype
        # - Pandas stores: categories + codes
        # - categories: unique values
        # - codes: integer mapping to categories
        # - Example:
        #   * Values: ['A', 'B', 'A', 'A', 'B']
        #   * Categories: ['A', 'B']
        #   * Codes: [0, 1, 0, 0, 1]
        
        memory_after = df[col].memory_usage(deep=True)
        memory_saved = memory_before - memory_after
        memory_saved_pct = (memory_saved / memory_before * 100) if memory_before > 0 else 0
        
        logger.info(f"  - {col}: {n_unique} unique values")
        logger.info(f"    Memory: {memory_before/1024:.1f}KB ‚Üí {memory_after/1024:.1f}KB ({memory_saved_pct:.1f}% saved)")
        
        categorical_converted += 1
        
        cleaning_summary['dtypes_changed'][col] = {
            'from': 'object',
            'to': 'category',
            'unique_values': int(n_unique),
            'memory_saved_kb': round(memory_saved / 1024, 2)
        }

logger.info(f"‚úÖ Converted {categorical_converted} columns to category dtype")

# ============================================================
# STEP 8: STANDARDIZE STRING FORMATS
# ============================================================
logger.info(f"‚úèÔ∏è  [STEP 8/10] Standardizing string formats...")

# Get remaining object columns (not converted to category)
string_cols = df.select_dtypes(include=['object']).columns

for col in string_cols:
    if df[col].dtype == 'object':  # Skip if already processed
        logger.info(f"  - Standardizing {col}...")
        
        # Strip whitespace
        # df[col].str: string accessor for vectorized string operations
        # - Only works on object/string columns
        # - Methods: .lower(), .upper(), .strip(), .replace(), etc.
        df[col] = df[col].str.strip()
        # .strip(): remove leading/trailing whitespace
        # - Equivalent to: df[col].apply(lambda x: x.strip() if isinstance(x, str) else x)
        # - Much faster than .apply()
        
        # Convert to lowercase (optional, based on use case)
        # df[col] = df[col].str.lower()
        
        cleaning_summary['transformations'].append({
            'type': 'standardize_strings',
            'column': col,
            'operations': ['strip_whitespace']
        })

logger.info(f"‚úÖ String standardization completed")

# ============================================================
# STEP 9: HANDLE OUTLIERS
# ============================================================
logger.info(f"üìä [STEP 9/10] Detecting and handling outliers...")

if outlier_method != 'none':
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    for col in numeric_cols:
        # Skip columns that are likely IDs or counts
        if any(keyword in col.lower() for keyword in ['id', 'count', 'quantity']):
            continue
        
        logger.info(f"  - Analyzing {col}...")
        
        if outlier_method == 'iqr':
            # IQR Method (Interquartile Range)
            # --------------------------------
            Q1 = df[col].quantile(0.25)
            # quantile(0.25): 25th percentile (Q1)
            # - 25% of data ‚â§ Q1
            
            Q3 = df[col].quantile(0.75)
            # quantile(0.75): 75th percentile (Q3)
            # - 75% of data ‚â§ Q3
            
            IQR = Q3 - Q1
            # IQR: Interquartile Range
            # - Middle 50% of data
            # - Robust to outliers (unlike std)
            
            # Define outlier boundaries
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            # 1.5 * IQR: standard multiplier
            # - Tukey's fences for outlier detection
            # - 1.5: moderate outliers
            # - 3.0: extreme outliers
            # - Values outside [lower, upper] = outliers
            
            outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)
            # |: logical OR
            # &: logical AND
            # ~: logical NOT
            
        elif outlier_method == 'zscore':
            # Z-score Method
            # --------------
            # z = (x - mean) / std
            # |z| > 3: outlier (99.7% within ¬±3œÉ in normal distribution)
            
            z_scores = np.abs(stats.zscore(df[col], nan_policy='omit'))
            # stats.zscore(): calculate z-scores
            # - nan_policy: 'propagate', 'raise', or 'omit'
            # - 'omit': ignore NaN values
            # - np.abs(): absolute value (direction doesn't matter)
            
            outlier_mask = z_scores > 3
            # Threshold: 3 standard deviations
            # - 3œÉ: 99.7% confidence
            # - 2.5œÉ: 99.0% confidence (more sensitive)
            
        else:
            # Other methods (isolation_forest) can be added here
            continue
        
        n_outliers = outlier_mask.sum()
        outlier_pct = (n_outliers / len(df) * 100) if len(df) > 0 else 0
        
        if n_outliers > 0:
            logger.info(f"    Found {n_outliers:,} outliers ({outlier_pct:.2f}%)")
            
            if outlier_action == 'flag':
                # Create flag column
                flag_col = f'{col}_is_outlier'
                df[flag_col] = outlier_mask.astype(int)
                logger.info(f"    Created flag: {flag_col}")
                
            elif outlier_action == 'cap':
                # Cap outliers at boundaries (winsorizing)
                if outlier_method == 'iqr':
                    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
                    # .clip(): limit values to range
                    # - lower: minimum value
                    # - upper: maximum value
                    # - Values < lower ‚Üí lower
                    # - Values > upper ‚Üí upper
                    logger.info(f"    Capped values to [{lower_bound:.2f}, {upper_bound:.2f}]")
                
            elif outlier_action == 'remove':
                # Remove outlier rows (data loss!)
                df = df[~outlier_mask]
                logger.info(f"    Removed {n_outliers:,} rows with outliers")
                
            elif outlier_action == 'transform':
                # Log transformation
                # Only if all values > 0
                if (df[col] > 0).all():
                    df[col] = np.log1p(df[col])
                    # np.log1p(x): log(1 + x)
                    # - Handles x=0 gracefully (log(1)=0)
                    # - Use np.expm1() to inverse: expm1(log1p(x)) = x
                    logger.info(f"    Applied log transformation")
                else:
                    logger.warning(f"    ‚ö†Ô∏è  Cannot transform (has negative/zero values)")
            
            cleaning_summary['outliers_info'][col] = {
                'method': outlier_method,
                'action': outlier_action,
                'count': int(n_outliers),
                'percentage': round(outlier_pct, 2)
            }
else:
    logger.info(f"  - Outlier detection skipped")

logger.info(f"‚úÖ Outlier handling completed")

# ============================================================
# STEP 10: SAVE CLEANED DATA
# ============================================================
logger.info(f"üíæ [STEP 10/10] Saving cleaned data...")

# Create output directory
output_dir = Path('/opt/airflow/data/processed')
output_dir.mkdir(parents=True, exist_ok=True)

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
output_filename = f'cleaned_{timestamp}.parquet'
output_path = output_dir / output_filename

try:
    df.to_parquet(
        path=str(output_path),
        engine='pyarrow',
        compression='snappy',
        index=False
    )
    
    output_size_mb = output_path.stat().st_size / (1024 * 1024)
    logger.info(f"‚úÖ Cleaned data saved: {output_size_mb:.2f} MB")
    logger.info(f"   Path: {output_path}")
    
except Exception as e:
    error_msg = f"‚ùå Error saving cleaned data: {str(e)}"
    logger.error(error_msg)
    raise

# ============================================================
# FINAL SUMMARY
# ============================================================
end_time = time.time()
duration = end_time - start_time

rows_after = len(df)
columns_after = len(df.columns)

logger.info(f"")
logger.info(f"{'='*60}")
logger.info(f"CLEANING SUMMARY")
logger.info(f"{'='*60}")
logger.info(f"Rows:    {rows_before:,} ‚Üí {rows_after:,} ({rows_before - rows_after:,} removed)")
logger.info(f"Columns: {columns_before} ‚Üí {columns_after} ({columns_before - columns_after} removed)")
logger.info(f"Duration: {duration:.2f}s")
logger.info(f"{'='*60}")

# Create metadata
metadata = {
    'status': 'success',
    'output_path': str(output_path),
    
    # Before/After comparison
    'rows_before': rows_before,
    'rows_after': rows_after,
    'rows_removed': rows_before - rows_after,
    'columns_before': columns_before,
    'columns_after': columns_after,
    'columns_removed': columns_before - columns_after,
    
    # Detailed cleaning summary
    'cleaning_summary': cleaning_summary,
    
    # Processing metadata
    'duration_seconds': round(duration, 2),
    'timestamp': datetime.now().isoformat(),
    
    # Final columns list
    'final_columns': df.columns.tolist(),
}

logger.info(f"‚úÖ Data cleaning completed successfully!")

return metadata

---

### üéì Key Learnings from Task 3

**1. Missing Values Strategy**:
- **Drop columns**: > threshold % missing
- **Numeric columns**: fill with median (robust to outliers)
- **Categorical columns**: fill with mode or 'Unknown'
- **Important columns**: flag instead of fill

**2. Duplicate Handling**:
- **keep_first/keep_last**: temporal logic
- **keep_all + flag**: safest approach
- **remove_all**: only after investigation

**3. Data Type Conversion**:
- **Dates**: use pd.to_datetime() with explicit format
- **Numerics**: pd.to_numeric() with errors='coerce'
- **Categories**: convert low-cardinality columns (memory savings)

**4. Outlier Detection**:
- **IQR method**: robust, good for skewed data
- **Z-score**: assumes normality
- **Action**: flag > cap > transform > remove

**5. Best Practices**:
- **Idempotency**: cleaning should be reproducible
- **Transparency**: log all transformations
- **Reversibility**: flag instead of remove when possible
- **Documentation**: track WHY not just WHAT

---

Task 4: Data Validation
üéØ ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å
=============================================================================
TASK 4: DATA VALIDATION - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á Cleaning
=============================================================================
‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:

‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà cleaned ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å Task 3
Validate schema (columns, data types)
Check data ranges (min/max values)
Validate business rules (cross-column logic)
Check referential integrity (foreign keys)
Statistical validation (distributions, anomalies)
Completeness checks (required fields)
Generate validation report ‡∏û‡∏£‡πâ‡∏≠‡∏° severity levels
Decide pass/fail criteria
Return validation results

üìä Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

Load cleaned data
Schema validation (columns, types)
Range validation (min/max, allowed values)
Business rules validation (domain logic)
Statistical validation (distributions)
Completeness validation (required fields)
Generate detailed report
Aggregate by severity (CRITICAL, WARNING, INFO)
Decide overall pass/fail
Return validation metadata

‚ö†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:

Validation vs Cleaning: Validation ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö, ‡πÑ‡∏°‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
Fail Fast: CRITICAL errors ‡∏Ñ‡∏ß‡∏£ fail pipeline ‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
Alert Fatigue: ‡∏°‡∏µ warnings ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí ‡∏ñ‡∏π‡∏Å‡∏•‡∏∞‡πÄ‡∏•‡∏¢
False Positives: validation rules ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí reject good data
Performance: validation ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏£‡πá‡∏ß (‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏ä‡πâ‡∏≤‡∏Å‡∏ß‡πà‡∏≤ cleaning)


üìù Complete Implementation
python@task(
    # task_id: ‡∏ä‡∏∑‡πà‡∏≠ task
    task_id='validate_data',
    
    # retries: validation ‡∏Ñ‡∏ß‡∏£ retry ‡∏ô‡πâ‡∏≠‡∏¢
    # - Validation ‡πÄ‡∏õ‡πá‡∏ô deterministic operation
    # - ‡∏ñ‡πâ‡∏≤ fail ‚Üí data ‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà transient error)
    # - Retry ‡πÄ‡∏û‡∏∑‡πà‡∏≠ handle I/O errors ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    retries=1,
    
    # retry_delay: delay ‡∏™‡∏±‡πâ‡∏ô
    retry_delay=timedelta(minutes=1),
    
    # execution_timeout: validation ‡∏Ñ‡∏ß‡∏£‡πÄ‡∏£‡πá‡∏ß
    # - ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤ cleaning (‡πÑ‡∏°‡πà‡∏°‡∏µ transformation)
    # - Estimation:
    #   * Small (< 100K rows): 1-2 minutes
    #   * Medium (100K-1M rows): 2-5 minutes
    #   * Large (> 1M rows): 5-10 minutes
    execution_timeout=timedelta(minutes=15),
    
    # pool: validation ‡πÄ‡∏õ‡πá‡∏ô CPU-bound
    pool='cpu_intensive',
    
    pool_slots=1,
    
    # trigger_rule: ‡∏Ñ‡∏ß‡∏£ trigger ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
    # - Default: 'all_success' (all upstream tasks must succeed)
    # - Options:
    #   * 'all_success': ‡∏£‡∏±‡∏ô‡∏ñ‡πâ‡∏≤ upstream tasks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î success
    #   * 'all_failed': ‡∏£‡∏±‡∏ô‡∏ñ‡πâ‡∏≤ upstream tasks ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î failed
    #   * 'all_done': ‡∏£‡∏±‡∏ô‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤ upstream ‡∏à‡∏∞ success/failed
    #   * 'one_success': ‡∏£‡∏±‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ upstream ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 success
    #   * 'one_failed': ‡∏£‡∏±‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ upstream ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 1 failed
    #   * 'none_failed': ‡∏£‡∏±‡∏ô‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ upstream failed (allow skipped)
    # - Best practice: 'all_success' (default)
    # - Use case: validation ‡∏Ñ‡∏ß‡∏£‡∏£‡∏±‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠ cleaning ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    trigger_rule='all_success',
)
def validate_data(
    cleaning_metadata: dict,
    # cleaning_metadata: metadata ‡∏à‡∏≤‡∏Å Task 3
    # - Required: output_path to cleaned Parquet
    
    expected_columns: list = None,
    # expected_columns: list ‡∏Ç‡∏≠‡∏á column names ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
    # - Type: list of strings or None
    # - None: ‡πÑ‡∏°‡πà validate schema (flexible)
    # - List: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ columns ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ (strict)
    # - Best practice: 
    #   * Development: None (flexible)
    #   * Production: specify explicitly (prevent schema drift)
    # - Example: ['customer_id', 'order_date', 'amount', 'country']
    
    required_columns: list = None,
    # required_columns: columns ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏° missing ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    # - Type: list of strings or None
    # - Subset of expected_columns
    # - Business-critical columns
    # - Example: ['customer_id', 'order_date', 'amount']
    # - Validation: every row must have non-null value
    
    column_types: dict = None,
    # column_types: expected data types
    # - Type: dict or None
    # - Format: {'column_name': 'expected_type'}
    # - Type strings:
    #   * 'int64', 'float64', 'object', 'category'
    #   * 'datetime64[ns]', 'bool'
    # - Example: {'customer_id': 'object', 'amount': 'float64'}
    # - Best practice: specify critical columns only
    
    numeric_ranges: dict = None,
    # numeric_ranges: min/max values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns
    # - Type: dict or None
    # - Format: {'column_name': {'min': value, 'max': value}}
    # - Example: {
    #     'age': {'min': 18, 'max': 120},
    #     'amount': {'min': 0, 'max': 1000000}
    #   }
    # - Validation: all values must be within range
    # - Use case: prevent unrealistic values
    
    date_ranges: dict = None,
    # date_ranges: min/max dates
    # - Type: dict or None
    # - Format: {'column_name': {'min': 'YYYY-MM-DD', 'max': 'YYYY-MM-DD'}}
    # - Example: {
    #     'order_date': {'min': '2020-01-01', 'max': '2025-12-31'}
    #   }
    # - Validation: dates must be within range
    
    categorical_values: dict = None,
    # categorical_values: allowed values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical columns
    # - Type: dict or None
    # - Format: {'column_name': ['value1', 'value2', ...]}
    # - Example: {
    #     'country': ['US', 'UK', 'FR', 'DE', 'JP'],
    #     'status': ['pending', 'completed', 'cancelled']
    #   }
    # - Validation: values must be in allowed list
    
    business_rules: list = None,
    # business_rules: custom validation rules
    # - Type: list of dicts or None
    # - Format: [
    #     {
    #       'name': 'rule_name',
    #       'description': 'what this rule checks',
    #       'expression': 'pandas boolean expression',
    #       'severity': 'CRITICAL' | 'WARNING' | 'INFO'
    #     }
    #   ]
    # - Example: [
    #     {
    #       'name': 'positive_quantity',
    #       'description': 'Quantity must be positive',
    #       'expression': 'df["Quantity"] > 0',
    #       'severity': 'CRITICAL'
    #     }
    #   ]
    # - Best practice: define in config file, not hardcode
    
    max_missing_pct: float = 5.0,
    # max_missing_pct: maximum % missing values allowed per column
    # - Type: float (0-100)
    # - Default: 5.0 (after cleaning ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ missing ‡∏ô‡πâ‡∏≠‡∏¢)
    # - Severity: WARNING if exceeded
    
    fail_on_critical: bool = True,
    # fail_on_critical: fail task ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ CRITICAL errors
    # - Type: bool
    # - True: raise exception on critical errors
    # - False: log errors but continue
    # - Best practice: True (prevent bad data from loading)
    
) -> dict:
    # Return type: validation results
    # Structure:
    # {
    #     'status': 'PASS' | 'PASS_WITH_WARNINGS' | 'FAILED',
    #     'overall_result': bool,
    #     'validation_report': {
    #         'schema': [...],
    #         'ranges': [...],
    #         'business_rules': [...],
    #         'statistical': [...],
    #         'completeness': [...]
    #     },
    #     'summary': {
    #         'total_checks': int,
    #         'passed': int,
    #         'warnings': int,
    #         'critical_errors': int
    #     },
    #     'failed_checks': [...],
    #     'timestamp': str
    # }
    
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á cleaning
    
    Args:
        cleaning_metadata: metadata ‡∏à‡∏≤‡∏Å cleaning task
        expected_columns: columns ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
        required_columns: columns ‡∏ó‡∏µ‡πà‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ missing
        column_types: data types ‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á
        numeric_ranges: min/max ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö numeric columns
        date_ranges: min/max ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö date columns
        categorical_values: allowed values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö categorical
        business_rules: custom validation rules
        max_missing_pct: maximum % missing allowed
        fail_on_critical: fail task on critical errors
    
    Returns:
        Dict containing:
        - status: PASS/PASS_WITH_WARNINGS/FAILED
        - validation_report: detailed results by category
        - summary: counts of passed/failed checks
        - failed_checks: list of failures
        
    Raises:
        FileNotFoundError: ‡∏ñ‡πâ‡∏≤‡πÑ‡∏ü‡∏•‡πå cleaned data ‡πÑ‡∏°‡πà‡∏û‡∏ö
        ValidationError: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ critical errors ‡πÅ‡∏•‡∏∞ fail_on_critical=True
    """
    
    import pandas as pd
    import numpy as np
    import logging
    from pathlib import Path
    from datetime import datetime
    import time
    
    # ============================================================
    # STEP 1: SETUP
    # ============================================================
    logger = logging.getLogger(__name__)
    logger.info("üöÄ [STEP 1/10] Starting data validation task")
    
    start_time = time.time()
    
    # Validate metadata
    if not cleaning_metadata or 'output_path' not in cleaning_metadata:
        error_msg = "‚ùå Invalid cleaning metadata"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    # Initialize validation report structure
    validation_report = {
        'schema': [],
        'ranges': [],
        'business_rules': [],
        'statistical': [],
        'completeness': []
    }
    
    # Counters
    total_checks = 0
    passed_checks = 0
    warnings = 0
    critical_errors = 0
    
    logger.info(f"‚úÖ Setup completed")
    
    # ============================================================
    # STEP 2: LOAD CLEANED DATA
    # ============================================================
    logger.info(f"üìñ [STEP 2/10] Loading cleaned data...")
    
    parquet_path = Path(cleaning_metadata['output_path'])
    
    if not parquet_path.exists():
        error_msg = f"‚ùå Cleaned data not found: {parquet_path}"
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        df = pd.read_parquet(parquet_path, engine='pyarrow')
        logger.info(f"‚úÖ Data loaded: {len(df):,} rows √ó {len(df.columns)} columns")
        
    except Exception as e:
        error_msg = f"‚ùå Error loading data: {str(e)}"
        logger.error(error_msg)
        raise
    
    # ============================================================
    # STEP 3: SCHEMA VALIDATION
    # ============================================================
    logger.info(f"üìã [STEP 3/10] Validating schema...")
    
    # Validation 3.1: Expected columns present
    # -----------------------------------------
    if expected_columns:
        total_checks += 1
        
        actual_columns = set(df.columns)
        expected_set = set(expected_columns)
        
        missing_columns = expected_set - actual_columns
        # Set difference: elements in expected but not in actual
        
        extra_columns = actual_columns - expected_set
        # Columns in data but not expected
        
        if missing_columns:
            critical_errors += 1
            validation_report['schema'].append({
                'check': 'expected_columns_present',
                'status': 'FAILED',
                'severity': 'CRITICAL',
                'message': f'Missing required columns: {list(missing_columns)}',
                'details': {
                    'missing': list(missing_columns),
                    'expected': expected_columns,
                    'actual': list(actual_columns)
                }
            })
            logger.error(f"  ‚ùå Missing columns: {missing_columns}")
        elif extra_columns:
            warnings += 1
            validation_report['schema'].append({
                'check': 'expected_columns_present',
                'status': 'WARNING',
                'severity': 'WARNING',
                'message': f'Extra columns found: {list(extra_columns)}',
                'details': {
                    'extra': list(extra_columns)
                }
            })
            logger.warning(f"  ‚ö†Ô∏è  Extra columns: {extra_columns}")
        else:
            passed_checks += 1
            validation_report['schema'].append({
                'check': 'expected_columns_present',
                'status': 'PASSED',
                'severity': 'INFO',
                'message': 'All expected columns present'
            })
            logger.info(f"  ‚úÖ All expected columns present")
    
    # Validation 3.2: Column data types
    # ---------------------------------
    if column_types:
        for col, expected_type in column_types.items():
            total_checks += 1
            
            if col not in df.columns:
                critical_errors += 1
                validation_report['schema'].append({
                    'check': f'column_type_{col}',
                    'status': 'FAILED',
                    'severity': 'CRITICAL',
                    'message': f'Column {col} not found in data'
                })
                continue
            
            actual_type = str(df[col].dtype)
            
            # Type matching with flexibility
            # Allow: int64 == int, float64 == float, etc.
            type_match = (
                actual_type == expected_type or
                expected_type in actual_type or
                (expected_type == 'int' and 'int' in actual_type) or
                (expected_type == 'float' and 'float' in actual_type) or
                (expected_type == 'datetime' and 'datetime' in actual_type)
            )
            
            if not type_match:
                critical_errors += 1
                validation_report['schema'].append({
                    'check': f'column_type_{col}',
                    'status': 'FAILED',
                    'severity': 'CRITICAL',
                    'message': f'Type mismatch for {col}: expected {expected_type}, got {actual_type}',
                    'details': {
                        'column': col,
                        'expected': expected_type,
                        'actual': actual_type
                    }
                })
                logger.error(f"  ‚ùå {col}: type mismatch (expected {expected_type}, got {actual_type})")
            else:
                passed_checks += 1
                validation_report['schema'].append({
                    'check': f'column_type_{col}',
                    'status': 'PASSED',
                    'severity': 'INFO',
                    'message': f'{col}: type correct ({actual_type})'
                })
                logger.info(f"  ‚úÖ {col}: type correct")
    
    logger.info(f"‚úÖ Schema validation completed")
    
    # ============================================================
    # STEP 4: RANGE VALIDATION
    # ============================================================
    logger.info(f"üìè [STEP 4/10] Validating numeric ranges...")
    
    if numeric_ranges:
        for col, ranges in numeric_ranges.items():
            if col not in df.columns:
                logger.warning(f"  ‚ö†Ô∏è  Column {col} not found, skipping range check")
                continue
            
            min_val = ranges.get('min')
            max_val = ranges.get('max')
            
            # Check minimum
            if min_val is not None:
                total_checks += 1
                
                violations = df[col] < min_val
                # Boolean Series: True where value < min
                
                n_violations = violations.sum()
                
                if n_violations > 0:
                    violation_pct = (n_violations / len(df) * 100)
                    
                    # Sample violating values
                    sample_violations = df.loc[violations, col].head(5).tolist()
                    # df.loc[boolean_series, column]: filter rows
                    
                    warnings += 1
                    validation_report['ranges'].append({
                        'check': f'{col}_min_value',
                        'status': 'WARNING',
                        'severity': 'WARNING',
                        'message': f'{col}: {n_violations:,} values below minimum {min_val}',
                        'details': {
                            'column': col,
                            'threshold': min_val,
                            'violations': int(n_violations),
                            'percentage': round(violation_pct, 2),
                            'sample_values': sample_violations
                        }
                    })
                    logger.warning(f"  ‚ö†Ô∏è  {col}: {n_violations:,} values < {min_val} ({violation_pct:.2f}%)")
                else:
                    passed_checks += 1
                    validation_report['ranges'].append({
                        'check': f'{col}_min_value',
                        'status': 'PASSED',
                        'severity': 'INFO',
                        'message': f'{col}: all values >= {min_val}'
                    })
            
            # Check maximum
            if max_val is not None:
                total_checks += 1
                
                violations = df[col] > max_val
                n_violations = violations.sum()
                
                if n_violations > 0:
                    violation_pct = (n_violations / len(df) * 100)
                    sample_violations = df.loc[violations, col].head(5).tolist()
                    
                    warnings += 1
                    validation_report['ranges'].append({
                        'check': f'{col}_max_value',
                        'status': 'WARNING',
                        'severity': 'WARNING',
                        'message': f'{col}: {n_violations:,} values above maximum {max_val}',
                        'details': {
                            'column': col,
                            'threshold': max_val,
                            'violations': int(n_violations),
                            'percentage': round(violation_pct, 2),
                            'sample_values': sample_violations
                        }
                    })
                    logger.warning(f"  ‚ö†Ô∏è  {col}: {n_violations:,} values > {max_val} ({violation_pct:.2f}%)")
                else:
                    passed_checks += 1
                    validation_report['ranges'].append({
                        'check': f'{col}_max_value',
                        'status': 'PASSED',
                        'severity': 'INFO',
                        'message': f'{col}: all values <= {max_val}'
                    })
    
    logger.info(f"‚úÖ Range validation completed")
    
    # ============================================================
    # STEP 5: DATE RANGE VALIDATION
    # ============================================================
    logger.info(f"üìÖ [STEP 5/10] Validating date ranges...")
    
    if date_ranges:
        for col, ranges in date_ranges.items():
            if col not in df.columns:
                logger.warning(f"  ‚ö†Ô∏è  Column {col} not found, skipping")
                continue
            
            # Convert to datetime if not already
            if not pd.api.types.is_datetime64_any_dtype(df[col]):
                logger.warning(f"  ‚ö†Ô∏è  {col} is not datetime type, skipping")
                continue
            
            min_date = pd.to_datetime(ranges.get('min')) if ranges.get('min') else None
            max_date = pd.to_datetime(ranges.get('max')) if ranges.get('max') else None
            
            if min_date is not None:
                total_checks += 1
                violations = df[col] < min_date
                n_violations = violations.sum()
                
                if n_violations > 0:
                    warnings += 1
                    validation_report['ranges'].append({
                        'check': f'{col}_min_date',
                        'status': 'WARNING',
                        'severity': 'WARNING',
                        'message': f'{col}: {n_violations:,} dates before {min_date.date()}',
                        'details': {
                            'column': col,
                            'threshold': str(min_date.date()),
                            'violations': int(n_violations)
                        }
                    })
                    logger.warning(f"  ‚ö†Ô∏è  {col}: {n_violations:,} dates < {min_date.date()}")
                else:
                    passed_checks += 1
            
            if max_date is not None:
                total_checks += 1
                violations = df[col] > max_date
                n_violations = violations.sum()
                
                if n_violations > 0:
                    warnings += 1
                    validation_report['ranges'].append({
                        'check': f'{col}_max_date',
                        'status': 'WARNING',
                        'severity': 'WARNING',
                        'message': f'{col}: {n_violations:,} dates after {max_date.date()}',
                        'details': {
                            'column': col,
                            'threshold': str(max_date.date()),
                            'violations': int(n_violations)
                        }
                    })
                    logger.warning(f"  ‚ö†Ô∏è  {col}: {n_violations:,} dates > {max_date.date()}")
                else:
                    passed_checks += 1
    
    logger.info(f"‚úÖ Date range validation completed")
    
    # ============================================================
    # STEP 6: CATEGORICAL VALUES VALIDATION
    # ============================================================
    logger.info(f"üè∑Ô∏è  [STEP 6/10] Validating categorical values...")
    
    if categorical_values:
        for col, allowed_values in categorical_values.items():
            total_checks += 1
            
            if col not in df.columns:
                logger.warning(f"  ‚ö†Ô∏è  Column {col} not found, skipping")
                continue
            
            # Get unique values in column
            actual_values = set(df[col].dropna().unique())
            # .dropna(): exclude NaN values
            # .unique(): get unique values
            # set(): convert to set for comparison
            
            allowed_set = set(allowed_values)
            
            # Find invalid values (not in allowed list)
            invalid_values = actual_values - allowed_set
            
            if invalid_values:
                # Count rows with invalid values
                n_violations = df[col].isin(invalid_values).sum()
                # .isin(list): check if values are in list
                # Returns: boolean Series
                
                violation_pct = (n_violations / len(df) * 100)
                
                warnings += 1
                validation_report['ranges'].append({
                    'check': f'{col}_allowed_values',
                    'status': 'WARNING',
                    'severity': 'WARNING',
                    'message': f'{col}: found {len(invalid_values)} invalid values',
                    'details': {
                        'column': col,
                        'invalid_values': list(invalid_values),
                        'allowed_values': allowed_values,
                        'violations': int(n_violations),
                        'percentage': round(violation_pct, 2)
                    }
                })
                logger.warning(f"  ‚ö†Ô∏è  {col}: invalid values found: {invalid_values}")
            else:
                passed_checks += 1
                validation_report['ranges'].append({
                    'check': f'{col}_allowed_values',
                    'status': 'PASSED',
                    'severity': 'INFO',
                    'message': f'{col}: all values are valid'
                })
                logger.info(f"  ‚úÖ {col}: all values valid")
    
    logger.info(f"‚úÖ Categorical validation completed")
    
    # ============================================================
    # STEP 7: BUSINESS RULES VALIDATION
    # ============================================================
    logger.info(f"üìê [STEP 7/10] Validating business rules...")
    
    if business_rules:
        for rule in business_rules:
            total_checks += 1
            
            rule_name = rule.get('name', 'unnamed_rule')
            description = rule.get('description', 'No description')
            expression = rule.get('expression')
            severity = rule.get('severity', 'WARNING')
            
            if not expression:
                logger.warning(f"  ‚ö†Ô∏è  Rule {rule_name}: no expression provided")
                continue
            
            try:
                # Evaluate expression
                # expression is string like: 'df["Quantity"] > 0'
                result = eval(expression)
                # eval(): evaluate Python expression
                # - Security risk: NEVER use with user input
                # - Safe here: expressions from config file (controlled)
                # - Alternative: use query() method (safer but less flexible)
                
                # result should be boolean Series
                if isinstance(result, pd.Series):
                    # Count violations (where condition is False)
                    violations = ~result
                    # ~ : logical NOT (invert boolean)
                    
                    n_violations = violations.sum()
                    violation_pct = (n_violations / len(df) * 100) if len(df) > 0 else 0
                    
                    if n_violations > 0:
                        # Get sample of violating rows
                        sample_rows = df[violations].head(3).to_dict('records')
                        # .to_dict('records'): convert to list of dicts
                        # Format: [{'col1': val1, 'col2': val2}, ...]
                        
                        if severity == 'CRITICAL':
                            critical_errors += 1
                        else:
                            warnings += 1
                        
                        validation_report['business_rules'].append({
                            'check': rule_name,
                            'status': 'FAILED',
                            'severity': severity,
                            'message': f'{description}: {n_violations:,} violations',
                            'details': {
                                'rule': expression,
                                'violations': int(n_violations),
                                'percentage': round(violation_pct, 2),
                                'sample_violations': sample_rows
                            }
                        })
                        
                        log_func = logger.error if severity == 'CRITICAL' else logger.warning
                        log_symbol = '‚ùå' if severity == 'CRITICAL' else '‚ö†Ô∏è'
                        log_func(f"  {log_symbol} {rule_name}: {n_violations:,} violations ({violation_pct:.2f}%)")
                    else:
                        passed_checks += 1
                        validation_report['business_rules'].append({
                            'check': rule_name,
                            'status': 'PASSED',
                            'severity': 'INFO',
                            'message': f'{description}: passed'
                        })
                        logger.info(f"  ‚úÖ {rule_name}: passed")
                else:
                    logger.warning(f"  ‚ö†Ô∏è  {rule_name}: expression did not return boolean Series")
                    
            except Exception as e:
                logger.error(f"  ‚ùå {rule_name}: error evaluating rule: {str(e)}")
                critical_errors += 1
                validation_report['business_rules'].append({
                    'check': rule_name,
                    'status': 'ERROR',
                    'severity': 'CRITICAL',
                    'message': f'Error evaluating rule: {str(e)}',
                    'details': {
                        'expression': expression,
                        'error': str(e)
                    }
                })
    else:
        logger.info(f"  - No business rules specified")
    
    logger.info(f"‚úÖ Business rules validation completed")
    
    # ============================================================
    # STEP 8: STATISTICAL VALIDATION
    # ============================================================
    logger.info(f"üìä [STEP 8/10] Statistical validation...")
    
    # Validation 8.1: Check for constant columns (should not exist after cleaning)
    # ---------------------------------------------------------------------------
    total_checks += 1
    
    constant_cols = []
    for col in df.columns:
        if df[col].nunique() == 1:
            # Column has only 1 unique value
            constant_cols.append(col)
    
    if constant_cols:
        warnings += 1
        validation_report['statistical'].append({
            'check': 'constant_columns',
            'status': 'WARNING',
            'severity': 'WARNING',
            'message': f'Found {len(constant_cols)} constant columns',
            'details': {
                'columns': constant_cols
            }
        })
        logger.warning(f"  ‚ö†Ô∏è  Constant columns found: {constant_cols}")
    else:
        passed_checks += 1
        validation_report['statistical'].append({
            'check': 'constant_columns',
            'status': 'PASSED',
            'severity': 'INFO',
            'message': 'No constant columns'
        })
        logger.info(f"  ‚úÖ No constant columns")
    
    # Validation 8.2: Check for high cardinality (potential data quality issue)
    # --------------------------------------------------------------------------
    total_checks += 1
    
    high_cardinality_threshold = 0.95  # > 95% unique = high cardinality
    high_cardinality_cols = []
    
    for col in df.select_dtypes(include=['object', 'category']).columns:
        unique_pct = (df[col].nunique() / len(df) * 100) if len(df) > 0 else 0
        
        if unique_pct > high_cardinality_threshold * 100:
            high_cardinality_cols.append({
                'column': col,
                'unique_pct': round(unique_pct, 2),
                'unique_count': df[col].nunique()
        })

if high_cardinality_cols:
    warnings += 1
    validation_report['statistical'].append({
        'check': 'high_cardinality',
        'status': 'WARNING',
        'severity': 'WARNING',
        'message': f'{len(high_cardinality_cols)} columns with very high cardinality',
        'details': {
            'columns': high_cardinality_cols
        }
    })
    logger.warning(f"  ‚ö†Ô∏è  High cardinality columns: {[c['column'] for c in high_cardinality_cols]}")
else:
    passed_checks += 1
    validation_report['statistical'].append({
        'check': 'high_cardinality',
        'status': 'PASSED',
        'severity': 'INFO',
        'message': 'No high cardinality issues'
    })

logger.info(f"‚úÖ Statistical validation completed")

# ============================================================
# STEP 9: COMPLETENESS VALIDATION
# ============================================================
logger.info(f"üìù [STEP 9/10] Validating completeness...")

# Validation 9.1: Required columns must have no missing values
# ------------------------------------------------------------
if required_columns:
    for col in required_columns:
        total_checks += 1
        
        if col not in df.columns:
            critical_errors += 1
            validation_report['completeness'].append({
                'check': f'{col}_required',
                'status': 'FAILED',
                'severity': 'CRITICAL',
                'message': f'Required column {col} not found'
            })
            logger.error(f"  ‚ùå Required column {col} not found")
            continue
        
        missing_count = df[col].isnull().sum()
        
        if missing_count > 0:
            critical_errors += 1
            validation_report['completeness'].append({
                'check': f'{col}_required',
                'status': 'FAILED',
                'severity': 'CRITICAL',
                'message': f'Required column {col} has {missing_count:,} missing values',
                'details': {
                    'column': col,
                    'missing': int(missing_count),
                    'percentage': round((missing_count / len(df) * 100), 2)
                }
            })
            logger.error(f"  ‚ùå {col}: {missing_count:,} missing values in required column")
        else:
            passed_checks += 1
            validation_report['completeness'].append({
                'check': f'{col}_required',
                'status': 'PASSED',
                'severity': 'INFO',
                'message': f'{col}: no missing values'
            })
            logger.info(f"  ‚úÖ {col}: complete")

# Validation 9.2: Check missing percentage per column
# ---------------------------------------------------
total_checks += 1

high_missing_cols = []
for col in df.columns:
    missing_pct = (df[col].isnull().sum() / len(df) * 100) if len(df) > 0 else 0
    
    if missing_pct > max_missing_pct:
        high_missing_cols.append({
            'column': col,
            'missing_pct': round(missing_pct, 2),
            'missing_count': df[col].isnull().sum()
        })

if high_missing_cols:
    warnings += 1
    validation_report['completeness'].append({
        'check': 'missing_threshold',
        'status': 'WARNING',
        'severity': 'WARNING',
        'message': f'{len(high_missing_cols)} columns exceed {max_missing_pct}% missing threshold',
        'details': {
            'threshold': max_missing_pct,
            'columns': high_missing_cols
        }
    })
    logger.warning(f"  ‚ö†Ô∏è  {len(high_missing_cols)} columns exceed missing threshold")
else:
    passed_checks += 1
    validation_report['completeness'].append({
        'check': 'missing_threshold',
        'status': 'PASSED',
        'severity': 'INFO',
        'message': f'All columns below {max_missing_pct}% missing threshold'
    })
    logger.info(f"  ‚úÖ All columns below missing threshold")

logger.info(f"‚úÖ Completeness validation completed")

# ============================================================
# STEP 10: GENERATE SUMMARY AND FINAL STATUS
# ============================================================
logger.info(f"üìã [STEP 10/10] Generating validation summary...")

end_time = time.time()
duration = end_time - start_time

# Determine overall status
if critical_errors > 0:
    overall_status = 'FAILED'
    overall_result = False
elif warnings > 0:
    overall_status = 'PASS_WITH_WARNINGS'
    overall_result = True
else:
    overall_status = 'PASS'
    overall_result = True

# Collect all failed/warning checks
failed_checks = []
for category, checks in validation_report.items():
    for check in checks:
        if check['status'] in ['FAILED', 'WARNING']:
            failed_checks.append({
                'category': category,
                **check
            })

# Create summary
summary = {
    'total_checks': total_checks,
    'passed': passed_checks,
    'warnings': warnings,
    'critical_errors': critical_errors,
    'pass_rate': round((passed_checks / total_checks * 100), 2) if total_checks > 0 else 0
}

# Log summary
logger.info(f"")
logger.info(f"{'='*60}")
logger.info(f"VALIDATION SUMMARY")
logger.info(f"{'='*60}")
logger.info(f"Status:          {overall_status}")
logger.info(f"Total checks:    {total_checks}")
logger.info(f"Passed:          {passed_checks} ‚úÖ")
logger.info(f"Warnings:        {warnings} ‚ö†Ô∏è")
logger.info(f"Critical errors: {critical_errors} ‚ùå")
logger.info(f"Pass rate:       {summary['pass_rate']:.2f}%")
logger.info(f"Duration:        {duration:.2f}s")
logger.info(f"{'='*60}")

# Save validation report to file
reports_dir = Path('/opt/airflow/data/reports')
reports_dir.mkdir(parents=True, exist_ok=True)

timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
report_filename = f'validation_report_{timestamp_str}.json'
report_path = reports_dir / report_filename

import json
try:
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump({
            'status': overall_status,
            'summary': summary,
            'validation_report': validation_report,
            'failed_checks': failed_checks,
            'metadata': {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'duration_seconds': round(duration, 2),
                'timestamp': datetime.now().isoformat()
            }
        }, f, indent=2, ensure_ascii=False)
    
    logger.info(f"‚úÖ Validation report saved: {report_path}")
except Exception as e:
    logger.warning(f"‚ö†Ô∏è  Could not save validation report: {str(e)}")

# Create metadata for XCom
metadata = {
    'status': overall_status,
    'overall_result': overall_result,
    'report_path': str(report_path),
    'summary': summary,
    'validation_report': validation_report,
    'failed_checks': failed_checks[:10],  # Limit to first 10 for XCom size
    'duration_seconds': round(duration, 2),
    'timestamp': datetime.now().isoformat()
}

# Fail task if critical errors and fail_on_critical=True
if critical_errors > 0 and fail_on_critical:
    error_msg = f"‚ùå Validation failed with {critical_errors} critical error(s)"
    logger.error(error_msg)
    logger.error(f"   See report for details: {report_path}")
    raise ValueError(error_msg)

logger.info(f"‚úÖ Data validation completed: {overall_status}")

return metadata

---

### üéì Key Learnings from Task 4

**1. Validation vs Cleaning**:
- **Cleaning**: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
- **Validation**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç
- Validation ‡∏´‡∏•‡∏±‡∏á cleaning ‡πÄ‡∏û‡∏∑‡πà‡∏≠ verify quality

**2. Severity Levels**:
- **CRITICAL**: Pipeline ‡∏ï‡πâ‡∏≠‡∏á fail (bad data ‡∏´‡πâ‡∏≤‡∏°‡∏ú‡πà‡∏≤‡∏ô)
- **WARNING**: Log ‡πÅ‡∏ï‡πà continue (investigate later)
- **INFO**: Informational only

**3. Validation Categories**:
- **Schema**: columns, types (CRITICAL)
- **Ranges**: min/max values (WARNING)
- **Business Rules**: domain logic (varies)
- **Statistical**: distributions, anomalies (WARNING)
- **Completeness**: missing values (varies)

**4. Best Practices**:
- **Fail Fast**: Stop on critical errors
- **Detailed Reporting**: Include samples, percentages
- **Actionable Messages**: Tell what's wrong AND how to fix
- **Version Control**: Save validation reports

**5. Common Pitfalls**:
- **Too Strict**: Reject good data (false positives)
- **Too Lenient**: Accept bad data (false negatives)
- **Alert Fatigue**: Too many warnings ‚Üí ignored
- **Poor Messages**: "Validation failed" without details

---

### ‚úÖ Testing Task 4
```python
# tests/test_validation.py
import pytest
from dags.tasks.validation import validate_data

def test_validation_pass():
    """Test validation with clean data"""
    cleaning_metadata = {
        'output_path': '/opt/airflow/tests/fixtures/clean_data.parquet'
    }
    
    metadata = validate_data(
        cleaning_metadata=cleaning_metadata,
        expected_columns=['col1', 'col2', 'col3'],
        fail_on_critical=False
    )
    
    assert metadata['overall_result'] == True
    assert metadata['summary']['critical_errors'] == 0

def test_validation_fail_missing_column():
    """Test validation with missing required column"""
    cleaning_metadata = {
        'output_path': '/opt/airflow/tests/fixtures/clean_data.parquet'
    }
    
    with pytest.raises(ValueError):
        validate_data(
            cleaning_metadata=cleaning_metadata,
            expected_columns=['col1', 'col2', 'missing_col'],
            fail_on_critical=True
        )

def test_validation_warnings():
    """Test validation with warnings"""
    cleaning_metadata = {
        'output_path': '/opt/airflow/tests/fixtures/data_with_outliers.parquet'
    }
    
    metadata = validate_data(
        cleaning_metadata=cleaning_metadata,
        numeric_ranges={'age': {'min': 0, 'max': 120}},
        fail_on_critical=False
    )
    
    assert metadata['status'] == 'PASS_WITH_WARNINGS'
    assert metadata['summary']['warnings'] > 0
```

---

Task 5: Data Loading
üéØ ‡∏à‡∏∏‡∏î‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å
=============================================================================
TASK 5: DATA LOADING - ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô validation ‡πÄ‡∏Ç‡πâ‡∏≤ Database ‡πÅ‡∏•‡∏∞ Data Lake
=============================================================================
‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:

‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà validated ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏≤‡∏Å Task 4
‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ PostgreSQL database
‡∏™‡∏£‡πâ‡∏≤‡∏á/update table schema
Load data ‡πÄ‡∏Ç‡πâ‡∏≤ PostgreSQL (Analytics Database)
‡∏™‡∏£‡πâ‡∏≤‡∏á indexes ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
Save ‡πÄ‡∏õ‡πá‡∏ô Parquet ‡πÉ‡∏ô Data Lake (partitioned)
Update metadata table (audit trail)
Archive source files
Verify data integrity post-load
Return loading summary

üìä Flow ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:

Load validated data
Create database connection
Check if table exists ‚Üí create/update schema
Choose loading strategy (full/incremental/upsert)
Load data to PostgreSQL
Create indexes for query performance
Verify row counts and data integrity
Save to Data Lake (partitioned Parquet)
Update metadata table
Archive source files
Return loading summary

‚ö†Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏£‡∏∞‡∏ß‡∏±‡∏á‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á:

Transaction Management: ‡πÉ‡∏ä‡πâ transactions ‡πÄ‡∏û‡∏∑‡πà‡∏≠ atomicity
Connection Pooling: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ connections ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
Batch Size: chunk data ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ memory overflow
Schema Evolution: handle column add/remove/rename
Rollback Strategy: ‡∏°‡∏µ‡πÅ‡∏ú‡∏ô‡∏™‡∏≥‡∏£‡∏≠‡∏á ‡∏ñ‡πâ‡∏≤ load fail
Performance: optimize batch size, parallel loading


üìù Complete Implementation
python@task(
    # task_id: ‡∏ä‡∏∑‡πà‡∏≠ task
    task_id='load_data',
    
    # retries: loading ‡∏Ñ‡∏ß‡∏£ retry
    # - Database operations ‡∏≠‡∏≤‡∏à‡∏°‡∏µ transient failures
    # - Network issues, connection timeouts
    # - Best practice: 2-3 retries
    retries=3,
    
    # retry_delay: delay ‡∏Å‡πà‡∏≠‡∏ô retry
    # - Database operations: ‡∏£‡∏≠‡πÉ‡∏´‡πâ connection pool recover
    # - Best practice: 2-5 minutes
    retry_delay=timedelta(minutes=3),
    
    # execution_timeout: loading ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô
    # - Depends on: data size, network speed, database performance
    # - Estimation:
    #   * Small (< 100K rows): 2-5 minutes
    #   * Medium (100K-1M rows): 5-15 minutes
    #   * Large (> 1M rows): 15-30 minutes
    execution_timeout=timedelta(minutes=30),
    
    # pool: loading ‡πÄ‡∏õ‡πá‡∏ô I/O intensive
    pool='io_intensive',
    
    # pool_slots: database loading ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏≤‡∏¢ slots
    # - ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ hold connection ‡∏ô‡∏≤‡∏ô
    pool_slots=2,
    
    # retry_exponential_backoff: ‡πÉ‡∏ä‡πâ exponential backoff
    # - Database recovery may take time
    retry_exponential_backoff=True,
    
    max_retry_delay=timedelta(minutes=15),
)
def load_data(
    validation_metadata: dict,
    # validation_metadata: metadata ‡∏à‡∏≤‡∏Å Task 4
    # - Includes: validation status, data path
    
    db_connection_id: str = 'postgres_default',
    # db_connection_id: Airflow connection ID
    # - Type: str
    # - Airflow Connections store: host, port, user, password, database
    # - Configure in: Airflow UI ‚Üí Admin ‚Üí Connections
    # - Example: 'postgres_default', 'postgres_analytics', 'postgres_prod'
    # - Best practice: different connections for dev/staging/prod
    
    target_table: str = 'marketing_data',
    # target_table: ‡∏ä‡∏∑‡πà‡∏≠ table ‡πÉ‡∏ô PostgreSQL
    # - Type: str
    # - Naming convention: snake_case, descriptive
    # - Best practice: prefix by domain (e.g., 'marketing_', 'sales_')
    
    target_schema: str = 'public',
    # target_schema: database schema
    # - Type: str
    # - Default: 'public' (PostgreSQL default schema)
    # - Best practice: organize by domain
    #   * 'raw': raw data
    #   * 'staging': intermediate tables
    #   * 'analytics': production tables
    # - Example: 'analytics.marketing_data'
    
    load_strategy: str = 'replace',
    # load_strategy: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£ load data
    # - Type: str
    # - Options:
    #   * 'replace': drop table ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà (full refresh)
    #   * 'append': ‡πÄ‡∏û‡∏¥‡πà‡∏° rows ‡πÉ‡∏´‡∏°‡πà (incremental)
    #   * 'upsert': update existing, insert new (merge)
    # - Comparison:
    #   * replace: simple, ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠ data size ‡πÄ‡∏•‡πá‡∏Å, no history
    #   * append: fast, ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö append-only data (logs, events)
    #   * upsert: complex, ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ï‡πâ‡∏≠‡∏á update existing records
    # - Best practice:
    #   * Development: 'replace' (simple)
    #   * Production: 'upsert' or 'append' (preserve history)
    
    upsert_keys: list = None,
    # upsert_keys: columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö identify unique rows
    # - Type: list of column names or None
    # - Required for: load_strategy='upsert'
    # - Example: ['customer_id', 'order_date']
    # - Use case: composite primary key
    # - PostgreSQL: ON CONFLICT (keys) DO UPDATE
    
    batch_size: int = 10000,
    # batch_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô rows ‡∏ï‡πà‡∏≠ batch insert
    # - Type: int
    # - Range: 1,000 - 50,000
    # - Trade-off:
    #   * Small batch: many transactions, slower, less memory
    #   * Large batch: few transactions, faster, more memory
    # - Best practice:
    #   * < 100K total rows: 10,000 (default)
    #   * 100K - 1M rows: 20,000
    #   * > 1M rows: 50,000
    # - Network consideration: larger batches if network is bottleneck
    
    create_indexes: bool = True,
    # create_indexes: ‡∏™‡∏£‡πâ‡∏≤‡∏á indexes ‡∏´‡∏•‡∏±‡∏á load ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    # - Type: bool
    # - True: create indexes automatically (recommended)
    # - False: skip (manual index creation later)
    # - Trade-off:
    #   * Create indexes: slower load, faster queries
    #   * No indexes: faster load, slower queries
    # - Best practice: True (query performance is critical)
    
    index_columns: list = None,
    # index_columns: columns ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á index
    # - Type: list of column names or None
    # - None: auto-detect (date columns, foreign keys)
    # - Example: ['customer_id', 'order_date', 'country']
    # - Guidelines:
    #   * Columns ‡πÉ‡∏ô WHERE clause ‡∏ö‡πà‡∏≠‡∏¢
    #   * Foreign keys
    #   * Date/timestamp columns
    #   * High selectivity columns
    # - Cost: indexes ‡πÉ‡∏ä‡πâ disk space, slow down inserts
    
    partition_by: list = None,
    # partition_by: columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö partition Parquet files
    # - Type: list of column names or None
    # - None: no partitioning (single file or auto-split)
    # - Example: ['year', 'month'] or ['country', 'year']
    # - Partitioning benefits:
    #   * Query efficiency (read ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ partitions ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    #   * Parallel processing
    #   * Data management (easy to delete old partitions)
    # - Best practice:
    #   * Partition by time (year/month/day)
    #   * Low cardinality (< 1000 partitions)
    #   * Columns ‡∏ó‡∏µ‡πà filter ‡∏ö‡πà‡∏≠‡∏¢
    
    archive_source: bool = True,
    # archive_source: archive raw files ‡∏´‡∏•‡∏±‡∏á load ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
    # - Type: bool
    # - True: move to archive directory (recommended)
    # - False: leave in place
    # - Best practice: True (clean up processed files)
    
) -> dict:
    # Return type: loading summary
    # Structure:
    # {
    #     'status': 'success' | 'failed',
    #     'table_name': str,
    #     'rows_loaded': int,
    #     'load_strategy': str,
    #     'database_size_mb': float,
    #     'parquet_path': str,
    #     'parquet_size_mb': float,
    #     'indexes_created': list,
    #     'duration_seconds': float,
    #     'timestamp': str
    # }
    
    """
    Load validated data ‡πÄ‡∏Ç‡πâ‡∏≤ PostgreSQL ‡πÅ‡∏•‡∏∞ Data Lake
    
    Args:
        validation_metadata: metadata ‡∏à‡∏≤‡∏Å validation task
        db_connection_id: Airflow connection ID
        target_table: table name ‡πÉ‡∏ô PostgreSQL
        target_schema: schema name
        load_strategy: 'replace', 'append', 'upsert'
        upsert_keys: columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö upsert (required if strategy='upsert')
        batch_size: rows ‡∏ï‡πà‡∏≠ batch insert
        create_indexes: ‡∏™‡∏£‡πâ‡∏≤‡∏á indexes ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        index_columns: columns ‡∏ó‡∏µ‡πà‡∏à‡∏∞ index
        partition_by: columns ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö partition Parquet
        archive_source: archive source files ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    
    Returns:
        Dict containing:
        - status: success/failed
        - loading statistics: rows, size, duration
        - database info: table name, indexes
        - data lake info: parquet path, partitions
        
    Raises:
        ValueError: ‡∏ñ‡πâ‡∏≤ validation failed
        ConnectionError: ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ database ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
        DatabaseError: ‡∏ñ‡πâ‡∏≤ load operation failed
    """
    
    import pandas as pd
    import numpy as np
    import logging
    from pathlib import Path
    from datetime import datetime
    import time
    from sqlalchemy import create_engine, inspect, Table, Column, Integer, String, Float, DateTime, Boolean, MetaData, Index, text
    from sqlalchemy.exc import SQLAlchemyError
    from sqlalchemy.dialects.postgresql import insert
    import shutil
    
    # ============================================================
    # STEP 1: SETUP AND VALIDATION
    # ============================================================
    logger = logging.getLogger(__name__)
    logger.info("üöÄ [STEP 1/11] Starting data loading task")
    
    start_time = time.time()
    
    # Check validation status
    if not validation_metadata:
        error_msg = "‚ùå No validation metadata received"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    validation_status = validation_metadata.get('status')
    if validation_status == 'FAILED':
        error_msg = f"‚ùå Cannot load data: validation failed"
        logger.error(error_msg)
        logger.error(f"   Critical errors: {validation_metadata.get('summary', {}).get('critical_errors', 0)}")
        raise ValueError(error_msg)
    
    if validation_status == 'PASS_WITH_WARNINGS':
        logger.warning(f"‚ö†Ô∏è  Loading data with {validation_metadata.get('summary', {}).get('warnings', 0)} warnings")
    
    # Validate upsert configuration
    if load_strategy == 'upsert' and not upsert_keys:
        error_msg = "‚ùå upsert_keys required when load_strategy='upsert'"
        logger.error(error_msg)
        raise ValueError(error_msg)
    
    logger.info(f"‚úÖ Configuration validated")
    logger.info(f"  - Load strategy: {load_strategy}")
    logger.info(f"  - Target: {target_schema}.{target_table}")
    logger.info(f"  - Batch size: {batch_size:,}")
    
    # ============================================================
    # STEP 2: LOAD VALIDATED DATA
    # ============================================================
    logger.info(f"üìñ [STEP 2/11] Loading validated data...")
    
    # Get data path from cleaning metadata (passed through validation)
    # In practice, we'd get this from validation_metadata or XCom
    # For this example, we'll assume it's in validation_metadata
    
    # Find the cleaned parquet file
    processed_dir = Path('/opt/airflow/data/processed')
    parquet_files = sorted(processed_dir.glob('cleaned_*.parquet'))
    
    if not parquet_files:
        error_msg = "‚ùå No cleaned data files found"
        logger.error(error_msg)
        raise FileNotFoundError(error_msg)
    
    # Use most recent file
    data_path = parquet_files[-1]
    logger.info(f"  - Loading from: {data_path}")
    
    try:
        df = pd.read_parquet(data_path, engine='pyarrow')
        logger.info(f"‚úÖ Data loaded: {len(df):,} rows √ó {len(df.columns)} columns")
        
    except Exception as e:
        error_msg = f"‚ùå Error loading data: {str(e)}"
        logger.error(error_msg)
        raise
    
    rows_to_load = len(df)
    
    # ============================================================
    # STEP 3: CREATE DATABASE CONNECTION
    # ============================================================
    logger.info(f"üîå [STEP 3/11] Creating database connection...")
    
    try:
        # Get connection from Airflow
        from airflow.hooks.base import BaseHook
        
        conn = BaseHook.get_connection(db_connection_id)
        # BaseHook.get_connection(): retrieve Airflow Connection
        # - Returns: Connection object
        # - Properties: host, port, schema, login, password
        # - Raises: AirflowException if connection not found
        
        # Build connection string
        # Format: postgresql://user:password@host:port/database
        connection_string = (
            f"postgresql://{conn.login}:{conn.password}"
            f"@{conn.host}:{conn.port}/{conn.schema}"
        )
        # Security note: ‡∏´‡πâ‡∏≤‡∏° log connection string (contains password)
        
        # Create SQLAlchemy engine
        engine = create_engine(
            connection_string,
            # URL: database connection string
            
            pool_size=5,
            # pool_size: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô connections ‡πÉ‡∏ô pool
            # - Default: 5
            # - Range: 2-10 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö most applications
            # - Benefits: reuse connections (faster than creating new)
            # - Trade-off: more connections = more database load
            
            max_overflow=10,
            # max_overflow: connections ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ñ‡πâ‡∏≤ pool full
            # - Default: 10
            # - Total connections = pool_size + max_overflow
            # - Example: pool_size=5, max_overflow=10 ‚Üí max 15 connections
            # - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô ‚Üí block until connection available
            
            pool_pre_ping=True,
            # pool_pre_ping: test connection ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ
            # - True: execute "SELECT 1" ‡∏Å‡πà‡∏≠‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
            # - Benefits: detect stale connections, auto-reconnect
            # - Trade-off: slight overhead (1-2ms per query)
            # - Best practice: True (reliability > performance)
            
            pool_recycle=3600,
            # pool_recycle: recycle connections after N seconds
            # - Default: -1 (no recycling)
            # - Recommended: 3600 (1 hour)
            # - Reason: prevent stale connections, database timeouts
            # - Database timeout ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ: 8 hours (PostgreSQL default)
            
            echo=False,
            # echo: log all SQL statements
            # - True: log to stdout (debugging)
            # - False: no logging (production)
            # - Best practice: False (use SQLAlchemy logger instead)
        )
        
        # Test connection
        with engine.connect() as connection:
            # engine.connect(): get connection from pool
            # - Context manager: auto-close connection
            # - Returns: Connection object
            
            result = connection.execute(text("SELECT version()"))
            # text(): create SQL text clause
            # - Required in SQLAlchemy 2.0+
            # - Returns: TextClause object
            
            version = result.fetchone()[0]
            # fetchone(): get first row
            # - Returns: Row object or None
            # [0]: get first column
            
            logger.info(f"‚úÖ Connected to PostgreSQL")
            logger.info(f"   Version: {version[:50]}...")  # Truncate long version string
        
    except Exception as e:
        error_msg = f"‚ùå Database connection failed: {str(e)}"
        logger.error(error_msg)
        raise ConnectionError(error_msg) from e
    
    # ============================================================
    # STEP 4: PREPARE TABLE SCHEMA
    # ============================================================
    logger.info(f"üìê [STEP 4/11] Preparing table schema...")
    
    # Create SQLAlchemy metadata
    metadata = MetaData(schema=target_schema)
    # MetaData: container for database schema
    # - Holds: Table definitions, relationships
    # - schema: database schema name
    
    # Inspect existing database
    inspector = inspect(engine)
    # inspect(): create Inspector object
    # - Used to: query database metadata
    # - Methods: get_table_names(), get_columns(), get_indexes()
    
    table_exists = inspector.has_table(target_table, schema=target_schema)
    # has_table(): check if table exists
    # - Returns: bool
    # - Alternative: target_table in inspector.get_table_names(schema)
    
    if table_exists:
        logger.info(f"  - Table {target_schema}.{target_table} exists")
        
        if load_strategy == 'replace':
            logger.info(f"  - Will drop and recreate table")
        elif load_strategy == 'append':
            logger.info(f"  - Will append to existing table")
        elif load_strategy == 'upsert':
            logger.info(f"  - Will upsert based on keys: {upsert_keys}")
    else:
        logger.info(f"  - Table {target_schema}.{target_table} does not exist")
        logger.info(f"  - Will create new table")
    
    # Map Pandas dtypes to SQL types
    # --------------------------------------------------------
    def pandas_dtype_to_sql_type(dtype):
        """
        Convert Pandas dtype to SQLAlchemy SQL type
        
        Mapping rules:
        - int64 ‚Üí Integer
        - float64 ‚Üí Float
        - object (strings) ‚Üí String(255) or Text
        - datetime64 ‚Üí DateTime
        - bool ‚Üí Boolean
        - category ‚Üí String(100)
        """
        dtype_str = str(dtype)
        
        if 'int' in dtype_str:
            return Integer
        elif 'float' in dtype_str:
            return Float
        elif 'datetime' in dtype_str:
            return DateTime
        elif dtype_str == 'bool':
            return Boolean
        elif dtype_str == 'category':
            return String(100)  # Categories usually short
        else:  # object (strings)
            # Estimate string length from sample
            # Check max length in first 1000 rows
            return String(255)  # Default string length
    
    # Create table definition
    # --------------------------------------------------------
    columns = []
    
    for col_name in df.columns:
        col_type = pandas_dtype_to_sql_type(df[col_name].dtype)
        
        # Create Column object
        col = Column(
            col_name,
            # name: column name
            
            col_type,
            # type_: SQLAlchemy type
            
            nullable=True,
            # nullable: allow NULL values
            # - True: column can be NULL
            # - False: NOT NULL constraint
            # - Best practice: True (flexible), use validation instead
        )
        
        columns.append(col)
    
    # Define table
    table = Table(
        target_table,
        # name: table name
        
        metadata,
        # metadata: MetaData container
        
        *columns,
        # *columns: unpack list of Column objects
        
        schema=target_schema,
        # schema: database schema
    )
    
    logger.info(f"‚úÖ Schema prepared with {len(columns)} columns")
    
    # ============================================================
    # STEP 5: CREATE/UPDATE TABLE
    # ============================================================
    logger.info(f"üî® [STEP 5/11] Creating/updating table...")
    
    try:
        if load_strategy == 'replace' and table_exists:
            # Drop existing table
            logger.info(f"  - Dropping existing table...")
            
            with engine.begin() as connection:
                # engine.begin(): start transaction
                # - Context manager: auto-commit on success, auto-rollback on error
                # - Best practice: use transactions for schema changes
                
                connection.execute(text(f'DROP TABLE IF EXISTS {target_schema}.{target_table} CASCADE'))
                # CASCADE: drop dependent objects (views, foreign keys)
                # - Required: ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ dependencies
                # - Alternative: RESTRICT (fail if dependencies exist)
            
            logger.info(f"  - Table dropped")
            table_exists = False
        
        if not table_exists:
            # Create new table
            logger.info(f"  - Creating table...")
            
            metadata.create_all(engine, tables=[table])
            # create_all(): create tables in database
            # - tables: list of specific tables to create
            # - None: create all tables in metadata
            # - Idempotent: won't error if table exists
            
            logger.info(f"  ‚úÖ Table created: {target_schema}.{target_table}")
        else:
            logger.info(f"  - Using existing table")
            
            # TODO: Handle schema evolution (column add/remove)
            # In production, compare existing schema with new schema
            # and alter table if needed
        
    except SQLAlchemyError as e:
        error_msg = f"‚ùå Error creating table: {str(e)}"
        logger.error(error_msg)
        raise
    
    # ============================================================
    # STEP 6: LOAD DATA TO POSTGRESQL
    # ============================================================
    logger.info(f"üìä [STEP 6/11] Loading data to PostgreSQL...")
    logger.info(f"  - Strategy: {load_strategy}")
    logger.info(f"  - Batch size: {batch_size:,}")
    
    load_start_time = time.time()
    rows_loaded = 0
    
    try:
        if load_strategy in ['replace', 'append']:
            # Use pandas to_sql for simple insert
            # -------------------------------------
            
            # Determine if_exists parameter
            if_exists_param = 'replace' if load_strategy == 'replace' else 'append'
            
            df.to_sql(
                name=target_table,
                # name: table name
                
                con=engine,
                # con: SQLAlchemy engine or connection
                
                schema=target_schema,
                # schema: database schema
                
                if_exists=if_exists_param,
                # if_exists: what to do if table exists
                # - 'fail': raise error (default)
                # - 'replace': drop table and recreate
                # - 'append': insert into existing table
                
                index=False,
                # index: write DataFrame index as column
                # - False: don't write index (recommended)
                # - True: write index (rarely needed)
                
                method='multi',
                # method: insert method
                # - None: individual INSERT statements (slowest)
                # - 'multi': multi-row INSERT (faster)
                # - callable: custom insert function
                # - 'multi' format: INSERT INTO table VALUES (row1), (row2), ...
                # - Performance: 'multi' is 5-10x faster than None
                
                chunksize=batch_size,
                # chunksize: rows per INSERT statement
                # - None: insert all at once (memory intensive)
                # - int: insert in batches (recommended)
                # - Trade-off: smaller chunks = more transactions
            )
            
            rows_loaded = len(df)
            logger.info(f"  ‚úÖ Loaded {rows_loaded:,} rows")
            
        elif load_strategy == 'upsert':
            # Use PostgreSQL ON CONFLICT for upsert
            # --------------------------------------
            
            logger.info(f"  - Upsert keys: {upsert_keys}")
            
            # Process in batches
            num_batches = (len(df) + batch_size - 1) // batch_size
            # Ceiling division: (n + d - 1) // d
            # Alternative: math.ceil(len(df) / batch_size)
            
            logger.info(f"  - Processing {num_batches} batches...")
            
            for batch_num in range(num_batches):
                start_idx = batch_num * batch_size
                end_idx = min((batch_num + 1) * batch_size, len(df))
                
                batch_df = df.iloc[start_idx:end_idx]
                # .iloc[start:end]: slice rows by integer position
                
                # Convert to records (list of dicts)
                records = batch_df.to_dict('records')
                # .to_dict('records'): list of row dicts
                # Format: [{'col1': val1, 'col2': val2}, ...]
                
                with engine.begin() as connection:
                    # Create INSERT statement
                    stmt = insert(table).values(records)
                    # insert(): PostgreSQL INSERT statement
                    # - dialect-specific (from sqlalchemy.dialects.postgresql)
                    # - values(): list of dicts to insert
                    
                    # Add ON CONFLICT clause
                    # UPDATE all columns except keys
                    update_cols = {
                        col: stmt.excluded[col]
                        for col in batch_df.columns
                        if col not in upsert_keys
                    }
                    # stmt.excluded: reference to proposed values
                    # - Used in ON CONFLICT DO UPDATE
                    # - Format: {col: stmt.excluded.col}
                    
                    stmt = stmt.on_conflict_do_update(
                        index_elements=upsert_keys,
                        # index_elements: columns ‡∏ó‡∏µ‡πà define conflict
                        # - Must have UNIQUE constraint or index
                        # - Composite key: list of columns
                        
                        set_=update_cols,
                        # set_: columns to update on conflict
                        # - dict: {col: value}
                        # - value: usually stmt.excluded.col
                    )
                    # ON CONFLICT (keys) DO UPDATE SET col1=excluded.col1, ...
                    
                    connection.execute(stmt)
                
                rows_loaded += len(batch_df)
                
                if (batch_num + 1) % 10 == 0:
                    logger.info(f"    Processed {batch_num + 1}/{num_batches} batches ({rows_loaded:,} rows)")
            
            logger.info(f"  ‚úÖ Upserted {rows_loaded:,} rows")
        
    except SQLAlchemyError as e:
        error_msg = f"‚ùå Error loading data: {str(e)}"
        logger.error(error_msg)
        raise
    
    load_duration = time.time() - load_start_time
    rows_per_second = rows_loaded / load_duration if load_duration > 0 else 0
    
    logger.info(f"  - Load duration: {load_duration:.2f}s")
    logger.info(f"  - Throughput: {rows_per_second:,.0f} rows/sec")
    
    # ============================================================
    # STEP 7: VERIFY DATA INTEGRITY
    # ============================================================
    logger.info(f"üîç [STEP 7/11] Verifying data integrity...")
    
    try:
        with engine.connect() as connection:
            # Count rows in table
            result = connection.execute(text(f'SELECT COUNT(*) FROM {target_schema}.{target_table}'))
            db_row_count = result.fetchone()[0]
            
            logger.info(f"  - Rows in database: {db_row_count:,}")
            
            if load_strategy == 'replace':
                # Row counts should match exactly
                if db_row_count != rows_to_load:
                    logger.warning(f"  ‚ö†Ô∏è  Row count mismatch: loaded {rows_to_load:,}, found {db_row_count:,}")
                else:
                    logger.info(f"  ‚úÖ Row count matches")
            elif load_strategy == 'append':
                # Database should have at least the loaded rows
                if db_row_count < rows_to_load:
                    logger.error(f"  ‚ùå Database has fewer rows than loaded!")
                else:
                    logger.info(f"  ‚úÖ Rows appended successfully")
            else:  # upsert
                logger.info(f"  ‚úÖ Upsert completed (total rows: {db_row_count:,})")
            
            # Get table size
            size_query = f"""
                SELECT pg_total_relation_size('{target_schema}.{target_table}')
            """
            # pg_total_relation_size(): total size including indexes, TOAST
            # - Returns: size in bytes
            # - Alternative: pg_relation_size() (table only, no indexes)
            
            result = connection.execute(text(size_query))
            table_size_bytes = result.fetchone()[0]
            table_size_mb = table_size_bytes / (1024 * 1024)
            
            logger.info(f"  - Table size: {table_size_mb:.2f} MB")
        
    except SQLAlchemyError as e:
        logger.warning(f"  ‚ö†Ô∏è  Could not verify integrity: {str(e)}")
        table_size_mb = 0
        db_row_count = rows_loaded
    
    # ============================================================
    # STEP 8: CREATE INDEXES
    # ============================================================
    logger.info(f"üîë [STEP 8/11] Creating indexes...")
    
    indexes_created = []
    
    if create_indexes:
        # Auto-detect index columns if not specified
        if index_columns is None:
            index_columns = []
            
            # Add datetime columns
            for col in df.select_dtypes(include=['datetime64']).columns:
                index_columns.append(col)
            
            # Add columns with '_id' in name (likely foreign keys)
            for col in df.columns:
                if '_id' in col.lower() and col not in index_columns:
                    index_columns.append(col)
            
            logger.info(f"  - Auto index columns: {index_columns}")
    if index_columns:
        try:
            with engine.begin() as connection:
                for col in index_columns:
                    if col not in df.columns:
                        logger.warning(f"    ‚ö†Ô∏è  Column {col} not found, skipping index")
                        continue
                    
                    # Create index name
                    # Convention: idx_{table}_{column}
                    index_name = f'idx_{target_table}_{col}'
                    
                    # Check if index already exists
                    existing_indexes = inspector.get_indexes(target_table, schema=target_schema)
                    # get_indexes(): list of index definitions
                    # Returns: [{'name': 'idx_name', 'column_names': ['col'], ...}, ...]
                    
                    if any(idx['name'] == index_name for idx in existing_indexes):
                        logger.info(f"    - Index {index_name} already exists")
                        continue
                    
                    # Create index
                    create_index_sql = f'''
                        CREATE INDEX {index_name}
                        ON {target_schema}.{target_table} ({col})
                    '''
                    # CREATE INDEX syntax:
                    # CREATE INDEX name ON table (column)
                    # Options:
                    # - UNIQUE: unique index
                    # - CONCURRENTLY: don't block writes (PostgreSQL)
                    # - USING method: btree (default), hash, gist, gin
                    
                    connection.execute(text(create_index_sql))
                    
                    indexes_created.append(index_name)
                    logger.info(f"    ‚úÖ Created index: {index_name}")
            
            logger.info(f"  ‚úÖ Created {len(indexes_created)} indexes")
            
        except SQLAlchemyError as e:
            logger.warning(f"  ‚ö†Ô∏è  Error creating indexes: {str(e)}")
            # Don't fail task - indexes are optimization, not critical
    else:
        logger.info(f"  - No index columns specified")
else:
    logger.info(f"  - Index creation skipped")

# ============================================================
# STEP 9: SAVE TO DATA LAKE (PARQUET)
# ============================================================
logger.info(f"üíæ [STEP 9/11] Saving to Data Lake...")

# Create Data Lake directory structure
datalake_dir = Path('/opt/airflow/data/datalake')
datalake_dir.mkdir(parents=True, exist_ok=True)

# Add timestamp columns for partitioning if not present
if partition_by:
    for col in partition_by:
        if col not in df.columns:
            # Try to extract from date column
            if 'date' in df.columns or 'timestamp' in df.columns:
                date_col = 'date' if 'date' in df.columns else 'timestamp'
                
                if pd.api.types.is_datetime64_any_dtype(df[date_col]):
                    if col == 'year':
                        df['year'] = df[date_col].dt.year
                        # .dt.year: extract year from datetime
                        # - Returns: int64 Series
                        # - .dt accessor: datetime-specific methods
                        
                    elif col == 'month':
                        df['month'] = df[date_col].dt.month
                        # .dt.month: extract month (1-12)
                        
                    elif col == 'day':
                        df['day'] = df[date_col].dt.day
                        # .dt.day: extract day (1-31)
                    
                    logger.info(f"    Created partition column: {col}")

try:
    if partition_by:
        # Save with partitioning
        parquet_path = datalake_dir / target_table
        
        df.to_parquet(
            path=str(parquet_path),
            engine='pyarrow',
            compression='snappy',
            index=False,
            partition_cols=partition_by,
            # partition_cols: columns to partition by
            # - Creates directory structure: col1=val1/col2=val2/
            # - Example: year=2024/month=12/data.parquet
            
            # Advanced options:
            # partition_filename_cb: custom filename function
            # - Default: auto-generated UUID
            # - Example: lambda x: f"data_{x}.parquet"
        )
        
        logger.info(f"  ‚úÖ Saved partitioned Parquet: {parquet_path}")
        logger.info(f"    Partitioned by: {partition_by}")
        
        # Calculate total size of partitioned files
        parquet_size_bytes = sum(f.stat().st_size for f in parquet_path.rglob('*.parquet'))
        # .rglob('*.parquet'): recursive glob (all subdirectories)
        
    else:
        # Save as single file or auto-partitioned
        timestamp_str = datetime.now().strftime('%Y%m%d_%H%M%S')
        parquet_filename = f'{target_table}_{timestamp_str}.parquet'
        parquet_path = datalake_dir / parquet_filename
        
        df.to_parquet(
            path=str(parquet_path),
            engine='pyarrow',
            compression='snappy',
            index=False
        )
        
        logger.info(f"  ‚úÖ Saved Parquet: {parquet_path}")
        
        parquet_size_bytes = parquet_path.stat().st_size
    
    parquet_size_mb = parquet_size_bytes / (1024 * 1024)
    logger.info(f"    Size: {parquet_size_mb:.2f} MB")
    
except Exception as e:
    logger.warning(f"  ‚ö†Ô∏è  Error saving to Data Lake: {str(e)}")
    parquet_path = None
    parquet_size_mb = 0

# ============================================================
# STEP 10: UPDATE METADATA TABLE
# ============================================================
logger.info(f"üìù [STEP 10/11] Updating metadata...")

try:
    metadata_table = 'etl_metadata'
    
    # Create metadata table if not exists
    metadata_create_sql = f'''
        CREATE TABLE IF NOT EXISTS {target_schema}.{metadata_table} (
            id SERIAL PRIMARY KEY,
            table_name VARCHAR(255),
            load_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            load_strategy VARCHAR(50),
            rows_loaded INTEGER,
            source_file VARCHAR(500),
            status VARCHAR(50),
            duration_seconds FLOAT,
            table_size_mb FLOAT,
            error_message TEXT
        )
    '''
    # SERIAL: auto-increment integer
    # - Equivalent: INTEGER AUTO_INCREMENT (MySQL)
    # - Generates: sequence for ID generation
    
    with engine.begin() as connection:
        connection.execute(text(metadata_create_sql))
        
        # Insert metadata record
        metadata_insert_sql = f'''
            INSERT INTO {target_schema}.{metadata_table}
            (table_name, load_strategy, rows_loaded, source_file, status, duration_seconds, table_size_mb)
            VALUES (:table, :strategy, :rows, :source, :status, :duration, :size)
        '''
        # Parameterized query: use :param_name for parameters
        # - Prevents SQL injection
        # - Automatic type conversion
        
        connection.execute(
            text(metadata_insert_sql),
            {
                'table': target_table,
                'strategy': load_strategy,
                'rows': rows_loaded,
                'source': str(data_path),
                'status': 'success',
                'duration': round(load_duration, 2),
                'size': round(table_size_mb, 2)
            }
        )
    
    logger.info(f"  ‚úÖ Metadata updated")
    
except SQLAlchemyError as e:
    logger.warning(f"  ‚ö†Ô∏è  Could not update metadata: {str(e)}")

# ============================================================
# STEP 11: ARCHIVE SOURCE FILES
# ============================================================
logger.info(f"üì¶ [STEP 11/11] Archiving source files...")

if archive_source:
    try:
        archive_dir = Path('/opt/airflow/data/archive')
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        # Create timestamped archive subdirectory
        archive_subdir = archive_dir / datetime.now().strftime('%Y%m%d')
        archive_subdir.mkdir(exist_ok=True)
        
        # Find original raw file
        raw_dir = Path('/opt/airflow/data/raw')
        raw_files = list(raw_dir.glob('*.csv'))
        
        for raw_file in raw_files:
            archive_dest = archive_subdir / raw_file.name
            
            # Move file
            shutil.move(str(raw_file), str(archive_dest))
            # shutil.move(): move file
            # - Similar to: os.rename() but works across filesystems
            # - Alternative: shutil.copy2() + os.remove() (preserve metadata)
            
            logger.info(f"  - Archived: {raw_file.name}")
        
        logger.info(f"  ‚úÖ Files archived to {archive_subdir}")
        
    except Exception as e:
        logger.warning(f"  ‚ö†Ô∏è  Error archiving files: {str(e)}")
else:
    logger.info(f"  - Archive skipped")

# ============================================================
# FINAL SUMMARY
# ============================================================
end_time = time.time()
total_duration = end_time - start_time

logger.info(f"")
logger.info(f"{'='*60}")
logger.info(f"LOADING SUMMARY")
logger.info(f"{'='*60}")
logger.info(f"Table:           {target_schema}.{target_table}")
logger.info(f"Strategy:        {load_strategy}")
logger.info(f"Rows loaded:     {rows_loaded:,}")
logger.info(f"Database size:   {table_size_mb:.2f} MB")
logger.info(f"Parquet size:    {parquet_size_mb:.2f} MB")
logger.info(f"Indexes created: {len(indexes_created)}")
logger.info(f"Total duration:  {total_duration:.2f}s")
logger.info(f"{'='*60}")

# Create metadata
metadata = {
    'status': 'success',
    'table_name': f'{target_schema}.{target_table}',
    'load_strategy': load_strategy,
    'rows_loaded': rows_loaded,
    'database_row_count': db_row_count,
    'database_size_mb': round(table_size_mb, 2),
    'parquet_path': str(parquet_path) if parquet_path else None,
    'parquet_size_mb': round(parquet_size_mb, 2),
    'indexes_created': indexes_created,
    'partitioned_by': partition_by,
    'load_duration_seconds': round(load_duration, 2),
    'total_duration_seconds': round(total_duration, 2),
    'rows_per_second': round(rows_per_second, 0),
    'timestamp': datetime.now().isoformat()
}

logger.info(f"‚úÖ Data loading completed successfully!")

return metadata

---

### üéì Key Learnings from Task 5

**1. Loading Strategies**:
- **Replace**: Full refresh, simple, no history
- **Append**: Incremental, fast, preserves history
- **Upsert**: Complex, handles updates, uses ON CONFLICT

**2. Database Best Practices**:
- **Connection Pooling**: Reuse connections (pool_size, max_overflow)
- **Transactions**: Use engine.begin() for atomicity
- **Batch Processing**: Insert in chunks (10K-50K rows)
- **Parameterized Queries**: Prevent SQL injection

**3. Performance Optimization**:
- **Batch Size**: Balance memory vs speed
- **method='multi'**: 5-10x faster than individual inserts
- **Indexes**: Create after bulk load (faster)
- **COPY command**: Fastest (alternative to to_sql)

**4. Data Lake Strategy**:
- **Partitioning**: By time (year/month) or dimension (country)
- **Compression**: Snappy (balanced) vs Gzip (smaller)
- **Organization**: Consistent directory structure

**5. Production Considerations**:
- **Rollback Plan**: Transaction-based, backup before replace
- **Monitoring**: Track throughput, duration, size
- **Metadata**: Audit trail of all loads
- **Archive**: Move processed files, retention policy

---

### ‚úÖ Testing Task 5
```python
# tests/test_loading.py
import pytest
from dags.tasks.loading import load_data
from sqlalchemy import create_engine, text

def test_load_replace_strategy():
    """Test full replace load"""
    validation_metadata = {
        'status': 'PASS',
        'summary': {'critical_errors': 0}
    }
    
    metadata = load_data(
        validation_metadata=validation_metadata,
        target_table='test_table',
        load_strategy='replace'
    )
    
    assert metadata['status'] == 'success'
    assert metadata['rows_loaded'] > 0

def test_load_with_indexes():
    """Test index creation"""
    validation_metadata = {
        'status': 'PASS',
        'summary': {'critical_errors': 0}
    }
    
    metadata = load_data(
        validation_metadata=validation_metadata,
        target_table='test_table',
        create_indexes=True,
        index_columns=['customer_id', 'order_date']
    )
    
    assert len(metadata['indexes_created']) >= 2

def test_load_partitioned_parquet():
    """Test partitioned Data Lake save"""
    validation_metadata = {
        'status': 'PASS',
        'summary': {'critical_errors': 0}
    }
    
    metadata = load_data(
        validation_metadata=validation_metadata,
        partition_by=['year', 'month']
    )
    
    assert metadata['partitioned_by'] == ['year', 'month']
    assert metadata['parquet_path'] is not None
```

---

## üéâ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏£‡∏ö‡∏ó‡∏±‡πâ‡∏á 5 Tasks ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á ETL Pipeline ‡πÅ‡∏•‡πâ‡∏ß! 

**‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏≠‡∏¢‡∏π‡πà‡∏Ñ‡∏∑‡∏≠ Phase 5: Production Features**:
1. Error Handling & Retry Logic
2. Monitoring & Logging
3. Configuration Management
4. Testing
5. Documentation

‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏â‡∏±‡∏ô‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ Phase 5 ‡∏ï‡πà‡∏≠‡πÑ‡∏´‡∏°? ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö 5 Tasks ‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏°‡∏≤? üöÄ
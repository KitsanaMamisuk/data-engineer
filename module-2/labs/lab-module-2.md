# Lab Module 2: ETL - Data Loading & Cleansing

**üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:**
- ‡∏ù‡∏∂‡∏Å‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö Optimized
- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Memory Saving ‡∏à‡∏≤‡∏Å dtype optimization
- ‡∏ù‡∏∂‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á‡πÜ
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Integrity ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Quality Report

**‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** 1.5 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

**üìã ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°:**
- Python 3.8+
- Pandas, NumPy ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
- Jupyter Notebook ‡∏´‡∏£‡∏∑‡∏≠ Python IDE

---

## üìö ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [Setup & Data Preparation](#setup--data-preparation)
2. [Lab 2.1: Optimized CSV Loading](#lab-21-optimized-csv-loading)
3. [Lab 2.2: Missing Value Handling](#lab-22-missing-value-handling)
4. [Lab 2.3: Data Validation & Integrity](#lab-23-data-validation--integrity)
5. [Challenge: Complete ETL Pipeline](#challenge-complete-etl-pipeline)
6. [Checkpoint & Submission](#checkpoint--submission)

---

## Setup & Data Preparation

### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Lab Directory

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Lab
mkdir -p ~/data-engineer-labs/module-2
cd ~/data-engineer-labs/module-2

# ‡∏™‡∏£‡πâ‡∏≤‡∏á data directory
mkdir data
```

### Step 2: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `generate_sensor_data.py`:

```python
"""
Generate sensor_large.csv for Lab 2
"""
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

def generate_sensor_data(num_records=100000):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• IoT Sensor ‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á

    Parameters:
        num_records: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô records (default: 100,000)

    Output:
        data/sensor_large.csv
    """
    print(f"Generating {num_records:,} sensor records...")

    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ random seed ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö reproducibility
    np.random.seed(42)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á timestamps (‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ)
    start_date = datetime(2025, 1, 1, 0, 0, 0)
    timestamps = [start_date + timedelta(minutes=5*i) for i in range(num_records)]

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á sensor IDs (20 sensors)
    sensor_ids = [f"S{i:03d}" for i in range(1, 21)]
    sensor_data = np.random.choice(sensor_ids, num_records)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• temperature (15-35¬∞C with pattern)
    base_temp = 25.0
    temp_variation = np.sin(np.linspace(0, 10*np.pi, num_records)) * 5  # Daily pattern
    temp_noise = np.random.normal(0, 1, num_records)
    temperature = base_temp + temp_variation + temp_noise

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• humidity (40-80% with correlation to temp)
    base_humidity = 60.0
    humidity_variation = -temp_variation * 0.8  # Inverse correlation
    humidity_noise = np.random.normal(0, 2, num_records)
    humidity = base_humidity + humidity_variation + humidity_noise
    humidity = np.clip(humidity, 0, 100).astype(int)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• pressure (1000-1020 hPa)
    pressure = np.random.normal(1013, 5, num_records)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á status (OK, WARNING, ERROR)
    status_probs = [0.85, 0.10, 0.05]  # 85% OK, 10% WARNING, 5% ERROR
    status = np.random.choice(['OK', 'WARNING', 'ERROR'], num_records, p=status_probs)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
    df = pd.DataFrame({
        'timestamp': timestamps,
        'sensor_id': sensor_data,
        'temperature': temperature,
        'humidity': humidity,
        'pressure': pressure,
        'status': status
    })

    # ‡πÅ‡∏ó‡∏£‡∏Å Missing Values (~5%)
    missing_indices = np.random.choice(num_records, size=int(num_records * 0.05), replace=False)
    df.loc[missing_indices[:len(missing_indices)//3], 'temperature'] = np.nan
    df.loc[missing_indices[len(missing_indices)//3:2*len(missing_indices)//3], 'humidity'] = np.nan
    df.loc[missing_indices[2*len(missing_indices)//3:], 'pressure'] = np.nan

    # ‡πÅ‡∏ó‡∏£‡∏Å Outliers (~1%)
    outlier_indices = np.random.choice(num_records, size=int(num_records * 0.01), replace=False)
    df.loc[outlier_indices, 'temperature'] = np.random.choice([150, -100, 999])

    # ‡πÅ‡∏ó‡∏£‡∏Å Duplicates (~0.5%)
    dup_indices = np.random.choice(num_records-100, size=int(num_records * 0.005), replace=False)
    duplicates = df.iloc[dup_indices].copy()
    df = pd.concat([df, duplicates], ignore_index=True)
    df = df.sample(frac=1).reset_index(drop=True)  # Shuffle

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
    output_path = 'data/sensor_large.csv'
    df.to_csv(output_path, index=False)

    file_size = os.path.getsize(output_path) / 1024**2  # MB
    print(f"‚úÖ Generated {len(df):,} records")
    print(f"‚úÖ File size: {file_size:.2f} MB")
    print(f"‚úÖ Saved to: {output_path}")

    return df

if __name__ == "__main__":
    import os
    df = generate_sensor_data(100000)

    # ‡πÅ‡∏™‡∏î‡∏á Preview
    print("\nData Preview:")
    print(df.head(10))

    print("\nData Info:")
    print(df.info(memory_usage='deep'))

    print("\nMissing Values:")
    print(df.isnull().sum())
```

### Step 3: ‡∏£‡∏±‡∏ô‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

```bash
python generate_sensor_data.py
```

**Expected Output:**
```
Generating 100,000 sensor records...
‚úÖ Generated 100,500 records
‚úÖ File size: 8.23 MB
‚úÖ Saved to: data/sensor_large.csv
```

---

## Lab 2.1: Optimized CSV Loading

### üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡πÇ‡∏´‡∏•‡∏î `sensor_large.csv` ‡πÅ‡∏ö‡∏ö Optimized ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Memory Saving

### Step 1: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥ (Baseline)

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `lab_2_1_loading.py`:

```python
"""
Lab 2.1: Optimized CSV Loading
"""
import pandas as pd
import numpy as np
import time

# ===========================
# Part 1: Normal Loading
# ===========================

print("=" * 60)
print("Part 1: Normal Loading (Baseline)")
print("=" * 60)

start_time = time.time()

# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥ - ‡πÑ‡∏°‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype
df_normal = pd.read_csv('data/sensor_large.csv')

load_time_normal = time.time() - start_time

# ‡∏ß‡∏±‡∏î Memory Usage
memory_normal = df_normal.memory_usage(deep=True).sum() / 1024**2  # MB

print(f"\nüìä Loading Results (Normal):")
print(f"   Rows:        {len(df_normal):,}")
print(f"   Columns:     {len(df_normal.columns)}")
print(f"   Load Time:   {load_time_normal:.3f} seconds")
print(f"   Memory:      {memory_normal:.2f} MB")

print(f"\nüìã Data Types:")
print(df_normal.dtypes)

print(f"\nüìà Memory Usage per Column:")
print(df_normal.memory_usage(deep=True) / 1024**2)  # MB
```

**Expected Output:**
```
============================================================
Part 1: Normal Loading (Baseline)
============================================================

üìä Loading Results (Normal):
   Rows:        100,500
   Columns:     6
   Load Time:   0.245 seconds
   Memory:      8.12 MB

üìã Data Types:
timestamp      object
sensor_id      object
temperature    float64
humidity       int64
pressure       float64
status         object
dtype: object

üìà Memory Usage per Column:
Index          0.77
timestamp      6.14
sensor_id      6.14
temperature    0.77
humidity       0.77
pressure       0.77
status         6.14
dtype: float64
```

### Step 2: ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö Optimized

‡πÄ‡∏û‡∏¥‡πà‡∏° code ‡∏ï‡πà‡∏≠‡πÉ‡∏ô `lab_2_1_loading.py`:

```python
# ===========================
# Part 2: Optimized Loading
# ===========================

print("\n" + "=" * 60)
print("Part 2: Optimized Loading")
print("=" * 60)

start_time = time.time()

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype specification
dtype_spec = {
    'sensor_id': 'category',      # 20 unique values ‚Üí category
    'temperature': 'float32',     # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á
    'humidity': 'int16',          # 0-100 ‚Üí int16
    'pressure': 'float32',        # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á
    'status': 'category'          # 3 unique values ‚Üí category
}

# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö Optimized
df_optimized = pd.read_csv(
    'data/sensor_large.csv',
    dtype=dtype_spec,
    parse_dates=['timestamp']     # ‡πÅ‡∏õ‡∏•‡∏á timestamp ‡πÄ‡∏õ‡πá‡∏ô datetime
)

load_time_optimized = time.time() - start_time

# ‡∏ß‡∏±‡∏î Memory Usage
memory_optimized = df_optimized.memory_usage(deep=True).sum() / 1024**2  # MB

print(f"\nüìä Loading Results (Optimized):")
print(f"   Rows:        {len(df_optimized):,}")
print(f"   Columns:     {len(df_optimized.columns)}")
print(f"   Load Time:   {load_time_optimized:.3f} seconds")
print(f"   Memory:      {memory_optimized:.2f} MB")

print(f"\nüìã Data Types:")
print(df_optimized.dtypes)

print(f"\nüìà Memory Usage per Column:")
print(df_optimized.memory_usage(deep=True) / 1024**2)  # MB
```

### Step 3: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Savings

‡πÄ‡∏û‡∏¥‡πà‡∏° code ‡∏ï‡πà‡∏≠:

```python
# ===========================
# Part 3: Comparison
# ===========================

print("\n" + "=" * 60)
print("Part 3: Comparison & Savings")
print("=" * 60)

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Savings
memory_saving = memory_normal - memory_optimized
memory_saving_percent = (memory_saving / memory_normal) * 100

time_diff = load_time_normal - load_time_optimized
time_diff_percent = (time_diff / load_time_normal) * 100

print(f"\nüíæ Memory Comparison:")
print(f"   Normal:        {memory_normal:.2f} MB")
print(f"   Optimized:     {memory_optimized:.2f} MB")
print(f"   Saving:        {memory_saving:.2f} MB ({memory_saving_percent:.1f}%)")

print(f"\n‚è±Ô∏è  Time Comparison:")
print(f"   Normal:        {load_time_normal:.3f} seconds")
print(f"   Optimized:     {load_time_optimized:.3f} seconds")
print(f"   Difference:    {time_diff:.3f} seconds ({time_diff_percent:.1f}%)")

# ‡∏™‡∏£‡πâ‡∏≤‡∏á Summary Table
summary = pd.DataFrame({
    'Metric': ['Rows', 'Memory (MB)', 'Load Time (s)'],
    'Normal': [len(df_normal), f"{memory_normal:.2f}", f"{load_time_normal:.3f}"],
    'Optimized': [len(df_optimized), f"{memory_optimized:.2f}", f"{load_time_optimized:.3f}"],
    'Saving (%)': [
        '0%',
        f"{memory_saving_percent:.1f}%",
        f"{time_diff_percent:.1f}%" if time_diff > 0 else '0%'
    ]
})

print(f"\nüìä Summary Table:")
print(summary.to_string(index=False))

# Visualization (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Jupyter)
print(f"\nüí° Optimization Tips:")
print("   ‚úÖ Use 'category' for repeated strings")
print("   ‚úÖ Use float32 instead of float64 (save 50%)")
print("   ‚úÖ Use int8/16 for small ranges")
print("   ‚úÖ Parse dates with parse_dates parameter")
```

**Expected Output:**
```
============================================================
Part 3: Comparison & Savings
============================================================

üíæ Memory Comparison:
   Normal:        8.12 MB
   Optimized:     1.89 MB
   Saving:        6.23 MB (76.7%)

‚è±Ô∏è  Time Comparison:
   Normal:        0.245 seconds
   Optimized:     0.198 seconds
   Difference:    0.047 seconds (19.2%)

üìä Summary Table:
   Metric  Normal Optimized Saving (%)
     Rows  100500   100500         0%
Memory (MB)   8.12     1.89      76.7%
Load Time (s)  0.245    0.198      19.2%
```

### üìù Checkpoint 2.1

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**
1. Memory Saving ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà? (‡∏Ñ‡∏ß‡∏£‡πÑ‡∏î‡πâ >70%)
2. Column ‡πÑ‡∏´‡∏ô‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Memory ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î? ‡∏ó‡∏≥‡πÑ‡∏°?
3. ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô `sensor_id` ‡∏à‡∏≤‡∏Å category ‡πÄ‡∏õ‡πá‡∏ô object ‡∏à‡∏∞‡πÄ‡∏Å‡∏¥‡∏î‡∏≠‡∏∞‡πÑ‡∏£‡∏Ç‡∏∂‡πâ‡∏ô?

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö:**
```python
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô dtype
df_test = df_optimized.copy()
df_test['sensor_id'] = df_test['sensor_id'].astype('object')
memory_test = df_test.memory_usage(deep=True).sum() / 1024**2
print(f"Memory with object: {memory_test:.2f} MB")
print(f"Memory increase: {((memory_test - memory_optimized) / memory_optimized * 100):.1f}%")
```

---

## Lab 2.2: Missing Value Handling

### üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡∏ù‡∏∂‡∏Å‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

### Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `lab_2_2_missing_values.py`:

```python
"""
Lab 2.2: Missing Value Handling
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÅ‡∏ö‡∏ö Optimized)
dtype_spec = {
    'sensor_id': 'category',
    'temperature': 'float32',
    'humidity': 'int16',
    'pressure': 'float32',
    'status': 'category'
}

df = pd.read_csv(
    'data/sensor_large.csv',
    dtype=dtype_spec,
    parse_dates=['timestamp']
)

# ‡∏ï‡∏±‡πâ‡∏á timestamp ‡πÄ‡∏õ‡πá‡∏ô index
df.set_index('timestamp', inplace=True)
df.sort_index(inplace=True)

print("=" * 60)
print("Lab 2.2: Missing Value Analysis")
print("=" * 60)

# ===========================
# Part 1: Detect Missing Values
# ===========================

print("\nüìä Part 1: Missing Value Detection")
print("-" * 60)

# ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Missing
missing_count = df.isnull().sum()
missing_percent = (df.isnull().sum() / len(df) * 100).round(2)

missing_report = pd.DataFrame({
    'Column': df.columns,
    'Missing Count': missing_count.values,
    'Missing %': missing_percent.values
})

print("\nüìã Missing Values Report:")
print(missing_report.to_string(index=False))

# ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing
print(f"\nüîç Total rows with missing values: {df.isnull().any(axis=1).sum():,}")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing
print("\nüìÑ Sample rows with missing values:")
print(df[df.isnull().any(axis=1)].head(10))
```

### Step 2: ‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á‡πÜ

‡πÄ‡∏û‡∏¥‡πà‡∏° code:

```python
# ===========================
# Part 2: Handling Strategies
# ===========================

print("\n" + "=" * 60)
print("Part 2: Missing Value Handling Strategies")
print("=" * 60)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ temperature column ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏•‡∏≠‡∏á
temp_original = df['temperature'].copy()
temp_with_missing = temp_original.isnull().sum()

print(f"\nüå°Ô∏è  Temperature Column:")
print(f"   Total values:    {len(temp_original):,}")
print(f"   Missing values:  {temp_with_missing:,} ({temp_with_missing/len(temp_original)*100:.2f}%)")

# Strategy 1: Drop
print("\n" + "-" * 60)
print("Strategy 1: DROP")
print("-" * 60)

temp_dropped = temp_original.dropna()
print(f"   Remaining values: {len(temp_dropped):,}")
print(f"   Lost values:      {len(temp_original) - len(temp_dropped):,}")
print(f"   Mean (before):    {temp_original.mean():.2f}¬∞C")
print(f"   Mean (after):     {temp_dropped.mean():.2f}¬∞C")

# Strategy 2: Forward Fill
print("\n" + "-" * 60)
print("Strategy 2: FORWARD FILL (Ffill)")
print("-" * 60)

temp_ffill = temp_original.fillna(method='ffill')
print(f"   Remaining NaN:    {temp_ffill.isnull().sum()}")
print(f"   Mean:             {temp_ffill.mean():.2f}¬∞C")
print(f"   Median:           {temp_ffill.median():.2f}¬∞C")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°
sample_idx = temp_original[temp_original.isnull()].index[0]
window = 5
print(f"\n   Example (forward fill):")
comparison = pd.DataFrame({
    'Original': temp_original.loc[sample_idx-pd.Timedelta(minutes=window):sample_idx+pd.Timedelta(minutes=window)],
    'Ffill': temp_ffill.loc[sample_idx-pd.Timedelta(minutes=window):sample_idx+pd.Timedelta(minutes=window)]
})
print(comparison)

# Strategy 3: Mean Fill
print("\n" + "-" * 60)
print("Strategy 3: MEAN FILL")
print("-" * 60)

temp_mean = temp_original.fillna(temp_original.mean())
print(f"   Remaining NaN:    {temp_mean.isnull().sum()}")
print(f"   Mean:             {temp_mean.mean():.2f}¬∞C")
print(f"   Fill value used:  {temp_original.mean():.2f}¬∞C")

# Strategy 4: Median Fill
print("\n" + "-" * 60)
print("Strategy 4: MEDIAN FILL")
print("-" * 60)

temp_median = temp_original.fillna(temp_original.median())
print(f"   Remaining NaN:    {temp_median.isnull().sum()}")
print(f"   Median:           {temp_median.median():.2f}¬∞C")
print(f"   Fill value used:  {temp_original.median():.2f}¬∞C")

# Strategy 5: Interpolation
print("\n" + "-" * 60)
print("Strategy 5: INTERPOLATION (Linear)")
print("-" * 60)

temp_interpolated = temp_original.interpolate(method='linear')
print(f"   Remaining NaN:    {temp_interpolated.isnull().sum()}")
print(f"   Mean:             {temp_interpolated.mean():.2f}¬∞C")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£ interpolate
print(f"\n   Example (interpolation):")
comparison = pd.DataFrame({
    'Original': temp_original.loc[sample_idx-pd.Timedelta(minutes=window):sample_idx+pd.Timedelta(minutes=window)],
    'Interpolated': temp_interpolated.loc[sample_idx-pd.Timedelta(minutes=window):sample_idx+pd.Timedelta(minutes=window)]
})
print(comparison)
```

### Step 3: ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå

```python
# ===========================
# Part 3: Comparison
# ===========================

print("\n" + "=" * 60)
print("Part 3: Strategy Comparison")
print("=" * 60)

# ‡∏™‡∏£‡πâ‡∏≤‡∏á comparison table
strategies = {
    'Original': temp_original,
    'Drop': temp_dropped,
    'Ffill': temp_ffill,
    'Mean': temp_mean,
    'Median': temp_median,
    'Interpolate': temp_interpolated
}

comparison_df = pd.DataFrame({
    'Strategy': strategies.keys(),
    'Count': [len(s) for s in strategies.values()],
    'Missing': [s.isnull().sum() for s in strategies.values()],
    'Mean': [f"{s.mean():.2f}" for s in strategies.values()],
    'Median': [f"{s.median():.2f}" for s in strategies.values()],
    'Std': [f"{s.std():.2f}" for s in strategies.values()],
    'Min': [f"{s.min():.2f}" for s in strategies.values()],
    'Max': [f"{s.max():.2f}" for s in strategies.values()]
})

print("\nüìä Comparison Table:")
print(comparison_df.to_string(index=False))

# Recommendations
print("\n" + "=" * 60)
print("üí° Recommendations")
print("=" * 60)

print("""
1. DROP:
   ‚úÖ Use when: Missing < 5%
   ‚ùå Avoid when: Losing too much data

2. FORWARD FILL (Ffill):
   ‚úÖ Use when: Time-series with short gaps
   ‚ùå Avoid when: Long gaps (propagates old values)

3. MEAN FILL:
   ‚úÖ Use when: No time dependency
   ‚ùå Avoid when: Time-series (ignores trends)

4. MEDIAN FILL:
   ‚úÖ Use when: Data has outliers
   ‚ùå Avoid when: Time-series (ignores trends)

5. INTERPOLATION:
   ‚úÖ Use when: Time-series with smooth trends
   ‚ùå Avoid when: Irregular patterns

Recommendation for IoT Sensor Data:
‚Üí Use INTERPOLATION or FFILL for temperature/humidity
‚Üí Consider time gaps: if gap > 30 min, use interpolation
""")
```

### Step 4: ‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

```python
# ===========================
# Part 4: Apply to Full Dataset
# ===========================

print("\n" + "=" * 60)
print("Part 4: Apply Strategy to Full Dataset")
print("=" * 60)

def clean_sensor_data(df, strategy='auto'):
    """
    ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• sensor ‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    """
    df_clean = df.copy()

    print(f"\nüßπ Cleaning with strategy: {strategy}")
    print("-" * 60)

    # Temperature & Pressure: Interpolation (time-series)
    print("   Temperature: Interpolation")
    df_clean['temperature'] = df_clean['temperature'].interpolate(method='linear')

    print("   Pressure: Interpolation")
    df_clean['pressure'] = df_clean['pressure'].interpolate(method='linear')

    # Humidity: Forward Fill then Interpolate
    print("   Humidity: Ffill + Interpolation")
    df_clean['humidity'] = df_clean['humidity'].fillna(method='ffill')
    df_clean['humidity'] = df_clean['humidity'].interpolate(method='linear')

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö remaining NaN
    remaining_nan = df_clean.isnull().sum().sum()
    print(f"\n   ‚úÖ Remaining NaN: {remaining_nan}")

    return df_clean

# ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î
df_cleaned = clean_sensor_data(df, strategy='auto')

print("\nüìä Before vs After:")
print("\nBefore:")
print(df.isnull().sum())

print("\nAfter:")
print(df_cleaned.isnull().sum())

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
df_cleaned.to_csv('data/sensor_cleaned.csv')
print(f"\nüíæ Saved cleaned data to: data/sensor_cleaned.csv")
```

### üìù Checkpoint 2.2

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**
1. Strategy ‡πÑ‡∏´‡∏ô‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö Temperature ‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î? ‡∏ó‡∏≥‡πÑ‡∏°?
2. Mean vs Median - ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£? ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Median?
3. Ffill vs Interpolation - ‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ?

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö:**
```python
# ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ gap ‡∏¢‡∏≤‡∏ß
# ‡∏™‡∏£‡πâ‡∏≤‡∏á missing gap 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (24 records)
test_series = temp_original.copy()
gap_start = 1000
test_series.iloc[gap_start:gap_start+24] = np.nan

# ‡∏ó‡∏î‡∏•‡∏≠‡∏á Ffill vs Interpolate
ffill_gap = test_series.fillna(method='ffill')
interp_gap = test_series.interpolate(method='linear')

print(f"Ffill (gap): {ffill_gap.iloc[gap_start:gap_start+24].values[:5]}")
print(f"Interpolate (gap): {interp_gap.iloc[gap_start:gap_start+24].values[:5]}")
```

---

## Lab 2.3: Data Validation & Integrity

### üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Quality ‡πÅ‡∏•‡∏∞ Integrity ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

### Step 1: Basic Validation Checks

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `lab_2_3_validation.py`:

```python
"""
Lab 2.3: Data Validation & Integrity Checks
"""
import pandas as pd
import numpy as np
import json

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà cleaned ‡πÅ‡∏•‡πâ‡∏ß
df = pd.read_csv(
    'data/sensor_cleaned.csv',
    parse_dates=['timestamp'],
    index_col='timestamp'
)

print("=" * 60)
print("Lab 2.3: Data Validation & Integrity")
print("=" * 60)

# ===========================
# Part 1: Data Type Validation
# ===========================

print("\nüìã Part 1: Data Type Validation")
print("-" * 60)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î expected types
expected_types = {
    'sensor_id': 'object',
    'temperature': 'float',
    'humidity': 'int',
    'pressure': 'float',
    'status': 'object'
}

print("\nüîç Type Check Results:")
type_check_passed = True

for col, expected in expected_types.items():
    actual = str(df[col].dtype)
    is_match = False

    if expected == 'float' and 'float' in actual:
        is_match = True
    elif expected == 'int' and 'int' in actual:
        is_match = True
    elif expected == 'object' and 'object' in actual:
        is_match = True
    elif expected == 'category' and 'category' in actual:
        is_match = True

    status = "‚úÖ PASS" if is_match else "‚ùå FAIL"
    print(f"   {col:15s} Expected: {expected:10s} | Actual: {actual:10s} | {status}")

    if not is_match:
        type_check_passed = False

print(f"\n{'‚úÖ All type checks passed' if type_check_passed else '‚ùå Some type checks failed'}")
```

### Step 2: Range Validation

```python
# ===========================
# Part 2: Range Validation
# ===========================

print("\n" + "=" * 60)
print("Part 2: Value Range Validation")
print("=" * 60)

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î valid ranges
range_rules = {
    'temperature': (-50, 100),   # ¬∞C
    'humidity': (0, 100),        # %
    'pressure': (900, 1100)      # hPa
}

print("\nüîç Range Check Results:")
range_violations = {}

for col, (min_val, max_val) in range_rules.items():
    # ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á
    out_of_range = df[
        (df[col] < min_val) | (df[col] > max_val)
    ]

    if len(out_of_range) > 0:
        range_violations[col] = out_of_range
        print(f"   ‚ùå {col:15s} [{min_val:6.1f}, {max_val:6.1f}] ‚Üí {len(out_of_range):,} violations")
        print(f"      Actual range: [{df[col].min():.2f}, {df[col].max():.2f}]")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
        print(f"      Sample violations:")
        print(out_of_range[[col]].head(3).to_string())
    else:
        print(f"   ‚úÖ {col:15s} [{min_val:6.1f}, {max_val:6.1f}] ‚Üí All values valid")

# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Outliers
if range_violations:
    print("\nüí° Outlier Handling:")
    print("   Option 1: Remove outliers")
    print("   Option 2: Clip to range")
    print("   Option 3: Replace with NaN then interpolate")

    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á: Clip to range
    df_cleaned = df.copy()
    for col, (min_val, max_val) in range_rules.items():
        df_cleaned[col] = df_cleaned[col].clip(min_val, max_val)

    print("\n   ‚úÖ Applied clipping to outliers")
else:
    df_cleaned = df.copy()
```

### Step 3: Duplicate Detection

```python
# ===========================
# Part 3: Duplicate Detection
# ===========================

print("\n" + "=" * 60)
print("Part 3: Duplicate Detection")
print("=" * 60)

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö duplicates (timestamp + sensor_id ‡∏ï‡πâ‡∏≠‡∏á unique)
duplicates = df_cleaned[df_cleaned.duplicated(subset=['sensor_id'], keep=False)]
dup_count = df_cleaned.duplicated(subset=['sensor_id']).sum()

print(f"\nüîç Duplicate Check:")
print(f"   Checking: (timestamp, sensor_id) combination")
print(f"   Duplicate records found: {dup_count:,}")

if dup_count > 0:
    print(f"\n   Sample duplicates:")
    print(duplicates[['sensor_id', 'temperature', 'humidity']].head(6))

    # ‡∏•‡∏ö duplicates
    df_cleaned = df_cleaned.drop_duplicates(subset=['sensor_id'], keep='first')
    print(f"\n   ‚úÖ Removed {dup_count:,} duplicate records")
    print(f"   Remaining records: {len(df_cleaned):,}")
else:
    print("   ‚úÖ No duplicates found")
```

### Step 4: Consistency Checks

```python
# ===========================
# Part 4: Consistency Checks
# ===========================

print("\n" + "=" * 60)
print("Part 4: Data Consistency Checks")
print("=" * 60)

# Check 1: Timestamp Sequence
print("\nüîç Check 1: Timestamp Sequence")
time_gaps = df_cleaned.index.to_series().diff()
expected_gap = pd.Timedelta(minutes=5)
irregular_gaps = time_gaps[time_gaps != expected_gap].dropna()

if len(irregular_gaps) > 0:
    print(f"   ‚ö†Ô∏è  Found {len(irregular_gaps):,} irregular time gaps")
    print(f"   Expected: 5 minutes")
    print(f"   Sample irregular gaps:")
    print(irregular_gaps.head(5))
else:
    print("   ‚úÖ All timestamps are sequential (5-minute intervals)")

# Check 2: Sensor ID Validity
print("\nüîç Check 2: Sensor ID Format")
valid_sensor_pattern = r'^S\d{3}$'  # S001, S002, ...
invalid_sensors = df_cleaned[~df_cleaned['sensor_id'].str.match(valid_sensor_pattern)]

if len(invalid_sensors) > 0:
    print(f"   ‚ùå Found {len(invalid_sensors):,} invalid sensor IDs")
    print(f"   Valid pattern: S001, S002, ...")
    print(invalid_sensors['sensor_id'].unique())
else:
    print("   ‚úÖ All sensor IDs are valid")

# Check 3: Status Values
print("\nüîç Check 3: Status Values")
valid_statuses = ['OK', 'WARNING', 'ERROR']
invalid_status = df_cleaned[~df_cleaned['status'].isin(valid_statuses)]

if len(invalid_status) > 0:
    print(f"   ‚ùå Found {len(invalid_status):,} invalid status values")
    print(f"   Valid values: {valid_statuses}")
    print(f"   Found: {invalid_status['status'].unique()}")
else:
    print("   ‚úÖ All status values are valid")

# Check 4: Null Values (should be 0 after cleaning)
print("\nüîç Check 4: Null Values")
null_count = df_cleaned.isnull().sum()
if null_count.sum() > 0:
    print("   ‚ùå Found NULL values:")
    print(null_count[null_count > 0])
else:
    print("   ‚úÖ No NULL values")
```

### Step 5: Generate Data Quality Report

```python
# ===========================
# Part 5: Data Quality Report
# ===========================

print("\n" + "=" * 60)
print("Part 5: Data Quality Report")
print("=" * 60)

def generate_quality_report(df):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á comprehensive data quality report
    """
    report = {
        'summary': {
            'total_records': len(df),
            'total_columns': len(df.columns),
            'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2),
            'date_range': {
                'start': str(df.index.min()),
                'end': str(df.index.max()),
                'duration_days': (df.index.max() - df.index.min()).days
            }
        },
        'data_quality': {
            'completeness': {},
            'validity': {},
            'uniqueness': {},
            'consistency': {}
        },
        'statistics': {}
    }

    # Completeness (Missing Values)
    for col in df.columns:
        missing = df[col].isnull().sum()
        report['data_quality']['completeness'][col] = {
            'missing_count': int(missing),
            'missing_percent': round(missing / len(df) * 100, 2),
            'complete': missing == 0
        }

    # Validity (Ranges)
    numeric_cols = df.select_dtypes(include=['number']).columns
    for col in numeric_cols:
        report['data_quality']['validity'][col] = {
            'min': float(df[col].min()),
            'max': float(df[col].max()),
            'mean': float(df[col].mean()),
            'median': float(df[col].median()),
            'std': float(df[col].std())
        }

    # Uniqueness
    for col in df.columns:
        unique_count = df[col].nunique()
        report['data_quality']['uniqueness'][col] = {
            'unique_values': int(unique_count),
            'unique_percent': round(unique_count / len(df) * 100, 2)
        }

    # Statistics Summary
    report['statistics'] = df.describe().to_dict()

    return report

# ‡∏™‡∏£‡πâ‡∏≤‡∏á report
quality_report = generate_quality_report(df_cleaned)

# ‡πÅ‡∏™‡∏î‡∏á report
print("\nüìä Data Quality Report Summary:")
print(f"   Total Records:  {quality_report['summary']['total_records']:,}")
print(f"   Total Columns:  {quality_report['summary']['total_columns']}")
print(f"   Memory Usage:   {quality_report['summary']['memory_usage_mb']} MB")
print(f"   Date Range:     {quality_report['summary']['date_range']['start']} to")
print(f"                   {quality_report['summary']['date_range']['end']}")

print("\nüìã Completeness (Missing Values):")
for col, metrics in quality_report['data_quality']['completeness'].items():
    status = "‚úÖ" if metrics['complete'] else "‚ùå"
    print(f"   {status} {col:15s} Missing: {metrics['missing_percent']}%")

print("\nüîç Uniqueness:")
for col, metrics in quality_report['data_quality']['uniqueness'].items():
    print(f"   {col:15s} Unique: {metrics['unique_values']:,} ({metrics['unique_percent']}%)")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å report ‡πÄ‡∏õ‡πá‡∏ô JSON
with open('data/quality_report.json', 'w') as f:
    json.dump(quality_report, f, indent=2)

print(f"\nüíæ Saved quality report to: data/quality_report.json")

# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà validated ‡πÅ‡∏•‡πâ‡∏ß
df_cleaned.to_csv('data/sensor_validated.csv')
print(f"üíæ Saved validated data to: data/sensor_validated.csv")
```

### üìù Checkpoint 2.3

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**
1. ‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Validation ‡πÅ‡∏•‡∏∞ Cleaning?
2. ‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (timestamp, sensor_id) combination ‡∏ß‡πà‡∏≤ unique?
3. Range Check vs Outlier Detection - ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?

**‡∏ó‡∏î‡∏™‡∏≠‡∏ö:**
```python
# ‡∏™‡∏£‡πâ‡∏≤‡∏á validation pipeline ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ
class DataValidator:
    def __init__(self, df):
        self.df = df
        self.report = {'passed': [], 'failed': [], 'warnings': []}

    def validate_all(self):
        self.check_types()
        self.check_ranges()
        self.check_nulls()
        self.check_duplicates()
        return self.report

    # ‡πÄ‡∏û‡∏¥‡πà‡∏° methods ‡∏≠‡∏∑‡πà‡∏ô‡πÜ...

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
validator = DataValidator(df)
validation_results = validator.validate_all()
```

---

## Challenge: Complete ETL Pipeline

### üéØ ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢
‡∏™‡∏£‡πâ‡∏≤‡∏á Complete ETL Pipeline ‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å Lab 2.1-2.3

### Challenge Requirements

‡∏™‡∏£‡πâ‡∏≤‡∏á `etl_pipeline.py` ‡∏ó‡∏µ‡πà‡∏°‡∏µ:

1. **Extract:** ‡πÇ‡∏´‡∏•‡∏î CSV ‡πÅ‡∏ö‡∏ö Optimized
2. **Transform:** Clean & Validate
3. **Load:** ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
4. **Report:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Quality Report

```python
"""
Complete ETL Pipeline for IoT Sensor Data
"""
import pandas as pd
import numpy as np
import json
from datetime import datetime

class SensorDataETL:
    """
    ETL Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IoT Sensor Data
    """

    def __init__(self, input_file):
        self.input_file = input_file
        self.df_raw = None
        self.df_cleaned = None
        self.metrics = {
            'extract': {},
            'transform': {},
            'load': {},
            'quality': {}
        }

    def extract(self):
        """
        Extract: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö Optimized
        """
        print("=" * 60)
        print("STEP 1: EXTRACT")
        print("=" * 60)

        # TODO: ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ dtype optimization
        # TODO: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì memory usage
        # TODO: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics

        pass

    def transform(self):
        """
        Transform: ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞ Validate
        """
        print("\n" + "=" * 60)
        print("STEP 2: TRANSFORM")
        print("=" * 60)

        # TODO: ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values
        # TODO: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ranges ‡πÅ‡∏•‡∏∞‡∏•‡∏ö Outliers
        # TODO: ‡∏•‡∏ö Duplicates
        # TODO: Validate Data Quality

        pass

    def load(self, output_dir='data/output'):
        """
        Load: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏•‡∏∞ Report
        """
        print("\n" + "=" * 60)
        print("STEP 3: LOAD")
        print("=" * 60)

        # TODO: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cleaned data
        # TODO: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å quality report
        # TODO: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics

        pass

    def run(self):
        """
        ‡∏£‡∏±‡∏ô ETL Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        print("\n" + "=" * 70)
        print("üöÄ IoT Sensor Data ETL Pipeline")
        print("=" * 70)
        print(f"Input:  {self.input_file}")
        print(f"Start:  {datetime.now()}")

        # Execute ETL
        self.extract()
        self.transform()
        self.load()

        print("\n" + "=" * 70)
        print("‚úÖ ETL Pipeline Completed")
        print("=" * 70)

        return self.df_cleaned, self.metrics

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    etl = SensorDataETL('data/sensor_large.csv')
    df_final, metrics = etl.run()

    # ‡πÅ‡∏™‡∏î‡∏á Summary
    print("\nüìä Pipeline Summary:")
    print(json.dumps(metrics, indent=2))
```

### Challenge Tasks

1. **‡πÄ‡∏ï‡∏¥‡∏° code ‡πÉ‡∏ô `extract()` method:**
   - ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡πâ‡∏ß‡∏¢ dtype optimization
   - ‡∏ß‡∏±‡∏î memory usage ‡πÅ‡∏•‡∏∞ load time
   - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metrics

2. **‡πÄ‡∏ï‡∏¥‡∏° code ‡πÉ‡∏ô `transform()` method:**
   - ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°)
   - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Outliers
   - ‡∏•‡∏ö Duplicates
   - Validate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î

3. **‡πÄ‡∏ï‡∏¥‡∏° code ‡πÉ‡∏ô `load()` method:**
   - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å cleaned data ‡πÄ‡∏õ‡πá‡∏ô CSV
   - ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å quality report (JSON)
   - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å pipeline metrics

4. **‡πÄ‡∏û‡∏¥‡πà‡∏° features:**
   - Logging system
   - Error handling
   - Progress indicators
   - Performance profiling

### Expected Output

```
======================================================================
üöÄ IoT Sensor Data ETL Pipeline
======================================================================
Input:  data/sensor_large.csv
Start:  2025-01-15 10:30:00

============================================================
STEP 1: EXTRACT
============================================================
‚úÖ Loaded 100,500 records
‚úÖ Memory optimization: 76.7% saving
‚úÖ Load time: 0.198 seconds

============================================================
STEP 2: TRANSFORM
============================================================
üßπ Handling missing values...
   ‚úÖ Temperature: Interpolated 5,025 values
   ‚úÖ Humidity: Interpolated 5,025 values
   ‚úÖ Pressure: Interpolated 5,025 values

üîç Validating ranges...
   ‚ùå Temperature: 1,005 outliers found
   ‚úÖ Removed outliers

üîÑ Checking duplicates...
   ‚úÖ Removed 503 duplicates

‚úÖ Final records: 99,000

============================================================
STEP 3: LOAD
============================================================
üíæ Saved: data/output/sensor_cleaned_20250115.csv
üíæ Saved: data/output/quality_report_20250115.json
üíæ Saved: data/output/pipeline_metrics_20250115.json

======================================================================
‚úÖ ETL Pipeline Completed
======================================================================
```

---

## Checkpoint & Submission

### ‚úÖ Checklist

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏≥‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å Lab:

- [ ] **Lab 2.1:** ‡πÇ‡∏´‡∏•‡∏î CSV ‡πÅ‡∏ö‡∏ö Optimized ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Memory Saving
  - [ ] Memory Saving > 70%
  - [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dtype ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
  - [ ] ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å category dtype

- [ ] **Lab 2.2:** ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values
  - [ ] ‡∏ó‡∏î‡∏•‡∏≠‡∏á 5 strategies (Drop, Ffill, Mean, Median, Interpolate)
  - [ ] ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
  - [ ] ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö Time-Series

- [ ] **Lab 2.3:** Data Validation
  - [ ] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Types
  - [ ] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Value Ranges
  - [ ] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Duplicates
  - [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á Quality Report

- [ ] **Challenge:** Complete ETL Pipeline
  - [ ] ‡∏£‡∏ß‡∏° Extract, Transform, Load
  - [ ] ‡∏°‡∏µ Metrics ‡πÅ‡∏•‡∏∞ Reporting
  - [ ] ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏ã‡πâ‡∏≥‡πÑ‡∏î‡πâ

### üì§ Submission

‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:

```
module-2/
‚îú‚îÄ‚îÄ lab_2_1_loading.py
‚îú‚îÄ‚îÄ lab_2_2_missing_values.py
‚îú‚îÄ‚îÄ lab_2_3_validation.py
‚îú‚îÄ‚îÄ etl_pipeline.py
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ sensor_large.csv
    ‚îú‚îÄ‚îÄ sensor_cleaned.csv
    ‚îú‚îÄ‚îÄ sensor_validated.csv
    ‚îî‚îÄ‚îÄ quality_report.json
```

### üéì Learning Outcomes

‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏ó‡∏≥ Lab ‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞:

‚úÖ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡πÅ‡∏ö‡∏ö Optimized ‡πÑ‡∏î‡πâ
‚úÖ ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Memory ‡∏î‡πâ‡∏ß‡∏¢ dtype optimization
‚úÖ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Quality ‡πÅ‡∏•‡∏∞ Integrity
‚úÖ ‡∏™‡∏£‡πâ‡∏≤‡∏á ETL Pipeline ‡πÅ‡∏ö‡∏ö‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

---

## üí° Additional Resources

### ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

- [Pandas dtype optimization](https://pandas.pydata.org/docs/user_guide/scale.html)
- [Handling Missing Data](https://pandas.pydata.org/docs/user_guide/missing_data.html)
- [Data Validation Best Practices](https://docs.python.org/3/library/dataclasses.html)

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ï‡πà‡∏≠‡πÑ‡∏õ

‡πÉ‡∏ô Module 3 ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
- Feature Engineering (Rolling Average)
- Performance Benchmarking (Pandas vs NumPy)
- Chunking ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å

---

**[‚¨ÖÔ∏è ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ Module 2](../module-2.md)** | **[‡πÑ‡∏õ‡∏ó‡∏µ‡πà Module 3 Labs ‚û°Ô∏è](../../module-3/labs/lab-module-3.md)**

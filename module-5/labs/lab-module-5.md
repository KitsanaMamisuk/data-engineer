# Lab Module 5: Airflow - Orchestration Basics

**üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:**
- ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Apache Airflow ‡∏î‡πâ‡∏ß‡∏¢ Docker
- ‡∏™‡∏£‡πâ‡∏≤‡∏á DAG ‡πÅ‡∏£‡∏Å‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
- ‡∏ù‡∏∂‡∏Å‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Operators ‡∏ï‡πà‡∏≤‡∏á‡πÜ
- ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏Å‡∏≥‡∏´‡∏ô‡∏î Task Dependencies
- ‡∏ô‡∏≥‡∏ó‡∏≤‡∏á Airflow UI ‡πÅ‡∏•‡∏∞ Monitoring

**‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

---

## üìã Pre-requisites

‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° Lab ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ:
- ‚úÖ Docker Desktop ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
- ‚úÖ Docker Compose (‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏° Docker Desktop)
- ‚úÖ Terminal/Command Prompt
- ‚úÖ Text Editor (VS Code ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥)
- ‚úÖ ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà Hard Disk ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 GB

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Docker:**
```bash
docker --version
docker compose version
```

---

## Lab 5.1: Install Airflow via Docker (30 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Apache Airflow ‡∏î‡πâ‡∏ß‡∏¢ Docker Compose

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Project Directory

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Airflow
mkdir ~/airflow-lab
cd ~/airflow-lab

# ‡∏™‡∏£‡πâ‡∏≤‡∏á subdirectories
mkdir -p ./dags ./logs ./plugins ./config
```

**‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á:**
```
airflow-lab/
‚îú‚îÄ‚îÄ dags/       ‚Üê ‡πÄ‡∏Å‡πá‡∏ö DAG files
‚îú‚îÄ‚îÄ logs/       ‚Üê Logs
‚îú‚îÄ‚îÄ plugins/    ‚Üê Custom plugins
‚îî‚îÄ‚îÄ config/     ‚Üê Configuration
```

#### Step 2: ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î docker-compose.yaml

```bash
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î official docker-compose file
curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.8.0/docker-compose.yaml'

# ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ wget
# wget 'https://airflow.apache.org/docs/apache-airflow/2.8.0/docker-compose.yaml'
```

#### Step 3: ‡∏™‡∏£‡πâ‡∏≤‡∏á .env File

```bash
# ‡∏™‡∏£‡πâ‡∏≤‡∏á .env file ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö User ID
echo -e "AIRFLOW_UID=$(id -u)" > .env

# ‡∏ö‡∏ô Windows ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ:
# echo AIRFLOW_UID=50000 > .env
```

#### Step 4: Initialize Airflow Database

```bash
# Initialize database ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Admin user
docker compose up airflow-init

# ‡∏£‡∏≠‡∏à‡∏ô‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°:
# airflow-init_1 exited with code 0
```

**Expected Output:**
```
[+] Running 1/0
 ‚†ø Container airflow-lab-airflow-init-1  Created
[+] Running 1/1
 ‚†ø Container airflow-lab-airflow-init-1  Started
...
Admin user airflow created
airflow-lab-airflow-init-1 exited with code 0
```

#### Step 5: Start Airflow Services

```bash
# Start ‡∏ó‡∏∏‡∏Å services (detached mode)
docker compose up -d

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö services ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô
docker compose ps
```

**Expected Output:**
```
NAME                                   STATUS    PORTS
airflow-lab-airflow-scheduler-1        Up        8080/tcp
airflow-lab-airflow-webserver-1        Up        0.0.0.0:8080->8080/tcp
airflow-lab-airflow-worker-1           Up        8080/tcp
airflow-lab-postgres-1                 Up        5432/tcp
airflow-lab-redis-1                    Up        6379/tcp
```

#### Step 6: ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Airflow UI

```bash
# ‡πÄ‡∏õ‡∏¥‡∏î Browser ‡πÑ‡∏õ‡∏ó‡∏µ‡πà:
http://localhost:8080

# Login Credentials:
Username: airflow
Password: airflow
```

**‡∏´‡∏ô‡πâ‡∏≤ Login:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Apache Airflow           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                             ‚îÇ
‚îÇ  Username: [airflow]        ‚îÇ
‚îÇ  Password: [********]       ‚îÇ
‚îÇ                             ‚îÇ
‚îÇ  [        Login       ]     ‚îÇ
‚îÇ                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Step 7: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Example DAGs

‡∏´‡∏•‡∏±‡∏á Login ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô:
- üìä DAGs List
- üü¢ Example DAGs (‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß)
- üìà Statistics

### ‚úÖ Checkpoint 5.1

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] Docker services ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤ Airflow UI ‡πÑ‡∏î‡πâ‡∏ó‡∏µ‡πà http://localhost:8080
- [ ] Login ‡∏î‡πâ‡∏ß‡∏¢ airflow/airflow ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] ‡πÄ‡∏´‡πá‡∏ô Example DAGs ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡πÅ‡∏£‡∏Å

### üîß Troubleshooting

**‡∏´‡∏≤‡∏Å Port 8080 ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡πâ‡∏ß:**
```bash
# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç docker-compose.yaml
# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô "8080:8080" ‡πÄ‡∏õ‡πá‡∏ô "8081:8080"
# ‡πÅ‡∏•‡πâ‡∏ß restart
docker compose down
docker compose up -d
```

**‡∏´‡∏≤‡∏Å Services ‡πÑ‡∏°‡πà‡∏Ç‡∏∂‡πâ‡∏ô:**
```bash
# ‡∏î‡∏π logs
docker compose logs airflow-webserver

# Restart services
docker compose down
docker compose up -d
```

---

## Lab 5.2: Create "Hello World" DAG (20 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏™‡∏£‡πâ‡∏≤‡∏á DAG ‡πÅ‡∏£‡∏Å‡∏î‡πâ‡∏ß‡∏¢ PythonOperator

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå DAG

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `hello_world_dag.py` ‡πÉ‡∏ô `dags/` folder:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Default arguments
default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# Python function
def print_hello():
    print("Hello World from Airflow!")
    print("This is my first DAG!")
    return "Success"

# Create DAG
dag = DAG(
    dag_id='hello_world_dag',
    default_args=default_args,
    description='My first Airflow DAG',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['tutorial', 'hello-world'],
)

# Create Task
task_hello = PythonOperator(
    task_id='say_hello',
    python_callable=print_hello,
    dag=dag,
)
```

**‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà:**
```
~/airflow-lab/dags/hello_world_dag.py
```

#### Step 2: ‡∏£‡∏≠ Airflow ‡πÇ‡∏´‡∏•‡∏î DAG

```bash
# Airflow ‡∏à‡∏∞‡∏™‡πÅ‡∏Å‡∏ô dags/ folder ‡∏ó‡∏∏‡∏Å 30 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
# ‡∏£‡∏≠‡∏™‡∏±‡∏Å‡∏Ñ‡∏£‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß Refresh Browser

# ‡∏´‡∏£‡∏∑‡∏≠ restart scheduler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô:
docker compose restart airflow-scheduler
```

#### Step 3: ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô DAG

‡πÉ‡∏ô Airflow UI:
1. ‡∏´‡∏≤ DAG ‡∏ä‡∏∑‡πà‡∏≠ `hello_world_dag`
2. ‡∏Ñ‡∏•‡∏¥‡∏Å Toggle (‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å ‚ö´ ‡πÄ‡∏õ‡πá‡∏ô üü¢)

#### Step 4: Trigger DAG ‡∏î‡πâ‡∏ß‡∏¢‡∏°‡∏∑‡∏≠

```
1. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏õ‡∏∏‡πà‡∏° "‚ñ∂Ô∏è Trigger DAG" (‡∏Ç‡∏ß‡∏≤‡∏°‡∏∑‡∏≠)
2. ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô "Trigger"
3. ‡∏£‡∏≠ DAG ‡∏£‡∏±‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à (refresh ‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö)
```

#### Step 5: ‡∏î‡∏π Logs

```
1. ‡∏Ñ‡∏•‡∏¥‡∏Å‡∏ä‡∏∑‡πà‡∏≠ DAG "hello_world_dag"
2. ‡∏Ñ‡∏•‡∏¥‡∏Å Graph View
3. ‡∏Ñ‡∏•‡∏¥‡∏Å Task "say_hello"
4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å "Log"
```

**Expected Log Output:**
```
[2025-01-01 12:00:00] INFO - Starting task: say_hello
[2025-01-01 12:00:01] INFO - Executing: print_hello()
[2025-01-01 12:00:01] INFO - Hello World from Airflow!
[2025-01-01 12:00:01] INFO - This is my first DAG!
[2025-01-01 12:00:01] INFO - Task returned: Success
[2025-01-01 12:00:01] INFO - Task exited with return code 0
```

### ‚úÖ Checkpoint 5.2

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `hello_world_dag.py` ‡πÉ‡∏ô `dags/` folder
- [ ] ‡πÄ‡∏´‡πá‡∏ô DAG ‡πÉ‡∏ô Airflow UI
- [ ] Trigger DAG ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à (üü¢)
- [ ] ‡∏≠‡πà‡∏≤‡∏ô Logs ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "Hello World from Airflow!"

---

## Lab 5.3: Multiple Tasks with Dependencies (25 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏™‡∏£‡πâ‡∏≤‡∏á DAG ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ Tasks ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î Dependencies

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á ETL DAG

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `simple_etl_dag.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta

# Python functions
def extract_data():
    print("=" * 50)
    print("EXTRACT: Reading data from source...")
    data = [
        {'sensor_id': 'S001', 'temperature': 25.5},
        {'sensor_id': 'S002', 'temperature': 26.0},
        {'sensor_id': 'S003', 'temperature': 24.8},
    ]
    print(f"Extracted {len(data)} records")
    print("=" * 50)
    return data

def transform_data(**context):
    print("=" * 50)
    print("TRANSFORM: Converting Celsius to Fahrenheit...")

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å previous task (XCom)
    ti = context['ti']
    data = ti.xcom_pull(task_ids='extract')

    transformed = []
    for record in data:
        celsius = record['temperature']
        fahrenheit = (celsius * 9/5) + 32
        transformed.append({
            'sensor_id': record['sensor_id'],
            'celsius': celsius,
            'fahrenheit': fahrenheit
        })

    print(f"Transformed {len(transformed)} records")
    for rec in transformed:
        print(f"  {rec['sensor_id']}: {rec['celsius']}¬∞C = {rec['fahrenheit']}¬∞F")
    print("=" * 50)
    return transformed

def load_data(**context):
    print("=" * 50)
    print("LOAD: Saving data to destination...")

    ti = context['ti']
    data = ti.xcom_pull(task_ids='transform')

    print(f"Loading {len(data)} records to database...")
    print("‚úÖ Data loaded successfully!")
    print("=" * 50)
    return "Load completed"

# Default args
default_args = {
    'owner': 'data-engineer',
    'retries': 2,
    'retry_delay': timedelta(minutes=3),
}

# Create DAG
dag = DAG(
    dag_id='simple_etl_dag',
    default_args=default_args,
    description='Simple ETL pipeline with 3 tasks',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['etl', 'tutorial'],
)

# Create Tasks
task_start = BashOperator(
    task_id='start',
    bash_command='echo "Starting ETL Pipeline..."',
    dag=dag,
)

task_extract = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag,
)

task_transform = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    provide_context=True,
    dag=dag,
)

task_load = PythonOperator(
    task_id='load',
    python_callable=load_data,
    provide_context=True,
    dag=dag,
)

task_end = BashOperator(
    task_id='end',
    bash_command='echo "ETL Pipeline completed successfully!"',
    dag=dag,
)

# Define Dependencies
task_start >> task_extract >> task_transform >> task_load >> task_end
```

**‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà:**
```
~/airflow-lab/dags/simple_etl_dag.py
```

#### Step 2: Trigger DAG

1. Refresh Airflow UI
2. ‡∏´‡∏≤ DAG `simple_etl_dag`
3. ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Toggle on)
4. Trigger DAG

#### Step 3: ‡∏î‡∏π Graph View

```
1. ‡∏Ñ‡∏•‡∏¥‡∏Å DAG "simple_etl_dag"
2. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Graph View
3. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï Task Dependencies:

   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ start ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ extract   ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ transform    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  load    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ  end  ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Step 4: ‡∏î‡∏π Logs ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Task

‡∏î‡∏π Logs ‡∏Ç‡∏≠‡∏á `transform` task:
```
[2025-01-01 12:00:05] INFO - TRANSFORM: Converting Celsius to Fahrenheit...
[2025-01-01 12:00:05] INFO -   S001: 25.5¬∞C = 77.9¬∞F
[2025-01-01 12:00:05] INFO -   S002: 26.0¬∞C = 78.8¬∞F
[2025-01-01 12:00:05] INFO -   S003: 24.8¬∞C = 76.64¬∞F
```

### ‚úÖ Checkpoint 5.3

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á `simple_etl_dag.py` ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] DAG ‡∏°‡∏µ 5 Tasks (start, extract, transform, load, end)
- [ ] Tasks ‡∏£‡∏±‡∏ô‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
- [ ] ‡∏î‡∏π Logs ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å Celsius ‚Üí Fahrenheit

---

## Lab 5.4: Parallel Tasks (20 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á Tasks ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô (Parallel)

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Parallel DAG

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `parallel_tasks_dag.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import time

# Python functions
def process_sensor_a():
    print("Processing Sensor A data...")
    time.sleep(5)  # Simulate processing time
    print("‚úÖ Sensor A completed")
    return "Sensor A: 100 records"

def process_sensor_b():
    print("Processing Sensor B data...")
    time.sleep(3)
    print("‚úÖ Sensor B completed")
    return "Sensor B: 150 records"

def process_sensor_c():
    print("Processing Sensor C data...")
    time.sleep(4)
    print("‚úÖ Sensor C completed")
    return "Sensor C: 120 records"

def aggregate_results(**context):
    ti = context['ti']

    # ‡∏î‡∏∂‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å 3 tasks
    result_a = ti.xcom_pull(task_ids='process_a')
    result_b = ti.xcom_pull(task_ids='process_b')
    result_c = ti.xcom_pull(task_ids='process_c')

    print("=" * 50)
    print("AGGREGATING RESULTS:")
    print(f"  - {result_a}")
    print(f"  - {result_b}")
    print(f"  - {result_c}")
    print("  - Total: 370 records")
    print("=" * 50)
    return "Aggregation completed"

# Default args
default_args = {
    'owner': 'data-engineer',
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

# Create DAG
dag = DAG(
    dag_id='parallel_tasks_dag',
    default_args=default_args,
    description='DAG with parallel task execution',
    schedule_interval='@hourly',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['parallel', 'tutorial'],
)

# Create Tasks
task_start = PythonOperator(
    task_id='start',
    python_callable=lambda: print("Starting parallel processing..."),
    dag=dag,
)

task_a = PythonOperator(
    task_id='process_a',
    python_callable=process_sensor_a,
    dag=dag,
)

task_b = PythonOperator(
    task_id='process_b',
    python_callable=process_sensor_b,
    dag=dag,
)

task_c = PythonOperator(
    task_id='process_c',
    python_callable=process_sensor_c,
    dag=dag,
)

task_aggregate = PythonOperator(
    task_id='aggregate',
    python_callable=aggregate_results,
    provide_context=True,
    dag=dag,
)

# Define Dependencies
# start ‚Üí [a, b, c] ‚Üí aggregate (a, b, c ‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô)
task_start >> [task_a, task_b, task_c] >> task_aggregate
```

#### Step 2: Trigger ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï

1. Trigger `parallel_tasks_dag`
2. ‡πÄ‡∏õ‡∏¥‡∏î Graph View
3. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ Tasks A, B, C ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô Running (üîµ) ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô

**Graph View:**
```
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îå‚îÄ‚îÄ> ‚îÇprocess_a  ‚îÇ ‚îÄ‚îÄ‚îê
         ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ start  ‚îÇ                ‚îú‚îÄ‚îÄ> ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ    ‚îÇ aggregate   ‚îÇ
         ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îú‚îÄ‚îÄ> ‚îÇprocess_b  ‚îÇ ‚îÄ‚îÄ‚î§
         ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
         ‚îÇ                    ‚îÇ
         ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
         ‚îî‚îÄ‚îÄ> ‚îÇprocess_c  ‚îÇ ‚îÄ‚îÄ‚îò
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### Step 3: ‡∏î‡∏π Gantt Chart

1. ‡πÄ‡∏õ‡∏¥‡∏î Gantt Chart View
2. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï‡∏ß‡πà‡∏≤ Tasks A, B, C ‡∏£‡∏±‡∏ô‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô (‡πÅ‡∏™‡∏î‡∏á Parallel Execution)

```
Gantt Chart:

start        |‚ñà‚ñà|
process_a       |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|
process_b       |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|
process_c       |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà|
aggregate                    |‚ñà‚ñà|
           12:00      12:05
```

### ‚úÖ Checkpoint 5.4

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á `parallel_tasks_dag.py` ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] Tasks A, B, C ‡∏£‡∏±‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
- [ ] Task aggregate ‡∏£‡∏≠‡πÉ‡∏´‡πâ‡∏ó‡∏±‡πâ‡∏á 3 tasks ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏Å‡πà‡∏≠‡∏ô
- [ ] ‡πÄ‡∏´‡πá‡∏ô Parallel execution ‡πÉ‡∏ô Gantt Chart

---

## Lab 5.5: Scheduling & Monitoring (15 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏ù‡∏∂‡∏Å‡∏Å‡∏≥‡∏´‡∏ô‡∏î Schedule ‡πÅ‡∏•‡∏∞ Monitor DAGs

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Scheduled DAG

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `scheduled_dag.py`:

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def daily_report():
    now = datetime.now()
    print("=" * 60)
    print(f"üìä DAILY REPORT - {now.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print("Execution Summary:")
    print("  - Total Sensors: 10")
    print("  - Records Processed: 14,400")
    print("  - Success Rate: 99.5%")
    print("  - Average Temperature: 25.3¬∞C")
    print("=" * 60)
    return "Report generated"

# Default args
default_args = {
    'owner': 'data-engineer',
    'retries': 1,
}

# DAG with different schedules
dag_daily = DAG(
    dag_id='daily_report_dag',
    default_args=default_args,
    description='Runs daily at 2 AM',
    schedule_interval='0 2 * * *',  # Cron: ‡∏ó‡∏∏‡∏Å‡∏ß‡∏±‡∏ô 02:00
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['scheduled', 'report'],
)

task_daily = PythonOperator(
    task_id='generate_daily_report',
    python_callable=daily_report,
    dag=dag_daily,
)

# DAG ‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏î‡∏™‡∏≠‡∏ö)
dag_frequent = DAG(
    dag_id='frequent_check_dag',
    default_args=default_args,
    description='Runs every 5 minutes',
    schedule_interval='*/5 * * * *',  # Cron: ‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['scheduled', 'monitoring'],
)

task_frequent = PythonOperator(
    task_id='health_check',
    python_callable=lambda: print("‚úÖ System health check passed"),
    dag=dag_frequent,
)
```

#### Step 2: ‡∏ó‡∏î‡∏™‡∏≠‡∏ö Schedule Intervals

**‡∏î‡∏π Schedule:**
1. ‡πÑ‡∏õ‡∏ó‡∏µ‡πà DAGs List
2. ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "Schedule"
3. ‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:
   - `daily_report_dag`: `0 2 * * *`
   - `frequent_check_dag`: `*/5 * * * *`

#### Step 3: ‡∏î‡∏π Tree View

1. ‡∏Ñ‡∏•‡∏¥‡∏Å DAG `frequent_check_dag`
2. ‡πÄ‡∏õ‡∏¥‡∏î Tree View
3. ‡∏£‡∏≠ 5-10 ‡∏ô‡∏≤‡∏ó‡∏µ
4. Refresh ‡πÅ‡∏•‡∏∞‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï DAG ‡∏£‡∏±‡∏ô‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

**Tree View:**
```
frequent_check_dag
‚îú‚îÄ 2025-01-01 12:00  ‚úÖ
‚îú‚îÄ 2025-01-01 12:05  ‚úÖ
‚îú‚îÄ 2025-01-01 12:10  ‚úÖ
‚îî‚îÄ 2025-01-01 12:15  ‚è∏Ô∏è Running
```

#### Step 4: ‡∏ù‡∏∂‡∏Å‡πÉ‡∏ä‡πâ UI Features

**Pause/Unpause DAG:**
- ‡∏Ñ‡∏•‡∏¥‡∏Å Toggle ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡∏¥‡∏î Scheduling

**Browse ‚Üí Task Instances:**
- ‡∏î‡∏π Task Instances ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö
- Filter by State, DAG, Date

**Browse ‚Üí DAG Runs:**
- ‡∏î‡∏π DAG Run history
- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÅ‡∏ï‡πà‡∏•‡∏∞ run

**Admin ‚Üí Connections:**
- ‡∏î‡∏π Connections ‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏ß‡πâ
- ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Database, Cloud, APIs

### ‚úÖ Checkpoint 5.5

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á DAGs ‡∏ó‡∏µ‡πà‡∏°‡∏µ schedule_interval ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Cron Expressions
- [ ] ‡∏î‡∏π Tree View ‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡πá‡∏ô execution history
- [ ] ‡πÉ‡∏ä‡πâ Pause/Unpause DAG ‡πÑ‡∏î‡πâ

---

## üèÜ Challenge Exercise (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°)

### Challenge 1: IoT Data Processor DAG

‡∏™‡∏£‡πâ‡∏≤‡∏á DAG ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• IoT ‡∏ó‡∏µ‡πà‡∏°‡∏µ:

**Requirements:**
1. Task: `check_file` (FileSensor ‡∏à‡∏≥‡∏•‡∏≠‡∏á) - ‡πÄ‡∏ä‡πá‡∏Ñ‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏´‡∏°
2. Task: `validate_data` - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
3. Tasks: `process_temperature`, `process_humidity` (Parallel)
4. Task: `send_notification` - ‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏™‡∏£‡πá‡∏à
5. Schedule: ‡∏ó‡∏∏‡∏Å 30 ‡∏ô‡∏≤‡∏ó‡∏µ

**DAG Structure:**
```
check_file ‚Üí validate_data ‚Üí [process_temp, process_humidity] ‚Üí send_notification
```

<details>
<summary>üí° Solution (‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π)</summary>

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def check_file():
    print("‚úÖ Checking for new IoT data files...")
    print("File found: sensor_data_2025_01_01.csv")
    return True

def validate_data():
    print("üîç Validating data integrity...")
    print("‚úÖ Validation passed: 1440 records")
    return True

def process_temperature():
    print("üå°Ô∏è Processing temperature data...")
    print("Average: 25.5¬∞C, Min: 20¬∞C, Max: 30¬∞C")
    return "Temperature processing completed"

def process_humidity():
    print("üíß Processing humidity data...")
    print("Average: 62%, Min: 45%, Max: 80%")
    return "Humidity processing completed"

def send_notification(**context):
    ti = context['ti']
    temp_result = ti.xcom_pull(task_ids='process_temp')
    humidity_result = ti.xcom_pull(task_ids='process_humidity')

    print("üìß Sending notification...")
    print(f"  - {temp_result}")
    print(f"  - {humidity_result}")
    print("‚úÖ Notification sent successfully!")

default_args = {
    'owner': 'data-engineer',
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    dag_id='iot_data_processor_dag',
    default_args=default_args,
    description='IoT data processing pipeline',
    schedule_interval='*/30 * * * *',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['iot', 'challenge'],
)

# Create tasks
task_check = PythonOperator(
    task_id='check_file',
    python_callable=check_file,
    dag=dag,
)

task_validate = PythonOperator(
    task_id='validate_data',
    python_callable=validate_data,
    dag=dag,
)

task_process_temp = PythonOperator(
    task_id='process_temp',
    python_callable=process_temperature,
    dag=dag,
)

task_process_humidity = PythonOperator(
    task_id='process_humidity',
    python_callable=process_humidity,
    dag=dag,
)

task_notify = PythonOperator(
    task_id='send_notification',
    python_callable=send_notification,
    provide_context=True,
    dag=dag,
)

# Define dependencies
task_check >> task_validate >> [task_process_temp, task_process_humidity] >> task_notify
```
</details>

### Challenge 2: Dynamic DAG with Variable Number of Sensors

‡∏™‡∏£‡πâ‡∏≤‡∏á DAG ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á Tasks ‡πÅ‡∏ö‡∏ö Dynamic ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢ Sensors

**Requirements:**
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Sensor IDs ‡πÅ‡∏ö‡∏ö Dynamic (S001, S002, S003, ...)
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Task ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Sensor ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- Aggregate ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å Sensors

<details>
<summary>üí° Solution (‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π)</summary>

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Sensor IDs (‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ)
SENSOR_IDS = ['S001', 'S002', 'S003', 'S004', 'S005']

def process_sensor(sensor_id):
    def _process():
        print(f"üìä Processing data for {sensor_id}...")
        # Simulate processing
        result = f"{sensor_id}: 288 records processed"
        print(f"‚úÖ {result}")
        return result
    return _process

def aggregate_all(**context):
    ti = context['ti']
    print("=" * 60)
    print("üìà AGGREGATING ALL SENSOR DATA")
    print("=" * 60)

    total = 0
    for sensor_id in SENSOR_IDS:
        result = ti.xcom_pull(task_ids=f'process_{sensor_id}')
        print(f"  - {result}")
        total += 288

    print(f"\n  Total Records: {total}")
    print("=" * 60)
    return f"Aggregated {total} records from {len(SENSOR_IDS)} sensors"

default_args = {
    'owner': 'data-engineer',
    'retries': 1,
    'retry_delay': timedelta(minutes=3),
}

dag = DAG(
    dag_id='dynamic_sensor_dag',
    default_args=default_args,
    description='Dynamic DAG for multiple sensors',
    schedule_interval='@hourly',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['dynamic', 'challenge'],
)

# Start task
task_start = PythonOperator(
    task_id='start',
    python_callable=lambda: print("Starting sensor processing..."),
    dag=dag,
)

# Dynamically create tasks for each sensor
sensor_tasks = []
for sensor_id in SENSOR_IDS:
    task = PythonOperator(
        task_id=f'process_{sensor_id}',
        python_callable=process_sensor(sensor_id),
        dag=dag,
    )
    sensor_tasks.append(task)

# Aggregate task
task_aggregate = PythonOperator(
    task_id='aggregate',
    python_callable=aggregate_all,
    provide_context=True,
    dag=dag,
)

# Define dependencies
# start ‚Üí [all sensor tasks] ‚Üí aggregate
task_start >> sensor_tasks >> task_aggregate
```
</details>

---

## üìä Lab Summary

### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

‚úÖ **Lab 5.1:** ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Airflow ‡∏î‡πâ‡∏ß‡∏¢ Docker Compose
‚úÖ **Lab 5.2:** ‡∏™‡∏£‡πâ‡∏≤‡∏á "Hello World" DAG ‡πÅ‡∏£‡∏Å
‚úÖ **Lab 5.3:** ‡∏™‡∏£‡πâ‡∏≤‡∏á ETL Pipeline ‡∏î‡πâ‡∏ß‡∏¢ Multiple Tasks
‚úÖ **Lab 5.4:** ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô Parallel Tasks
‚úÖ **Lab 5.5:** ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Scheduling ‡πÅ‡∏•‡∏∞ Monitoring

### Skills Acquired

| Skill | Confidence Level |
|-------|------------------|
| Airflow Installation | ‚≠ê‚≠ê‚≠ê |
| DAG Creation | ‚≠ê‚≠ê‚≠ê |
| Using Operators | ‚≠ê‚≠ê‚≠ê |
| Task Dependencies | ‚≠ê‚≠ê‚≠ê |
| Scheduling | ‚≠ê‚≠ê |
| UI Navigation | ‚≠ê‚≠ê‚≠ê |

---

## üéØ ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Module 6 ‡πÅ‡∏•‡πâ‡∏ß!

**üëâ [Module 6: Airflow - Building the IoT Pipeline](../../module-6/module-6.md)**

‡πÉ‡∏ô Module 6 ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ:
- ‡πÉ‡∏ä‡πâ FileSensor ‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡πÑ‡∏ü‡∏•‡πå
- Integrate ETL code ‡∏à‡∏≤‡∏Å Module 2-3
- Upload ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏õ Cloud Storage
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Complete IoT Pipeline

---

## üõ†Ô∏è Docker Commands Cheat Sheet

```bash
# Start Airflow
docker compose up -d

# Stop Airflow
docker compose down

# View logs
docker compose logs -f airflow-scheduler
docker compose logs -f airflow-webserver

# Restart services
docker compose restart

# Remove all (including volumes)
docker compose down -v

# Check container status
docker compose ps

# Execute command in container
docker compose exec airflow-scheduler bash
```

---

## üìû Need Help?

‡∏´‡∏≤‡∏Å‡∏ï‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤:

1. **DAG ‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡πÉ‡∏ô UI:**
   - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô `dags/` folder
   - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Syntax Error (Browse ‚Üí Import Errors)
   - Restart scheduler: `docker compose restart airflow-scheduler`

2. **Task ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß:**
   - ‡∏î‡∏π Logs ‡∏Ç‡∏≠‡∏á Task
   - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Python function ‡∏°‡∏µ Error ‡πÑ‡∏´‡∏°
   - ‡∏•‡∏≠‡∏á Clear Task ‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà

3. **Port Conflict:**
   - ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç port ‡πÉ‡∏ô docker-compose.yaml
   - ‡∏´‡∏£‡∏∑‡∏≠‡∏´‡∏¢‡∏∏‡∏î service ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ port 8080

4. **Out of Memory:**
   - ‡∏õ‡∏¥‡∏î DAGs ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ
   - ‡πÄ‡∏û‡∏¥‡πà‡∏° Memory ‡πÉ‡∏´‡πâ Docker Desktop

---

**[‚¨ÖÔ∏è ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ Module 5](../module-5.md)** | **[üè† ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å](../../wiki.md)**

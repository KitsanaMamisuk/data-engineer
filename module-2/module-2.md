# Module 2: ETL - Data Loading & Cleansing

**üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:**
- ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ ETL (Extract, Transform, Load)
- ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• CSV ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÅ‡∏ö‡∏ö Optimized
- ‡πÉ‡∏ä‡πâ dtype optimization ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î Memory
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á‡πÜ (Ffill, Bfill, Mean, Median)
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Integrity ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

**‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** 3.5 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (‡∏ó‡∏§‡∏©‡∏é‡∏µ 2 ‡∏ä‡∏°. + Lab 1.5 ‡∏ä‡∏°.)

---

## üìö ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [ETL Pipeline Overview](#1-etl-pipeline-overview)
2. [Data Loading - Optimized CSV Reading](#2-data-loading---optimized-csv-reading)
3. [dtype Optimization & Memory Management](#3-dtype-optimization--memory-management)
4. [Handling Missing Values](#4-handling-missing-values)
5. [Data Integrity Checks](#5-data-integrity-checks)
6. [Labs & Practical Exercises](./labs/lab-module-2.md)

---

## 1. ETL Pipeline Overview

### 1.1 ETL ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**ETL** = Extract, Transform, Load - ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Data Engineering

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ               ETL Pipeline Process                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ EXTRACT  ‚îÇ  ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇTRANSFORM ‚îÇ  ‚îÄ‚îÄ‚îÄ‚ñ∂ ‚îÇ  LOAD    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚îÇ                   ‚îÇ                   ‚îÇ
    ‚ñº                   ‚ñº                   ‚ñº
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  ‚îÇData ‚îÇ           ‚îÇClean‚îÇ           ‚îÇStore‚îÇ
  ‚îÇSource‚îÇ          ‚îÇ+Valid‚îÇ          ‚îÇTarget‚îÇ
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Examples:
  CSV File         - Remove NULL      Database
  API              - Type Convert     Data Lake
  Database         - Validate Range   Data Warehouse
  Sensor           - Calculate        Cloud Storage
```

### 1.2 ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Stage ‡∏Ç‡∏≠‡∏á ETL

#### **EXTRACT (‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)**
- ‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Source (CSV, Database, API, Sensor)
- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
- **‡πÉ‡∏ô Module ‡∏ô‡∏µ‡πâ:** ‡πÇ‡∏´‡∏•‡∏î CSV ‡πÅ‡∏ö‡∏ö Optimized

#### **TRANSFORM (‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)**
- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Cleansing)
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values
- ‡πÅ‡∏õ‡∏•‡∏á Data Types
- ‡∏™‡∏£‡πâ‡∏≤‡∏á Features ‡πÉ‡∏´‡∏°‡πà
- **‡πÉ‡∏ô Module ‡∏ô‡∏µ‡πâ:** Cleansing + Missing Value Handling

#### **LOAD (‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö)**
- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Target System
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Integrity
- **‡πÉ‡∏ô Module ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ:** Transformation & Storage

### 1.3 ETL ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IoT Sensor Data

```
IoT ETL Workflow Example:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  sensor_raw.csv  ‚îÇ  ‚Üê Raw sensor data (50 MB)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ EXTRACT
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  df = pd.read_csv(...)       ‚îÇ
‚îÇ  - Specify dtype             ‚îÇ  ‚Üê Memory Optimization
‚îÇ  - Parse dates               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ TRANSFORM
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Data Cleansing              ‚îÇ
‚îÇ  - Handle Missing Values     ‚îÇ  ‚Üê Ffill, Mean, Drop
‚îÇ  - Remove Outliers           ‚îÇ
‚îÇ  - Validate Ranges           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ LOAD
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  df.to_parquet(...)          ‚îÇ
‚îÇ  df.to_sql(...)              ‚îÇ  ‚Üê Store to Target
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.4 ‡∏ó‡∏≥‡πÑ‡∏° ETL ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç?

‚úÖ **Data Quality:** ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
‚úÖ **Performance:** Optimized Loading ‡∏•‡∏î Memory ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤
‚úÖ **Reliability:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Integrity ‡∏Å‡πà‡∏≠‡∏ô‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ
‚úÖ **Scalability:** ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

---

## 2. Data Loading - Optimized CSV Reading

### 2.1 ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î CSV ‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥ vs Optimized

#### 2.1.1 ‡∏ß‡∏¥‡∏ò‡∏µ‡∏õ‡∏Å‡∏ï‡∏¥ (‡πÑ‡∏°‡πà Optimize)

```python
import pandas as pd

# ‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥ - Pandas ‡∏à‡∏∞‡πÄ‡∏î‡∏≤ dtype ‡πÄ‡∏≠‡∏á
df = pd.read_csv('sensor_data.csv')

print(df.info(memory_usage='deep'))
```

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:**
- üìà ‡πÉ‡∏ä‡πâ Memory ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
- ‚è±Ô∏è ‡∏ä‡πâ‡∏≤ (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏î‡∏≤ dtype)
- ‚ùå ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà

#### 2.1.2 ‡∏ß‡∏¥‡∏ò‡∏µ Optimized (‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype)

```python
import pandas as pd

# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏ï‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î
dtype_spec = {
    'sensor_id': 'category',      # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤ ‚Üí category
    'temperature': 'float32',     # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏™‡∏π‡∏á ‚Üí float32
    'humidity': 'int16',          # ‡∏Ñ‡πà‡∏≤ 0-100 ‚Üí int16
    'status': 'category'          # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Ñ‡πà‡∏≤ (OK/ERROR) ‚Üí category
}

df = pd.read_csv(
    'sensor_data.csv',
    dtype=dtype_spec,
    parse_dates=['timestamp']     # ‡πÅ‡∏õ‡∏•‡∏á timestamp ‡πÄ‡∏õ‡πá‡∏ô datetime
)

print(df.info(memory_usage='deep'))
```

**‡∏Ç‡πâ‡∏≠‡∏î‡∏µ:**
- ‚úÖ ‡∏•‡∏î Memory ‡πÑ‡∏î‡πâ 50-80%
- ‚úÖ ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
- ‚úÖ ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° Data Type ‡πÑ‡∏î‡πâ

### 2.2 ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Memory Usage

```python
import pandas as pd
import numpy as np

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
data = {
    'sensor_id': ['S001', 'S002', 'S001'] * 10000,
    'temperature': np.random.uniform(20, 30, 30000),
    'humidity': np.random.randint(40, 80, 30000)
}

# ‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
df_normal = pd.DataFrame(data)
memory_normal = df_normal.memory_usage(deep=True).sum() / 1024**2  # MB

# ‡πÅ‡∏ö‡∏ö Optimized
df_optimized = pd.DataFrame(data)
df_optimized['sensor_id'] = df_optimized['sensor_id'].astype('category')
df_optimized['temperature'] = df_optimized['temperature'].astype('float32')
df_optimized['humidity'] = df_optimized['humidity'].astype('int16')
memory_optimized = df_optimized.memory_usage(deep=True).sum() / 1024**2  # MB

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Saving
saving_percent = ((memory_normal - memory_optimized) / memory_normal) * 100

print(f"Memory (Normal):    {memory_normal:.2f} MB")
print(f"Memory (Optimized): {memory_optimized:.2f} MB")
print(f"Saving:             {saving_percent:.1f}%")
```

**Expected Output:**
```
Memory (Normal):    2.29 MB
Memory (Optimized): 0.52 MB
Saving:             77.3%
```

### 2.3 ‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å dtype ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°

| Data Type | Range | Memory | Use Case |
|-----------|-------|--------|----------|
| **int8** | -128 to 127 | 1 byte | ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞, Flag (0/1) |
| **int16** | -32,768 to 32,767 | 2 bytes | Humidity (0-100), Small counts |
| **int32** | -2B to 2B | 4 bytes | IDs, Large counts |
| **int64** | -9E18 to 9E18 | 8 bytes | Default (‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ) |
| **float32** | ~7 decimals | 4 bytes | Temperature, Sensor values |
| **float64** | ~15 decimals | 8 bytes | Default (‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ) |
| **category** | Varies | Optimized | Repeated strings (sensor_id, status) |

### 2.4 Best Practices ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î CSV

```python
import pandas as pd

# ‚úÖ Recommended Approach
df = pd.read_csv(
    'sensor_large.csv',

    # 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î dtype
    dtype={
        'sensor_id': 'category',
        'temperature': 'float32',
        'humidity': 'int16',
        'pressure': 'float32',
        'status': 'category'
    },

    # 2. Parse datetime columns
    parse_dates=['timestamp'],

    # 3. Set index (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    # index_col='timestamp',

    # 4. ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
    usecols=['timestamp', 'sensor_id', 'temperature', 'humidity'],

    # 5. ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏ï‡∏≠‡∏ô‡πÇ‡∏´‡∏•‡∏î (‡∏ñ‡πâ‡∏≤‡∏£‡∏π‡πâ‡πÅ‡∏ô‡πà‡∏ä‡∏±‡∏î)
    # na_values=['NA', 'null', ''],

    # 6. ‡πÉ‡∏ä‡πâ chunksize ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å
    # chunksize=10000
)

print(f"Loaded {len(df):,} rows")
print(f"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
```

---

## 3. dtype Optimization & Memory Management

### 3.1 ‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏î Memory Usage

```python
import pandas as pd

def memory_usage_mb(df):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Memory ‡∏ó‡∏µ‡πà DataFrame ‡πÉ‡∏ä‡πâ (MB)"""
    return df.memory_usage(deep=True).sum() / 1024**2

# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
df = pd.read_csv('sensor_data.csv')

# ‡πÅ‡∏™‡∏î‡∏á Memory ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Column
print(df.memory_usage(deep=True))
print(f"\nTotal: {memory_usage_mb(df):.2f} MB")
```

**Output:**
```
Index          128
sensor_id    24000
temperature  24000
humidity     24000
dtype: int64

Total: 2.29 MB
```

### 3.2 Automatic dtype Optimization

```python
def optimize_dtypes(df):
    """
    Optimize DataFrame dtypes automatically

    Returns:
        Optimized DataFrame ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Memory Saving
    """
    memory_before = df.memory_usage(deep=True).sum() / 1024**2

    # Optimize ‡∏ï‡∏≤‡∏° Column Type
    for col in df.columns:
        col_type = df[col].dtype

        # 1. Optimize Integer Columns
        if col_type == 'int64':
            c_min = df[col].min()
            c_max = df[col].max()

            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                df[col] = df[col].astype(np.int8)
            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                df[col] = df[col].astype(np.int16)
            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                df[col] = df[col].astype(np.int32)

        # 2. Optimize Float Columns
        elif col_type == 'float64':
            df[col] = df[col].astype(np.float32)

        # 3. Convert Object to Category (‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞)
        elif col_type == 'object':
            num_unique = df[col].nunique()
            num_total = len(df[col])

            # ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50% ‚Üí ‡πÉ‡∏ä‡πâ category
            if num_unique / num_total < 0.5:
                df[col] = df[col].astype('category')

    memory_after = df.memory_usage(deep=True).sum() / 1024**2
    saving = ((memory_before - memory_after) / memory_before) * 100

    print(f"Memory Before: {memory_before:.2f} MB")
    print(f"Memory After:  {memory_after:.2f} MB")
    print(f"Saving:        {saving:.1f}%")

    return df

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
df_optimized = optimize_dtypes(df)
```

### 3.3 Memory Profiling Workflow

```
Memory Optimization Process:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Load Data        ‚îÇ
‚îÇ    (without dtype)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Check Memory     ‚îÇ
‚îÇ    df.info()        ‚îÇ
‚îÇ    .memory_usage()  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Analyze Columns  ‚îÇ
‚îÇ    - Value ranges   ‚îÇ
‚îÇ    - Unique counts  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Choose dtype     ‚îÇ
‚îÇ    int8/16/32       ‚îÇ
‚îÇ    float32          ‚îÇ
‚îÇ    category         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Reload with      ‚îÇ
‚îÇ    dtype spec       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. Verify Saving    ‚îÇ
‚îÇ    Compare memory   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.4 Category dtype - ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏´‡∏£‡πà‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ?

**Category dtype** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Columns ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÄ‡∏¢‡∏≠‡∏∞

```python
import pandas as pd

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á
sensor_ids = ['S001', 'S002', 'S003', 'S004', 'S005'] * 10000  # 50,000 rows

# ‡πÅ‡∏ö‡∏ö object (string)
df_object = pd.DataFrame({'sensor_id': sensor_ids})
memory_object = df_object.memory_usage(deep=True).sum() / 1024

# ‡πÅ‡∏ö‡∏ö category
df_category = pd.DataFrame({'sensor_id': pd.Categorical(sensor_ids)})
memory_category = df_category.memory_usage(deep=True).sum() / 1024

print(f"Memory (object):   {memory_object:.2f} KB")
print(f"Memory (category): {memory_category:.2f} KB")
print(f"Saving: {((memory_object - memory_category) / memory_object * 100):.1f}%")
```

**Output:**
```
Memory (object):   2929.69 KB
Memory (category): 49.84 KB
Saving: 98.3%
```

**üí° ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏ä‡πâ category ‡πÄ‡∏°‡∏∑‡πà‡∏≠:**
- ‡∏Ñ‡πà‡∏≤‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤ 50% (‡πÄ‡∏ä‡πà‡∏ô sensor_id, status, location)
- ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡πà‡∏≤ unique ‡∏ô‡πâ‡∏≠‡∏¢ (< 50% ‡∏Ç‡∏≠‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)

---

## 4. Handling Missing Values

### 4.1 Missing Values ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**Missing Values** = ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ ‡πÅ‡∏™‡∏î‡∏á‡∏î‡πâ‡∏ß‡∏¢ `NaN`, `None`, `NULL`

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:**
- üîå Sensor ‡∏Ç‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏á
- üì° Network ‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢
- üíæ Storage Error
- üêõ Software Bug

### 4.2 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values

```python
import pandas as pd
import numpy as np

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing Values
data = {
    'timestamp': pd.date_range('2025-01-01', periods=10, freq='5min'),
    'sensor_id': ['S001'] * 10,
    'temperature': [25.5, np.nan, 26.0, 24.8, np.nan, 25.2, 26.5, np.nan, 25.0, 24.5],
    'humidity': [60, 62, np.nan, 58, 61, np.nan, 63, 59, np.nan, 60]
}
df = pd.DataFrame(data)

# 1. ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Missing Values ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Column
print("Missing Values Count:")
print(df.isnull().sum())

# 2. ‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå Missing
print("\nMissing Values Percentage:")
print((df.isnull().sum() / len(df) * 100).round(2))

# 3. ‡πÅ‡∏™‡∏î‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing Values
print("\nRows with Missing Values:")
print(df[df.isnull().any(axis=1)])

# 4. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Pattern
import matplotlib.pyplot as plt
import seaborn as sns

# Visualize Missing Pattern (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏ç‡πà)
# sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
```

**Output:**
```
Missing Values Count:
timestamp      0
sensor_id      0
temperature    3
humidity       3
dtype: int64

Missing Values Percentage:
timestamp      0.0
sensor_id      0.0
temperature   30.0
humidity      30.0
dtype: float64
```

### 4.3 ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values

```
Missing Value Handling Strategies:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    1. DROP (‡∏•‡∏ö‡∏ó‡∏¥‡πâ‡∏á)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úÖ ‡πÉ‡∏ä‡πâ‡πÄ‡∏°‡∏∑‡πà‡∏≠: Missing < 5%          ‚îÇ
‚îÇ  ‚ùå ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏µ‡∏¢: ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    2. FILL (‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤)                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  a) Forward Fill (ffill)            ‚îÇ
‚îÇ     ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏ï‡∏¥‡∏°             ‚îÇ
‚îÇ  b) Backward Fill (bfill)           ‚îÇ
‚îÇ     ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ‡∏°‡∏≤‡πÄ‡∏ï‡∏¥‡∏°                ‚îÇ
‚îÇ  c) Mean/Median/Mode                ‚îÇ
‚îÇ     ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏ï‡∏¥‡∏°                  ‚îÇ
‚îÇ  d) Interpolate                     ‚îÇ
‚îÇ     ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    3. FLAG (‡∏ó‡∏≥‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢)          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‡∏™‡∏£‡πâ‡∏≤‡∏á Column ‡πÉ‡∏´‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤ Missing   ‚îÇ
‚îÇ  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Machine Learning        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### 4.3.1 Method 1: Drop Missing Values

```python
# ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing Values
df_dropped = df.dropna()
print(f"Original: {len(df)} rows")
print(f"After Drop: {len(df_dropped)} rows")

# ‡∏•‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Column ‡∏ó‡∏µ‡πà‡∏°‡∏µ Missing > 50%
threshold = len(df) * 0.5
df_cleaned = df.dropna(thresh=threshold, axis=1)
```

#### 4.3.2 Method 2: Forward Fill (Ffill)

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Time-Series:** ‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏°‡∏≤‡πÄ‡∏ï‡∏¥‡∏°

```python
# Forward Fill - ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤
df_ffill = df.copy()
df_ffill['temperature'] = df_ffill['temperature'].fillna(method='ffill')
df_ffill['humidity'] = df_ffill['humidity'].fillna(method='ffill')

print(df_ffill)
```

**Before:**
```
   timestamp  temperature  humidity
0  00:00:00         25.5      60.0
1  00:05:00          NaN      62.0
2  00:10:00         26.0       NaN
```

**After Ffill:**
```
   timestamp  temperature  humidity
0  00:00:00         25.5      60.0
1  00:05:00         25.5      62.0  ‚Üê ‡πÉ‡∏ä‡πâ 25.5 ‡∏à‡∏≤‡∏Å‡∏ö‡∏ô‡∏°‡∏≤
2  00:10:00         26.0      62.0  ‚Üê ‡πÉ‡∏ä‡πâ 62 ‡∏à‡∏≤‡∏Å‡∏ö‡∏ô‡∏°‡∏≤
```

#### 4.3.3 Method 3: Mean/Median Fill

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ Time Dependency

```python
# Mean Fill - ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢
df_mean = df.copy()
df_mean['temperature'] = df_mean['temperature'].fillna(df_mean['temperature'].mean())
df_mean['humidity'] = df_mean['humidity'].fillna(df_mean['humidity'].mean())

print(f"Temperature Mean: {df['temperature'].mean():.2f}")
print(df_mean)

# Median Fill - ‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢ Median (‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ Outliers)
df_median = df.copy()
df_median['temperature'] = df_median['temperature'].fillna(df_median['temperature'].median())
```

#### 4.3.4 Method 4: Interpolation

**‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Time-Series:** ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á

```python
# Linear Interpolation
df_interpolated = df.copy()
df_interpolated['temperature'] = df_interpolated['temperature'].interpolate(method='linear')
df_interpolated['humidity'] = df_interpolated['humidity'].interpolate(method='linear')

print(df_interpolated)
```

**Before:**
```
   temperature
0         25.5
1          NaN
2         26.0
```

**After Interpolation:**
```
   temperature
0         25.5
1         25.75  ‚Üê (25.5 + 26.0) / 2
2         26.0
```

### 4.4 ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Methods

| Method | Use Case | Pros | Cons |
|--------|----------|------|------|
| **Drop** | Missing < 5% | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∞‡∏≠‡∏≤‡∏î 100% | ‡∏™‡∏π‡∏ç‡πÄ‡∏™‡∏µ‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• |
| **Ffill** | Time-Series Sequential | ‡∏£‡∏±‡∏Å‡∏©‡∏≤‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° | ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏ñ‡πâ‡∏≤ Missing ‡∏ô‡∏≤‡∏ô |
| **Mean** | Non-temporal | ‡∏á‡πà‡∏≤‡∏¢, ‡∏£‡∏ß‡∏î‡πÄ‡∏£‡πá‡∏ß | ‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° |
| **Median** | Data ‡∏°‡∏µ Outliers | Robust to outliers | ‡πÑ‡∏°‡πà‡∏™‡∏∞‡∏ó‡πâ‡∏≠‡∏ô‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏° |
| **Interpolate** | Time-Series | ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á | ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Å‡∏ß‡πà‡∏≤ |

### 4.5 Best Practices

```python
def handle_missing_values(df, strategy='auto'):
    """
    ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

    Parameters:
        df: DataFrame
        strategy: 'auto', 'drop', 'ffill', 'mean'

    Returns:
        DataFrame ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡πÅ‡∏•‡πâ‡∏ß
    """
    df_clean = df.copy()

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Missing Values
    missing_percent = (df_clean.isnull().sum() / len(df_clean) * 100)

    print("Missing Values Report:")
    print(missing_percent[missing_percent > 0])

    if strategy == 'auto':
        # ‡∏ñ‡πâ‡∏≤ Missing < 5% ‚Üí Drop
        # ‡∏ñ‡πâ‡∏≤ Time-Series ‚Üí Ffill
        # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ‚Üí Mean

        for col in df_clean.columns:
            missing_pct = missing_percent[col]

            if missing_pct == 0:
                continue
            elif missing_pct < 5:
                print(f"{col}: Drop (Missing {missing_pct:.1f}%)")
                df_clean = df_clean.dropna(subset=[col])
            elif pd.api.types.is_datetime64_any_dtype(df_clean.index):
                print(f"{col}: Forward Fill (Time-Series)")
                df_clean[col] = df_clean[col].fillna(method='ffill')
            else:
                print(f"{col}: Mean Fill")
                df_clean[col] = df_clean[col].fillna(df_clean[col].mean())

    elif strategy == 'drop':
        df_clean = df_clean.dropna()
    elif strategy == 'ffill':
        df_clean = df_clean.fillna(method='ffill')
    elif strategy == 'mean':
        df_clean = df_clean.fillna(df_clean.mean())

    print(f"\nOriginal: {len(df)} rows")
    print(f"Cleaned: {len(df_clean)} rows")

    return df_clean

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
df_cleaned = handle_missing_values(df, strategy='auto')
```

---

## 5. Data Integrity Checks

### 5.1 Data Integrity ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£?

**Data Integrity** = ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

**‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:**
- ‚úÖ Data Types ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
- ‚úÖ Ranges ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î
- ‚úÖ ‡πÑ‡∏°‡πà‡∏°‡∏µ Duplicates ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ
- ‚úÖ Relationships ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á

### 5.2 ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Data Integrity Checks

```
Data Integrity Check Types:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Type Check                    ‚îÇ
‚îÇ    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Type             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  - temperature ‚Üí float           ‚îÇ
‚îÇ  - timestamp ‚Üí datetime          ‚îÇ
‚îÇ  - sensor_id ‚Üí string/category   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Range Check                   ‚îÇ
‚îÇ    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤                ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  - temperature: -50¬∞C to 100¬∞C   ‚îÇ
‚îÇ  - humidity: 0% to 100%          ‚îÇ
‚îÇ  - pressure: 900 to 1100 hPa     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Null Check                    ‚îÇ
‚îÇ    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤ Missing            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  - Critical columns ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà NULL ‚îÇ
‚îÇ  - timestamp, sensor_id          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Duplicate Check               ‚îÇ
‚îÇ    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  - (timestamp, sensor_id) unique ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Consistency Check             ‚îÇ
‚îÇ    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  - Timestamp sequence            ‚îÇ
‚îÇ  - Related values logic          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 5.3 ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Types

```python
import pandas as pd

def check_data_types(df, expected_types):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Data Types ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏´‡∏°

    Parameters:
        df: DataFrame
        expected_types: dict ‡∏Ç‡∏≠‡∏á column: expected_type

    Returns:
        dict ‡∏Ç‡∏≠‡∏á validation results
    """
    results = {}

    for col, expected_type in expected_types.items():
        if col not in df.columns:
            results[col] = {'status': 'MISSING', 'message': 'Column not found'}
            continue

        actual_type = df[col].dtype

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö type
        if expected_type == 'datetime':
            is_valid = pd.api.types.is_datetime64_any_dtype(df[col])
        elif expected_type == 'numeric':
            is_valid = pd.api.types.is_numeric_dtype(df[col])
        elif expected_type == 'category':
            is_valid = df[col].dtype.name == 'category'
        else:
            is_valid = str(actual_type) == expected_type

        results[col] = {
            'status': 'PASS' if is_valid else 'FAIL',
            'expected': expected_type,
            'actual': str(actual_type)
        }

    return results

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
expected = {
    'timestamp': 'datetime',
    'sensor_id': 'category',
    'temperature': 'float32',
    'humidity': 'int16'
}

type_check = check_data_types(df, expected)
for col, result in type_check.items():
    print(f"{col}: {result['status']} - Expected: {result['expected']}, Actual: {result['actual']}")
```

### 5.4 ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Ranges

```python
def check_ranges(df, range_rules):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÑ‡∏´‡∏°

    Parameters:
        df: DataFrame
        range_rules: dict ‡∏Ç‡∏≠‡∏á column: (min, max)

    Returns:
        DataFrame ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏•‡∏∞‡πÄ‡∏°‡∏¥‡∏î rules
    """
    violations = {}

    for col, (min_val, max_val) in range_rules.items():
        if col not in df.columns:
            continue

        # ‡∏´‡∏≤‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏ô‡∏≠‡∏Å‡∏ä‡πà‡∏ß‡∏á
        out_of_range = df[
            (df[col] < min_val) | (df[col] > max_val)
        ]

        if len(out_of_range) > 0:
            violations[col] = {
                'count': len(out_of_range),
                'expected_range': (min_val, max_val),
                'actual_range': (df[col].min(), df[col].max()),
                'violations': out_of_range
            }
            print(f"‚ùå {col}: {len(out_of_range)} violations")
            print(f"   Expected: [{min_val}, {max_val}]")
            print(f"   Found: [{df[col].min():.2f}, {df[col].max():.2f}]")
        else:
            print(f"‚úÖ {col}: All values in range [{min_val}, {max_val}]")

    return violations

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
range_rules = {
    'temperature': (-50, 100),   # ¬∞C
    'humidity': (0, 100),        # %
    'pressure': (900, 1100)      # hPa
}

violations = check_ranges(df, range_rules)
```

### 5.5 ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Duplicates

```python
def check_duplicates(df, subset=None, keep='first'):
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥

    Parameters:
        df: DataFrame
        subset: columns ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏ = ‡∏ó‡∏∏‡∏Å column)
        keep: 'first', 'last', False

    Returns:
        DataFrame ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ã‡πâ‡∏≥
    """
    # ‡∏´‡∏≤ Duplicates
    duplicates = df[df.duplicated(subset=subset, keep=False)]
    num_duplicates = len(duplicates)

    if num_duplicates > 0:
        print(f"‚ùå Found {num_duplicates} duplicate rows")
        print("\nDuplicate Examples:")
        print(duplicates.head())

        # ‡∏•‡∏ö Duplicates
        df_cleaned = df.drop_duplicates(subset=subset, keep=keep)
        print(f"\n‚úÖ Removed {len(df) - len(df_cleaned)} duplicates")
        return df_cleaned
    else:
        print("‚úÖ No duplicates found")
        return df

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô - ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö (timestamp, sensor_id) ‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
df_no_dup = check_duplicates(df, subset=['timestamp', 'sensor_id'])
```

### 5.6 Data Quality Report

```python
def generate_data_quality_report(df):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô Data Quality ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô

    Returns:
        dict ‡∏Ç‡∏≠‡∏á quality metrics
    """
    report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'missing_values': {},
        'duplicates': 0,
        'data_types': {},
        'numeric_summary': {}
    }

    # 1. Missing Values
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        if missing_count > 0:
            report['missing_values'][col] = {
                'count': int(missing_count),
                'percentage': float((missing_count / len(df) * 100).round(2))
            }

    # 2. Duplicates
    report['duplicates'] = int(df.duplicated().sum())

    # 3. Data Types
    for col in df.columns:
        report['data_types'][col] = str(df[col].dtype)

    # 4. Numeric Summary
    numeric_cols = df.select_dtypes(include=['number']).columns
    for col in numeric_cols:
        report['numeric_summary'][col] = {
            'min': float(df[col].min()),
            'max': float(df[col].max()),
            'mean': float(df[col].mean()),
            'median': float(df[col].median()),
            'std': float(df[col].std())
        }

    return report

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
import json

quality_report = generate_data_quality_report(df)
print(json.dumps(quality_report, indent=2))
```

### 5.7 Complete Data Validation Pipeline

```python
def validate_sensor_data(df):
    """
    Validation Pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö IoT Sensor Data

    Returns:
        tuple: (is_valid, cleaned_df, report)
    """
    report = {
        'checks': [],
        'errors': [],
        'warnings': []
    }

    df_clean = df.copy()
    is_valid = True

    # Check 1: Required Columns
    required_cols = ['timestamp', 'sensor_id', 'temperature', 'humidity']
    missing_cols = [col for col in required_cols if col not in df_clean.columns]

    if missing_cols:
        report['errors'].append(f"Missing columns: {missing_cols}")
        is_valid = False
        return is_valid, df_clean, report

    report['checks'].append("‚úÖ All required columns present")

    # Check 2: Data Types
    if not pd.api.types.is_datetime64_any_dtype(df_clean['timestamp']):
        report['warnings'].append("timestamp is not datetime type")
        df_clean['timestamp'] = pd.to_datetime(df_clean['timestamp'])

    report['checks'].append("‚úÖ Data types validated")

    # Check 3: Missing Values
    critical_cols = ['timestamp', 'sensor_id']
    for col in critical_cols:
        missing = df_clean[col].isnull().sum()
        if missing > 0:
            report['errors'].append(f"{col} has {missing} NULL values")
            is_valid = False

    report['checks'].append("‚úÖ Critical columns checked")

    # Check 4: Value Ranges
    temp_out = df_clean[(df_clean['temperature'] < -50) | (df_clean['temperature'] > 100)]
    if len(temp_out) > 0:
        report['warnings'].append(f"Temperature out of range: {len(temp_out)} rows")

    hum_out = df_clean[(df_clean['humidity'] < 0) | (df_clean['humidity'] > 100)]
    if len(hum_out) > 0:
        report['warnings'].append(f"Humidity out of range: {len(hum_out)} rows")

    report['checks'].append("‚úÖ Value ranges checked")

    # Check 5: Duplicates
    dup_count = df_clean.duplicated(subset=['timestamp', 'sensor_id']).sum()
    if dup_count > 0:
        report['warnings'].append(f"Found {dup_count} duplicate records")
        df_clean = df_clean.drop_duplicates(subset=['timestamp', 'sensor_id'])

    report['checks'].append("‚úÖ Duplicates checked")

    return is_valid, df_clean, report

# ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
is_valid, df_validated, validation_report = validate_sensor_data(df)

print("Validation Report:")
for check in validation_report['checks']:
    print(check)

if validation_report['errors']:
    print("\nErrors:")
    for error in validation_report['errors']:
        print(f"  ‚ùå {error}")

if validation_report['warnings']:
    print("\nWarnings:")
    for warning in validation_report['warnings']:
        print(f"  ‚ö†Ô∏è  {warning}")

if is_valid:
    print("\n‚úÖ Data validation PASSED")
else:
    print("\n‚ùå Data validation FAILED")
```

---

## 6. ‡∏™‡∏£‡∏∏‡∏õ Module 2

### 6.1 Key Takeaways

‚úÖ **ETL Pipeline:** Extract ‚Üí Transform ‚Üí Load ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Data Engineering
‚úÖ **Optimized Loading:** ‡πÉ‡∏ä‡πâ dtype specification ‡∏•‡∏î Memory ‡πÑ‡∏î‡πâ 50-80%
‚úÖ **Missing Values:** ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (Drop, Ffill, Mean, Interpolate)
‚úÖ **Data Integrity:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Types, Ranges, Nulls, Duplicates ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
‚úÖ **Automation:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Functions ‡πÄ‡∏û‡∏∑‡πà‡∏≠ Validate ‡πÅ‡∏•‡∏∞ Clean ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥

### 6.2 Memory Optimization Summary

```
Memory Optimization Techniques:

Before:
  int64    ‚Üí 8 bytes
  float64  ‚Üí 8 bytes
  object   ‚Üí Varies (high)

After:
  int8/16  ‚Üí 1-2 bytes    ‚úÖ 75-87.5% saving
  float32  ‚Üí 4 bytes      ‚úÖ 50% saving
  category ‚Üí Optimized    ‚úÖ 90%+ saving

Total Saving: 50-80% üéâ
```

### 6.3 Missing Value Strategy Decision Tree

```
Missing Value Decision:

              Missing Values?
                    ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                       ‚îÇ
    Missing < 5%           Missing > 5%
        ‚îÇ                       ‚îÇ
      DROP                 Time-Series?
                          ‚îÇ           ‚îÇ
                         Yes         No
                          ‚îÇ           ‚îÇ
                      FFILL/     MEAN/MEDIAN
                   INTERPOLATE
```

### 6.4 Skills ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Module ‡∏ô‡∏µ‡πâ

| Skill | Level |
|-------|-------|
| ETL Pipeline Concepts | ‚≠ê‚≠ê‚≠ê |
| Optimized CSV Loading | ‚≠ê‚≠ê‚≠ê |
| dtype Optimization | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Missing Value Handling | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Data Validation | ‚≠ê‚≠ê‚≠ê |
| Memory Profiling | ‚≠ê‚≠ê |

### 6.5 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Module 3

‡πÉ‡∏ô Module ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
- **Feature Engineering:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Rolling Average, Time-based Features
- **Performance Benchmarking:** Pandas vs NumPy
- **Chunking:** ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å‡πÜ
- **Data Transformation Patterns**

---

## üìù Challenge Questions

‡∏Å‡πà‡∏≠‡∏ô‡πÑ‡∏õ Module 3 ‡∏•‡∏≠‡∏á‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ:

1. **Memory Optimization:** ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• humidity 0-100 ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ dtype ‡∏≠‡∏∞‡πÑ‡∏£? ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Memory Saving
   ```python
   # 100,000 rows
   # Original: int64
   # Optimized: ?
   # Saving = ?%
   ```

2. **Missing Values:** Sensor ‡∏Ç‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• 2 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (24 records) ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ‡πÑ‡∏´‡∏ô? ‡∏ó‡∏≥‡πÑ‡∏°?
   - a) Drop
   - b) Ffill
   - c) Mean
   - d) Interpolate

3. **Data Validation:** ‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏∑‡∏≠ Critical Columns ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ NULL ‡πÉ‡∏ô IoT Data?

4. **Range Check:** Temperature sensor ‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ valid range ‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà? Humidity?

5. **Category dtype:** ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ sensor_id 1000 ‡∏Ñ‡πà‡∏≤ ‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô 40% ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ category ‡πÑ‡∏´‡∏°?

---

## üéØ ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ: Labs & Practical Exercises

‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏≥ Lab ‡πÅ‡∏•‡πâ‡∏ß‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏±‡∏á?

**üëâ [‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥ Lab Module 2](./labs/lab-module-2.md)**

‡πÉ‡∏ô Lab ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ:
- **Lab 2.1:** ‡πÇ‡∏´‡∏•‡∏î `sensor_large.csv` ‡πÅ‡∏ö‡∏ö Optimized ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Memory Saving
- **Lab 2.2:** ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Missing Values ‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ï‡πà‡∏≤‡∏á‡πÜ
- **Lab 2.3:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Integrity ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á Quality Report
- **Challenge:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Complete ETL Pipeline

**‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤ Lab:** 1.5 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

---

**[‚¨ÖÔ∏è Module 1: Python Fundamentals](../module-1/module-1.md)** | **[‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏ó‡∏µ‡πà Wiki ‡∏´‡∏•‡∏±‡∏Å](../wiki.md)** | **[Module 3: ETL - Transformation & Optimization ‚û°Ô∏è](../module-3/module-3.md)**

# Module 3: ETL - Transformation & Optimization

**ğŸ¯ à¸§à¸±à¸•à¸–à¸¸à¸›à¸£à¸°à¸ªà¸‡à¸„à¹Œà¸à¸²à¸£à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰:**
- à¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸«à¸¥à¸±à¸à¸à¸²à¸£ Data Transformation à¹à¸¥à¸° Feature Engineering
- à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ Rolling Window Operations à¸ªà¸³à¸«à¸£à¸±à¸š Time-Series
- à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¹à¸¥à¸°à¸§à¸±à¸”à¸œà¸¥ Performance à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Pandas à¹à¸¥à¸° NumPy
- à¹ƒà¸Šà¹‰à¹€à¸—à¸„à¸™à¸´à¸„ Chunking à¸ªà¸³à¸«à¸£à¸±à¸šà¸ˆà¸±à¸”à¸à¸²à¸£à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
- à¹€à¸à¸´à¹ˆà¸¡à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸à¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸”à¹‰à¸§à¸¢ Vectorized Operations

**â±ï¸ à¹€à¸§à¸¥à¸²à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰:** 4.5 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ (à¸—à¸¤à¸©à¸à¸µ 2.5 à¸Šà¸¡. + Lab 2 à¸Šà¸¡.)

---

## ğŸ“š à¸ªà¸²à¸£à¸šà¸±à¸

1. [Data Transformation Overview](#1-data-transformation-overview)
2. [Feature Engineering à¸ªà¸³à¸«à¸£à¸±à¸š Time-Series](#2-feature-engineering-à¸ªà¸³à¸«à¸£à¸±à¸š-time-series)
3. [Rolling Window Operations](#3-rolling-window-operations)
4. [Performance Optimization Techniques](#4-performance-optimization-techniques)
5. [Pandas vs NumPy Performance](#5-pandas-vs-numpy-performance)
6. [Chunking Strategy à¸ªà¸³à¸«à¸£à¸±à¸š Big Data](#6-chunking-strategy-à¸ªà¸³à¸«à¸£à¸±à¸š-big-data)
7. [Vectorized Operations](#7-vectorized-operations)
8. [Labs & Practical Exercises](./labs/lab-module-3.md)

---

## 1. Data Transformation Overview

### 1.1 Data Transformation à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

**Data Transformation** à¹€à¸›à¹‡à¸™à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸ªà¸³à¸„à¸±à¸à¹ƒà¸™ ETL Pipeline à¸—à¸µà¹ˆà¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸”à¸´à¸šà¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸šà¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ

```
ETL Pipeline:

Extract          Transform              Load
(à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)  â†’  (à¹à¸›à¸¥à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)      â†’  (à¸šà¸±à¸™à¸—à¸¶à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥)
   â†“                  â†“                     â†“
Raw Data    â†’   Processing Steps   â†’   Clean Data
                â€¢ Cleaning                  â†“
                â€¢ Feature Eng.          Analytics
                â€¢ Aggregation           ML Models
                â€¢ Optimization          Reports
```

### 1.2 à¸›à¸£à¸°à¹€à¸ à¸—à¸‚à¸­à¸‡ Transformation

**1. Data Cleaning Transformations**
- à¸ˆà¸±à¸”à¸à¸²à¸£ Missing Values (Ffill, Interpolate)
- à¸¥à¸š Outliers
- à¹à¸à¹‰à¹„à¸‚ Data Types

**2. Feature Engineering Transformations**
- à¸ªà¸£à¹‰à¸²à¸‡ Features à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡
- Rolling Statistics (Mean, Std, Min, Max)
- Time-based Features (Hour, Day of Week)

**3. Aggregation Transformations**
- Group by operations
- Resampling (à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ Frequency)
- Pivot tables

**4. Performance Optimization Transformations**
- Vectorization
- Chunking
- Efficient Data Types

### 1.3 Transformation Workflow à¹ƒà¸™ IoT Data Pipeline

```python
import pandas as pd
import numpy as np

# Raw Data
df = pd.read_csv('sensor_raw.csv')

# 1. Data Cleaning
df['timestamp'] = pd.to_datetime(df['timestamp'])
df['temperature'].fillna(method='ffill', inplace=True)

# 2. Feature Engineering
df['temp_rolling_5min'] = df['temperature'].rolling(window=5).mean()
df['hour'] = df['timestamp'].dt.hour

# 3. Aggregation
hourly_avg = df.groupby('hour')['temperature'].mean()

# 4. Optimization
df['temperature'] = df['temperature'].astype(np.float32)
```

---

## 2. Feature Engineering à¸ªà¸³à¸«à¸£à¸±à¸š Time-Series

### 2.1 Feature Engineering à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

**Feature Engineering** = à¸à¸²à¸£à¸ªà¸£à¹‰à¸²à¸‡ Features (à¸„à¸¸à¸“à¸ªà¸¡à¸šà¸±à¸•à¸´) à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹€à¸à¸·à¹ˆà¸­à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸«à¸£à¸·à¸­ ML Model

**à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡:**
- à¸ˆà¸²à¸ `timestamp` â†’ à¸ªà¸£à¹‰à¸²à¸‡ `hour`, `day_of_week`, `is_weekend`
- à¸ˆà¸²à¸ `temperature` â†’ à¸ªà¸£à¹‰à¸²à¸‡ `temp_rolling_avg`, `temp_change`

### 2.2 Time-Series Features à¸—à¸µà¹ˆà¸ªà¸³à¸„à¸±à¸

#### 2.2.1 Time-based Features

```python
import pandas as pd

# à¸ªà¸£à¹‰à¸²à¸‡ Sample Data
df = pd.DataFrame({
    'timestamp': pd.date_range('2025-01-01', periods=100, freq='5min'),
    'temperature': np.random.normal(25, 2, 100)
})

df['timestamp'] = pd.to_datetime(df['timestamp'])

# Extract Time-based Features
df['hour'] = df['timestamp'].dt.hour
df['day_of_week'] = df['timestamp'].dt.dayofweek  # 0=Monday
df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5
df['month'] = df['timestamp'].dt.month
df['date'] = df['timestamp'].dt.date

print(df[['timestamp', 'hour', 'day_of_week', 'is_weekend']].head())
```

**Output:**
```
            timestamp  hour  day_of_week  is_weekend
0 2025-01-01 00:00:00     0            2       False
1 2025-01-01 00:05:00     0            2       False
2 2025-01-01 00:10:00     0            2       False
3 2025-01-01 00:15:00     0            2       False
4 2025-01-01 00:20:00     0            2       False
```

#### 2.2.2 Lag Features (à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¢à¹‰à¸­à¸™à¸«à¸¥à¸±à¸‡)

```python
# Previous values
df['temp_lag_1'] = df['temperature'].shift(1)  # 1 period à¸à¹ˆà¸­à¸™
df['temp_lag_2'] = df['temperature'].shift(2)  # 2 periods à¸à¹ˆà¸­à¸™

# Change from previous
df['temp_change'] = df['temperature'] - df['temp_lag_1']
df['temp_change_pct'] = (df['temperature'] - df['temp_lag_1']) / df['temp_lag_1'] * 100

print(df[['temperature', 'temp_lag_1', 'temp_change', 'temp_change_pct']].head())
```

**Use Cases:**
- **Anomaly Detection:** à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¸à¸²à¸£à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¸œà¸´à¸”à¸›à¸à¸•à¸´
- **Trend Analysis:** à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸™à¸§à¹‚à¸™à¹‰à¸¡
- **ML Models:** Features à¸ªà¸³à¸«à¸£à¸±à¸š Prediction

#### 2.2.3 Rolling Statistics Features

```python
# Rolling Mean (Moving Average)
df['temp_ma_5'] = df['temperature'].rolling(window=5).mean()
df['temp_ma_10'] = df['temperature'].rolling(window=10).mean()

# Rolling Std (Volatility)
df['temp_std_5'] = df['temperature'].rolling(window=5).std()

# Rolling Min/Max
df['temp_min_5'] = df['temperature'].rolling(window=5).min()
df['temp_max_5'] = df['temperature'].rolling(window=5).max()

# Rolling Range
df['temp_range_5'] = df['temp_max_5'] - df['temp_min_5']

print(df[['temperature', 'temp_ma_5', 'temp_std_5', 'temp_range_5']].head(10))
```

### 2.3 à¸ à¸²à¸à¹à¸ªà¸”à¸‡ Feature Engineering Flow

```
Raw Time-Series Data
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Original Features                    â”‚
â”‚  â€¢ timestamp: 2025-01-01 00:00:00    â”‚
â”‚  â€¢ temperature: 25.5                  â”‚
â”‚  â€¢ humidity: 60                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ Feature Engineering
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Engineered Features                  â”‚
â”‚                                       â”‚
â”‚  Time-based:                          â”‚
â”‚  â€¢ hour: 0                            â”‚
â”‚  â€¢ day_of_week: 2                     â”‚
â”‚  â€¢ is_weekend: False                  â”‚
â”‚                                       â”‚
â”‚  Lag Features:                        â”‚
â”‚  â€¢ temp_lag_1: 25.3                   â”‚
â”‚  â€¢ temp_change: +0.2                  â”‚
â”‚                                       â”‚
â”‚  Rolling Features:                    â”‚
â”‚  â€¢ temp_rolling_5min: 25.4            â”‚
â”‚  â€¢ temp_std_5min: 0.3                 â”‚
â”‚                                       â”‚
â”‚  Domain Features:                     â”‚
â”‚  â€¢ temp_category: Normal              â”‚
â”‚  â€¢ temp_fahrenheit: 77.9              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.4 Domain-Specific Features à¸ªà¸³à¸«à¸£à¸±à¸š IoT

```python
# Temperature Category
def categorize_temp(temp):
    if temp < 20:
        return 'Cold'
    elif temp < 25:
        return 'Cool'
    elif temp < 28:
        return 'Normal'
    else:
        return 'Hot'

df['temp_category'] = df['temperature'].apply(categorize_temp)

# Temperature Comfort Index (à¸ªà¸¡à¸¡à¸•à¸´)
df['comfort_index'] = df['temperature'] - (0.55 * (1 - df['humidity']/100) * (df['temperature'] - 14.5))

# Celsius to Fahrenheit
df['temp_fahrenheit'] = (df['temperature'] * 9/5) + 32

print(df[['temperature', 'humidity', 'temp_category', 'comfort_index']].head())
```

---

## 3. Rolling Window Operations

### 3.1 Rolling Window à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

**Rolling Window** (à¸«à¸£à¸·à¸­ Moving Window) à¸„à¸·à¸­à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸„à¹ˆà¸²à¸ªà¸–à¸´à¸•à¸´à¸ˆà¸²à¸ Subset à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸µà¹ˆà¹€à¸¥à¸·à¹ˆà¸­à¸™à¹„à¸›à¸•à¸²à¸¡à¸¥à¸³à¸”à¸±à¸šà¹€à¸§à¸¥à¸²

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ:**
- âœ… **Smoothing:** à¸¥à¸”à¸„à¸§à¸²à¸¡à¸œà¸±à¸™à¸œà¸§à¸™à¸‚à¸­à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
- âœ… **Trend Detection:** à¸¡à¸­à¸‡à¹€à¸«à¹‡à¸™à¹à¸™à¸§à¹‚à¸™à¹‰à¸¡à¹„à¸”à¹‰à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸‚à¸¶à¹‰à¸™
- âœ… **Noise Reduction:** à¸¥à¸”à¸ªà¸±à¸à¸à¸²à¸“à¸£à¸šà¸à¸§à¸™

### 3.2 à¸ à¸²à¸à¹à¸ªà¸”à¸‡ Rolling Window Concept

```
Temperature Data: [22, 24, 23, 25, 26, 24, 23, 22, 21, 23]

Rolling Window = 3:

Window 1: [22, 24, 23] â†’ Mean = 23.0
Window 2:     [24, 23, 25] â†’ Mean = 24.0
Window 3:         [23, 25, 26] â†’ Mean = 24.7
Window 4:             [25, 26, 24] â†’ Mean = 25.0
Window 5:                 [26, 24, 23] â†’ Mean = 24.3
...

Result: [NaN, NaN, 23.0, 24.0, 24.7, 25.0, 24.3, ...]
```

### 3.3 à¸à¸²à¸£à¹ƒà¸Šà¹‰à¸‡à¸²à¸™ Rolling Window à¹ƒà¸™ Pandas

#### 3.3.1 Rolling Mean (Moving Average)

```python
import pandas as pd
import numpy as np

# à¸ªà¸£à¹‰à¸²à¸‡ Sample Data
np.random.seed(42)
df = pd.DataFrame({
    'timestamp': pd.date_range('2025-01-01', periods=50, freq='5min'),
    'temperature': 25 + np.random.normal(0, 2, 50)
})

# Rolling Mean
df['temp_ma_5'] = df['temperature'].rolling(window=5).mean()
df['temp_ma_10'] = df['temperature'].rolling(window=10).mean()

print("Rolling Mean Example:")
print(df[['temperature', 'temp_ma_5', 'temp_ma_10']].head(15))
```

**Parameters:**
- `window`: à¸ˆà¸³à¸™à¸§à¸™ periods à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“
- `min_periods`: à¸ˆà¸³à¸™à¸§à¸™ periods à¸‚à¸±à¹‰à¸™à¸•à¹ˆà¸³à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸¡à¸µà¹€à¸à¸·à¹ˆà¸­à¸„à¸³à¸™à¸§à¸“ (default = window)
- `center`: à¸–à¹‰à¸² True à¸ˆà¸°à¸„à¸³à¸™à¸§à¸“à¸ˆà¸²à¸à¸•à¸£à¸‡à¸à¸¥à¸²à¸‡ window

#### 3.3.2 Rolling Standard Deviation

```python
# Rolling Std (à¸§à¸±à¸”à¸„à¸§à¸²à¸¡à¸œà¸±à¸™à¸œà¸§à¸™)
df['temp_volatility_5'] = df['temperature'].rolling(window=5).std()

# Rolling Coefficient of Variation
df['temp_cv_5'] = df['temp_volatility_5'] / df['temp_ma_5']

print("Rolling Std Example:")
print(df[['temperature', 'temp_ma_5', 'temp_volatility_5']].head(15))
```

#### 3.3.3 Rolling Min/Max (Range)

```python
# Rolling Min/Max
df['temp_min_10'] = df['temperature'].rolling(window=10).min()
df['temp_max_10'] = df['temperature'].rolling(window=10).max()
df['temp_range_10'] = df['temp_max_10'] - df['temp_min_10']

print("Rolling Range Example:")
print(df[['temperature', 'temp_min_10', 'temp_max_10', 'temp_range_10']].tail(10))
```

### 3.4 Time-based Rolling Windows

à¸ªà¸³à¸«à¸£à¸±à¸š Time-Series à¸„à¸§à¸£à¹ƒà¸Šà¹‰ Time-based Window à¹à¸—à¸™ Count-based

```python
# à¸•à¸±à¹‰à¸‡ timestamp à¹€à¸›à¹‡à¸™ index
df.set_index('timestamp', inplace=True)

# Rolling Window 30 à¸™à¸²à¸—à¸µ (à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰ window=6)
df['temp_ma_30min'] = df['temperature'].rolling('30min').mean()

# Rolling Window 1 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡
df['temp_ma_1h'] = df['temperature'].rolling('1h').mean()

# Rolling Window 1 à¸§à¸±à¸™
df['temp_ma_1d'] = df['temperature'].rolling('1D').mean()

print("Time-based Rolling Example:")
print(df[['temperature', 'temp_ma_30min', 'temp_ma_1h']].head(20))
```

**à¸‚à¹‰à¸­à¸”à¸µà¸‚à¸­à¸‡ Time-based Rolling:**
- âœ… à¸£à¸­à¸‡à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ Irregular frequency
- âœ… à¸„à¸§à¸²à¸¡à¸«à¸¡à¸²à¸¢à¸Šà¸±à¸”à¹€à¸ˆà¸™à¸à¸§à¹ˆà¸² (30 à¸™à¸²à¸—à¸µà¹à¸—à¸™ 6 periods)
- âœ… à¹„à¸¡à¹ˆà¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹à¸›à¸¥à¸‡à¹€à¸¡à¸·à¹ˆà¸­ frequency à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™

### 3.5 Rolling Window à¸ªà¸³à¸«à¸£à¸±à¸š Anomaly Detection

```python
# à¸„à¸³à¸™à¸§à¸“ Upper/Lower Bounds (Mean Â± 2*Std)
window_size = 20
df['temp_ma'] = df['temperature'].rolling(window=window_size).mean()
df['temp_std'] = df['temperature'].rolling(window=window_size).std()

df['upper_bound'] = df['temp_ma'] + (2 * df['temp_std'])
df['lower_bound'] = df['temp_ma'] - (2 * df['temp_std'])

# à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸š Anomaly
df['is_anomaly'] = (df['temperature'] > df['upper_bound']) | \
                   (df['temperature'] < df['lower_bound'])

print(f"Anomalies detected: {df['is_anomaly'].sum()}")
print("\nAnomalies:")
print(df[df['is_anomaly']][['temperature', 'temp_ma', 'upper_bound', 'lower_bound']])
```

---

## 4. Performance Optimization Techniques

### 4.1 à¸—à¸³à¹„à¸¡à¸•à¹‰à¸­à¸‡ Optimize Performance?

à¹€à¸¡à¸·à¹ˆà¸­à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¡à¸µà¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆà¸‚à¸¶à¹‰à¸™ Performance à¸à¸¥à¸²à¸¢à¹€à¸›à¹‡à¸™à¸›à¸±à¸à¸«à¸²à¸ªà¸³à¸„à¸±à¸

**à¸›à¸±à¸à¸«à¸²à¸—à¸µà¹ˆà¸à¸š:**
- â±ï¸ **Slow Processing:** à¹ƒà¸Šà¹‰à¹€à¸§à¸¥à¸²à¸™à¸²à¸™à¹€à¸à¸´à¸™à¹„à¸›
- ğŸ’¾ **Memory Error:** RAM à¹„à¸¡à¹ˆà¸à¸­
- ğŸ”¥ **High CPU Usage:** à¹ƒà¸Šà¹‰à¸—à¸£à¸±à¸à¸¢à¸²à¸à¸£à¸¡à¸²à¸à¹€à¸à¸´à¸™à¹„à¸›

### 4.2 Performance Optimization Strategies

```
Optimization Techniques:

1. Data Type Optimization
   â”œâ”€â–¶ à¹ƒà¸Šà¹‰ dtype à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡ (float32 à¹à¸—à¸™ float64)
   â”œâ”€â–¶ à¹ƒà¸Šà¹‰ category à¸ªà¸³à¸«à¸£à¸±à¸š String à¸‹à¹‰à¸³à¹†
   â””â”€â–¶ à¸¥à¸” Memory Usage 50-90%

2. Vectorization
   â”œâ”€â–¶ à¹ƒà¸Šà¹‰ NumPy operations
   â”œâ”€â–¶ à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ Python loops
   â””â”€â–¶ à¹€à¸£à¹‡à¸§à¸‚à¸¶à¹‰à¸™ 10-100 à¹€à¸—à¹ˆà¸²

3. Chunking
   â”œâ”€â–¶ à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¹†
   â”œâ”€â–¶ à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸—à¸µà¸¥à¸° Chunk
   â””â”€â–¶ à¸£à¸­à¸‡à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ

4. Efficient Algorithms
   â”œâ”€â–¶ à¹€à¸¥à¸·à¸­à¸ Algorithm à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡
   â”œâ”€â–¶ à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ Nested Loops
   â””â”€â–¶ à¹ƒà¸Šà¹‰ Built-in Functions
```

### 4.3 Data Type Optimization

#### 4.3.1 Numeric Data Types

```python
import pandas as pd
import numpy as np

# à¸ªà¸£à¹‰à¸²à¸‡ Sample Data
df = pd.DataFrame({
    'sensor_id': ['S001'] * 1000,
    'temperature': np.random.uniform(20, 30, 1000),
    'humidity': np.random.randint(40, 80, 1000)
})

print("=== Before Optimization ===")
print(df.dtypes)
print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\n")

# Optimize
df_optimized = df.copy()
df_optimized['sensor_id'] = df_optimized['sensor_id'].astype('category')
df_optimized['temperature'] = df_optimized['temperature'].astype(np.float32)
df_optimized['humidity'] = df_optimized['humidity'].astype(np.int8)

print("=== After Optimization ===")
print(df_optimized.dtypes)
print(f"Memory Usage: {df_optimized.memory_usage(deep=True).sum() / 1024:.2f} KB")

memory_saved = df.memory_usage(deep=True).sum() - df_optimized.memory_usage(deep=True).sum()
percent_saved = (memory_saved / df.memory_usage(deep=True).sum()) * 100
print(f"Memory Saved: {memory_saved / 1024:.2f} KB ({percent_saved:.1f}%)")
```

#### 4.3.2 à¸•à¸²à¸£à¸²à¸‡à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸š Data Types

| Type | Size | Range | Use Case |
|------|------|-------|----------|
| **int8** | 1 byte | -128 to 127 | Temperature (Â°C), Small integers |
| **int16** | 2 bytes | -32,768 to 32,767 | Medium integers |
| **int32** | 4 bytes | -2B to 2B | Large integers |
| **float32** | 4 bytes | Â±3.4e38 (7 digits) | Temperature (decimal) |
| **float64** | 8 bytes | Â±1.7e308 (15 digits) | High precision (default) |
| **category** | Variable | N/A | Repeated strings |

### 4.4 Function-based Optimization

```python
import time

# Sample Data
n = 1_000_000
temperatures = np.random.uniform(20, 30, n)

# âŒ Bad: Python Loop
start = time.time()
fahrenheit_loop = []
for temp in temperatures:
    fahrenheit_loop.append((temp * 9/5) + 32)
time_loop = time.time() - start

# âŒ Bad: Apply Function
df = pd.DataFrame({'temperature': temperatures})
start = time.time()
df['fahrenheit'] = df['temperature'].apply(lambda x: (x * 9/5) + 32)
time_apply = time.time() - start

# âœ… Good: Vectorized Operation
start = time.time()
fahrenheit_vectorized = (temperatures * 9/5) + 32
time_vectorized = time.time() - start

print(f"Python Loop:  {time_loop:.4f} sec")
print(f"Pandas Apply: {time_apply:.4f} sec")
print(f"Vectorized:   {time_vectorized:.4f} sec")
print(f"\nSpeedup (Loop â†’ Vectorized): {time_loop / time_vectorized:.1f}x")
```

**Expected Output:**
```
Python Loop:  0.5234 sec
Pandas Apply: 0.3421 sec
Vectorized:   0.0045 sec

Speedup (Loop â†’ Vectorized): 116.3x
```

---

## 5. Pandas vs NumPy Performance

### 5.1 Pandas vs NumPy: à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆà¸„à¸§à¸£à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£?

| Aspect | Pandas | NumPy |
|--------|--------|-------|
| **à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§** | âš¡âš¡ à¸„à¹ˆà¸­à¸™à¸‚à¹‰à¸²à¸‡à¹€à¸£à¹‡à¸§ | âš¡âš¡âš¡ à¹€à¸£à¹‡à¸§à¸¡à¸²à¸ |
| **Use Case** | Data Analysis, ETL | Numerical Computing |
| **à¹‚à¸„à¸£à¸‡à¸ªà¸£à¹‰à¸²à¸‡** | DataFrame (Labels) | Array (No Labels) |
| **Operations** | Rich (groupby, merge) | Basic Math Operations |
| **Memory** | à¹ƒà¸Šà¹‰à¸¡à¸²à¸à¸à¸§à¹ˆà¸² | à¹ƒà¸Šà¹‰à¸™à¹‰à¸­à¸¢à¸à¸§à¹ˆà¸² |

### 5.2 Performance Comparison: Basic Operations

```python
import pandas as pd
import numpy as np
import time

# Prepare Data
n = 10_000_000
data = np.random.uniform(0, 100, n)
df = pd.DataFrame({'value': data})

print(f"Data Size: {n:,} records\n")

# Test 1: Addition
start = time.time()
result_pandas = df['value'] + 10
time_pandas_add = time.time() - start

start = time.time()
result_numpy = data + 10
time_numpy_add = time.time() - start

print("=== Addition (x + 10) ===")
print(f"Pandas: {time_pandas_add:.4f} sec")
print(f"NumPy:  {time_numpy_add:.4f} sec")
print(f"NumPy is {time_pandas_add / time_numpy_add:.1f}x faster\n")

# Test 2: Mathematical Operations
start = time.time()
result_pandas = (df['value'] * 2) - 5
time_pandas_math = time.time() - start

start = time.time()
result_numpy = (data * 2) - 5
time_numpy_math = time.time() - start

print("=== Math Operations (x * 2 - 5) ===")
print(f"Pandas: {time_pandas_math:.4f} sec")
print(f"NumPy:  {time_numpy_math:.4f} sec")
print(f"NumPy is {time_pandas_math / time_numpy_math:.1f}x faster\n")

# Test 3: Statistical Operations
start = time.time()
mean_pandas = df['value'].mean()
time_pandas_mean = time.time() - start

start = time.time()
mean_numpy = data.mean()
time_numpy_mean = time.time() - start

print("=== Mean Calculation ===")
print(f"Pandas: {time_pandas_mean:.6f} sec (mean = {mean_pandas:.2f})")
print(f"NumPy:  {time_numpy_mean:.6f} sec (mean = {mean_numpy:.2f})")
print(f"NumPy is {time_pandas_mean / time_numpy_mean:.1f}x faster")
```

### 5.3 Benchmark: Array Subtraction

à¸à¸²à¸£à¹€à¸›à¸£à¸µà¸¢à¸šà¹€à¸—à¸µà¸¢à¸šà¸à¸²à¸£ Subtract à¸£à¸°à¸«à¸§à¹ˆà¸²à¸‡ Arrays

```python
import numpy as np
import pandas as pd
import time

# Prepare Data
size = 5_000_000
arr1 = np.random.uniform(0, 100, size)
arr2 = np.random.uniform(0, 100, size)

s1 = pd.Series(arr1)
s2 = pd.Series(arr2)

# NumPy Subtraction
start = time.time()
result_numpy = arr1 - arr2
time_numpy = time.time() - start

# Pandas Series Subtraction
start = time.time()
result_pandas = s1 - s2
time_pandas = time.time() - start

print("=== Array/Series Subtraction Benchmark ===")
print(f"Data Size: {size:,} elements\n")
print(f"NumPy Array:   {time_numpy:.6f} sec")
print(f"Pandas Series: {time_pandas:.6f} sec")
print(f"\nSpeedup: NumPy is {time_pandas / time_numpy:.2f}x faster")
```

**Expected Output:**
```
=== Array/Series Subtraction Benchmark ===
Data Size: 5,000,000 elements

NumPy Array:   0.008234 sec
Pandas Series: 0.015678 sec

Speedup: NumPy is 1.90x faster
```

### 5.4 à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆà¸„à¸§à¸£à¹ƒà¸Šà¹‰ NumPy?

**âœ… à¹ƒà¸Šà¹‰ NumPy à¹€à¸¡à¸·à¹ˆà¸­:**
- à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸—à¸²à¸‡à¸„à¸“à¸´à¸•à¸¨à¸²à¸ªà¸•à¸£à¹Œà¸šà¸£à¸´à¸ªà¸¸à¸—à¸˜à¸´à¹Œ
- à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Performance à¸ªà¸¹à¸‡à¸ªà¸¸à¸”
- à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Labels (Column Names, Index)
- à¸—à¸³à¸‡à¸²à¸™à¸à¸±à¸š Multi-dimensional Arrays

**âœ… à¹ƒà¸Šà¹‰ Pandas à¹€à¸¡à¸·à¹ˆà¸­:**
- à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Labels à¹à¸¥à¸° Column Names
- à¸—à¸³ ETL, Data Cleaning
- à¹ƒà¸Šà¹‰ groupby, merge, pivot operations
- à¸­à¹ˆà¸²à¸™/à¹€à¸‚à¸µà¸¢à¸™à¹„à¸Ÿà¸¥à¹Œ (CSV, Excel, SQL)

### 5.5 Best Practice: à¹ƒà¸Šà¹‰à¸£à¹ˆà¸§à¸¡à¸à¸±à¸™

```python
# à¹ƒà¸Šà¹‰ Pandas à¸ªà¸³à¸«à¸£à¸±à¸š Data Structure
df = pd.read_csv('sensor_data.csv')

# à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ NumPy à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸«à¸™à¸±à¸
temperatures = df['temperature'].values  # NumPy array
humidities = df['humidity'].values

# à¹ƒà¸Šà¹‰ NumPy à¸„à¸³à¸™à¸§à¸“ (à¹€à¸£à¹‡à¸§)
comfort_index = temperatures - (0.55 * (1 - humidities/100) * (temperatures - 14.5))

# à¹€à¸­à¸²à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸à¸¥à¸±à¸šà¹„à¸›à¹ƒà¸™ Pandas
df['comfort_index'] = comfort_index
```

---

## 6. Chunking Strategy à¸ªà¸³à¸«à¸£à¸±à¸š Big Data

### 6.1 Chunking à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

**Chunking** = à¹à¸šà¹ˆà¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆà¹€à¸›à¹‡à¸™à¸ªà¹ˆà¸§à¸™à¹€à¸¥à¹‡à¸à¹† (Chunks) à¹à¸¥à¹‰à¸§à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¸—à¸µà¸¥à¸°à¸ªà¹ˆà¸§à¸™

**à¸›à¸£à¸°à¹‚à¸¢à¸Šà¸™à¹Œ:**
- âœ… **à¸¥à¸” Memory Usage:** à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹‚à¸«à¸¥à¸”à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¸£à¹‰à¸­à¸¡à¸à¸±à¸™
- âœ… **à¸£à¸­à¸‡à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ:** à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸”à¹‰à¹à¸¡à¹‰ RAM à¸ˆà¸³à¸à¸±à¸”
- âœ… **Streaming Processing:** à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹„à¸”à¹‰à¸—à¸±à¸™à¸—à¸µà¸—à¸µà¹ˆà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¡à¸²

### 6.2 à¸ à¸²à¸à¹à¸ªà¸”à¸‡ Chunking Concept

```
Large CSV File (1 GB):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Row 1 - 100,000                    â”‚ â† Chunk 1
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Row 100,001 - 200,000              â”‚ â† Chunk 2
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Row 200,001 - 300,000              â”‚ â† Chunk 3
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ...                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Process Flow:
Chunk 1 â†’ Process â†’ Write
Chunk 2 â†’ Process â†’ Write
Chunk 3 â†’ Process â†’ Write
...

Memory Usage: à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰ 1 GB â†’ à¹ƒà¸Šà¹‰à¹à¸„à¹ˆ ~10 MB/chunk
```

### 6.3 à¸à¸²à¸£à¹ƒà¸Šà¹‰ Chunking à¹ƒà¸™ Pandas

#### 6.3.1 à¸­à¹ˆà¸²à¸™ CSV à¹à¸šà¸š Chunking

```python
import pandas as pd

# à¸­à¹ˆà¸²à¸™ CSV à¸—à¸µà¸¥à¸° Chunk
chunk_size = 100_000
chunks_processed = 0

for chunk in pd.read_csv('sensor_large.csv', chunksize=chunk_size):
    # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸•à¹ˆà¸¥à¸° chunk
    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])
    chunk['temp_celsius'] = chunk['temperature']
    chunk['temp_fahrenheit'] = (chunk['temperature'] * 9/5) + 32

    # à¸šà¸±à¸™à¸—à¸¶à¸à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ
    mode = 'w' if chunks_processed == 0 else 'a'
    header = True if chunks_processed == 0 else False
    chunk.to_csv('processed_output.csv', mode=mode, header=header, index=False)

    chunks_processed += 1
    print(f"Processed chunk {chunks_processed}: {len(chunk)} records")

print(f"\nTotal chunks processed: {chunks_processed}")
```

#### 6.3.2 Chunking à¸ªà¸³à¸«à¸£à¸±à¸š Aggregation

```python
# à¸„à¸³à¸™à¸§à¸“ Statistics à¸ˆà¸²à¸à¹„à¸Ÿà¸¥à¹Œà¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
chunk_size = 100_000
temp_sum = 0
temp_count = 0
temp_min = float('inf')
temp_max = float('-inf')

for chunk in pd.read_csv('sensor_large.csv', chunksize=chunk_size):
    temp_sum += chunk['temperature'].sum()
    temp_count += len(chunk)
    temp_min = min(temp_min, chunk['temperature'].min())
    temp_max = max(temp_max, chunk['temperature'].max())

# à¸„à¸³à¸™à¸§à¸“ Overall Statistics
temp_mean = temp_sum / temp_count

print("=== Overall Statistics ===")
print(f"Total Records: {temp_count:,}")
print(f"Mean Temperature: {temp_mean:.2f}Â°C")
print(f"Min Temperature: {temp_min:.2f}Â°C")
print(f"Max Temperature: {temp_max:.2f}Â°C")
```

### 6.4 Chunking Strategy Best Practices

#### 6.4.1 à¸à¸²à¸£à¹€à¸¥à¸·à¸­à¸ Chunk Size

```python
import os

def calculate_optimal_chunk_size(file_path, available_memory_mb=500):
    """
    à¸„à¸³à¸™à¸§à¸“ Chunk Size à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡

    Args:
        file_path: Path to CSV file
        available_memory_mb: Available memory in MB

    Returns:
        Optimal chunk size (rows)
    """
    # à¸›à¸£à¸°à¸¡à¸²à¸“ file size
    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)

    # à¸­à¹ˆà¸²à¸™ sample à¹€à¸à¸·à¹ˆà¸­à¸›à¸£à¸°à¸¡à¸²à¸“ memory per row
    sample = pd.read_csv(file_path, nrows=1000)
    memory_per_1000_rows = sample.memory_usage(deep=True).sum() / (1024 * 1024)
    memory_per_row = memory_per_1000_rows / 1000

    # à¸„à¸³à¸™à¸§à¸“ chunk size
    chunk_size = int(available_memory_mb / memory_per_row)

    print(f"File Size: {file_size_mb:.2f} MB")
    print(f"Memory per Row: {memory_per_row:.4f} MB")
    print(f"Recommended Chunk Size: {chunk_size:,} rows")

    return chunk_size

# Example
# optimal_size = calculate_optimal_chunk_size('sensor_large.csv')
```

**à¹à¸™à¸§à¸—à¸²à¸‡:**
- ğŸ”¹ **Small Files (<100 MB):** à¸­à¹ˆà¸²à¸™à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹„à¸”à¹‰à¹€à¸¥à¸¢
- ğŸ”¹ **Medium Files (100 MB - 1 GB):** Chunk size = 100,000 - 500,000 rows
- ğŸ”¹ **Large Files (>1 GB):** Chunk size = 50,000 - 100,000 rows

#### 6.4.2 Parallel Processing à¸à¸±à¸š Chunking

```python
from multiprocessing import Pool
import pandas as pd

def process_chunk(chunk_data):
    """
    Function à¸ªà¸³à¸«à¸£à¸±à¸šà¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥ 1 chunk
    """
    chunk = chunk_data['chunk']
    chunk_id = chunk_data['chunk_id']

    # à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥
    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])
    chunk['temp_fahrenheit'] = (chunk['temperature'] * 9/5) + 32

    return chunk_id, len(chunk)

# à¸­à¹ˆà¸²à¸™à¹à¸¥à¸°à¹€à¸•à¸£à¸µà¸¢à¸¡ chunks
chunks = []
for i, chunk in enumerate(pd.read_csv('sensor_large.csv', chunksize=100_000)):
    chunks.append({'chunk': chunk, 'chunk_id': i})

# Process à¹à¸šà¸š Parallel
with Pool(processes=4) as pool:
    results = pool.map(process_chunk, chunks)

print(f"Processed {len(results)} chunks in parallel")
```

### 6.5 Chunking vs Full Load Comparison

```python
import time
import pandas as pd

# Test File
file_path = 'sensor_large.csv'

# Method 1: Full Load (à¸–à¹‰à¸²à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸à¹ˆà¸ˆà¸° Memory Error)
print("=== Full Load ===")
start = time.time()
try:
    df = pd.read_csv(file_path)
    memory_usage = df.memory_usage(deep=True).sum() / (1024 * 1024)
    time_full = time.time() - start
    print(f"Time: {time_full:.2f} sec")
    print(f"Memory: {memory_usage:.2f} MB")
    print(f"Rows: {len(df):,}")
except MemoryError:
    print("MemoryError: File too large to load fully")

# Method 2: Chunking
print("\n=== Chunking ===")
start = time.time()
chunk_size = 100_000
total_rows = 0

for chunk in pd.read_csv(file_path, chunksize=chunk_size):
    total_rows += len(chunk)
    # Process chunk here

time_chunk = time.time() - start
print(f"Time: {time_chunk:.2f} sec")
print(f"Memory: ~{chunk_size * 0.01:.2f} MB (per chunk)")
print(f"Rows: {total_rows:,}")
```

---

## 7. Vectorized Operations

### 7.1 Vectorization à¸„à¸·à¸­à¸­à¸°à¹„à¸£?

**Vectorization** = à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸—à¸±à¹‰à¸‡ Array/Series à¹ƒà¸™à¸„à¸³à¸ªà¸±à¹ˆà¸‡à¹€à¸”à¸µà¸¢à¸§ à¹à¸—à¸™à¸—à¸µà¹ˆà¸ˆà¸°à¹ƒà¸Šà¹‰ Loop

**à¸„à¸§à¸²à¸¡à¹à¸•à¸à¸•à¹ˆà¸²à¸‡:**

```python
# âŒ Non-Vectorized (Loop)
result = []
for i in range(len(data)):
    result.append(data[i] * 2)

# âœ… Vectorized
result = data * 2
```

### 7.2 à¸—à¸³à¹„à¸¡ Vectorization à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸²?

```
Python Loop:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ For i  â”‚ â†’ Check â†’ Calculate â†’ Append â†’ Repeat (Slow)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (Python Overhead for each iteration)

Vectorized Operation:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ NumPy/Pandas   â”‚ â†’ Calculate ALL at once (Fast)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   (C/Fortran Implementation)

Speedup: 10-100x faster
```

### 7.3 Vectorization Examples

#### 7.3.1 Basic Math Operations

```python
import numpy as np
import time

n = 1_000_000
data = np.random.uniform(0, 100, n)

# âŒ Loop (Slow)
start = time.time()
result_loop = []
for x in data:
    result_loop.append(x * 2 + 10)
time_loop = time.time() - start

# âœ… Vectorized (Fast)
start = time.time()
result_vectorized = data * 2 + 10
time_vectorized = time.time() - start

print(f"Loop:       {time_loop:.4f} sec")
print(f"Vectorized: {time_vectorized:.4f} sec")
print(f"Speedup:    {time_loop / time_vectorized:.1f}x")
```

#### 7.3.2 Conditional Operations

```python
# âŒ Loop
result_loop = []
for temp in temperatures:
    if temp > 25:
        result_loop.append('Hot')
    else:
        result_loop.append('Cool')

# âœ… Vectorized
result_vectorized = np.where(temperatures > 25, 'Hot', 'Cool')

# âœ… Pandas Vectorized
df['temp_label'] = df['temperature'].apply(lambda x: 'Hot' if x > 25 else 'Cool')  # Slower
df['temp_label'] = np.where(df['temperature'] > 25, 'Hot', 'Cool')  # Faster
```

#### 7.3.3 Multiple Conditions

```python
# Categorize temperature
# Cold: < 20, Cool: 20-25, Normal: 25-28, Hot: > 28

# âŒ Loop
categories = []
for temp in temperatures:
    if temp < 20:
        categories.append('Cold')
    elif temp < 25:
        categories.append('Cool')
    elif temp < 28:
        categories.append('Normal')
    else:
        categories.append('Hot')

# âœ… Vectorized (np.select)
conditions = [
    temperatures < 20,
    (temperatures >= 20) & (temperatures < 25),
    (temperatures >= 25) & (temperatures < 28),
    temperatures >= 28
]
choices = ['Cold', 'Cool', 'Normal', 'Hot']
categories_vectorized = np.select(conditions, choices)

# âœ… Pandas cut (à¸ªà¸³à¸«à¸£à¸±à¸š binning)
df['temp_category'] = pd.cut(
    df['temperature'],
    bins=[-np.inf, 20, 25, 28, np.inf],
    labels=['Cold', 'Cool', 'Normal', 'Hot']
)
```

### 7.4 Vectorization Best Practices

**âœ… DO:**
```python
# à¹ƒà¸Šà¹‰ NumPy/Pandas operations
result = df['temperature'] * 1.8 + 32

# à¹ƒà¸Šà¹‰ np.where à¸ªà¸³à¸«à¸£à¸±à¸š conditions
df['status'] = np.where(df['temperature'] > 30, 'Alert', 'Normal')

# à¹ƒà¸Šà¹‰ Built-in functions
df['temp_mean'] = df['temperature'].mean()
```

**âŒ DON'T:**
```python
# à¸­à¸¢à¹ˆà¸²à¹ƒà¸Šà¹‰ Loop
for i in range(len(df)):
    df.loc[i, 'fahrenheit'] = df.loc[i, 'temperature'] * 1.8 + 32

# à¸­à¸¢à¹ˆà¸²à¹ƒà¸Šà¹‰ apply à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µ Vectorized alternative
df['fahrenheit'] = df['temperature'].apply(lambda x: x * 1.8 + 32)  # Slow
```

### 7.5 Performance Comparison Summary

```python
import pandas as pd
import numpy as np
import time

# Prepare Data
n = 500_000
df = pd.DataFrame({
    'temperature': np.random.uniform(20, 35, n)
})

print(f"Data Size: {n:,} records\n")

# Method 1: Loop with loc (à¸Šà¹‰à¸²à¸¡à¸²à¸)
start = time.time()
df_loop = df.copy()
for i in range(len(df_loop)):
    df_loop.loc[i, 'fahrenheit'] = df_loop.loc[i, 'temperature'] * 1.8 + 32
time_loop = time.time() - start

# Method 2: Apply (à¸Šà¹‰à¸²)
start = time.time()
df_apply = df.copy()
df_apply['fahrenheit'] = df_apply['temperature'].apply(lambda x: x * 1.8 + 32)
time_apply = time.time() - start

# Method 3: Vectorized (à¹€à¸£à¹‡à¸§)
start = time.time()
df_vectorized = df.copy()
df_vectorized['fahrenheit'] = df_vectorized['temperature'] * 1.8 + 32
time_vectorized = time.time() - start

print("=== Performance Comparison ===")
print(f"Loop + loc:  {time_loop:.4f} sec (Baseline)")
print(f"Apply:       {time_apply:.4f} sec ({time_loop / time_apply:.1f}x faster than loop)")
print(f"Vectorized:  {time_vectorized:.4f} sec ({time_loop / time_vectorized:.1f}x faster than loop)")
```

**Expected Output:**
```
=== Performance Comparison ===
Loop + loc:  45.2341 sec (Baseline)
Apply:       0.8234 sec (54.9x faster than loop)
Vectorized:  0.0156 sec (2900.3x faster than loop)
```

---

## 8. à¸ªà¸£à¸¸à¸› Module 3

### 8.1 Key Takeaways

âœ… **Data Transformation** à¹€à¸›à¹‡à¸™à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸ªà¸³à¸„à¸±à¸à¹ƒà¸™ ETL Pipeline
âœ… **Feature Engineering** à¸ªà¸£à¹‰à¸²à¸‡ Features à¹ƒà¸«à¸¡à¹ˆà¸ˆà¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸”à¸´à¸¡à¹€à¸à¸·à¹ˆà¸­à¹€à¸à¸´à¹ˆà¸¡à¸„à¸¸à¸“à¸„à¹ˆà¸²
âœ… **Rolling Window** à¹ƒà¸Šà¹‰à¸ªà¸³à¸«à¸£à¸±à¸š Smoothing, Trend Detection, Anomaly Detection
âœ… **Performance Optimization** à¸¥à¸”à¹€à¸§à¸¥à¸²à¹à¸¥à¸° Memory à¸”à¹‰à¸§à¸¢ dtype, vectorization, chunking
âœ… **NumPy à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² Pandas** à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸šà¸£à¸´à¸ªà¸¸à¸—à¸˜à¸´à¹Œ (~2x)
âœ… **Chunking** à¸£à¸­à¸‡à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆà¸—à¸µà¹ˆà¹€à¸à¸´à¸™ RAM
âœ… **Vectorization à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² Loop** à¸¡à¸²à¸ (10-1000x)

### 8.2 Skills à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸ˆà¸²à¸ Module à¸™à¸µà¹‰

| Skill | Level |
|-------|-------|
| Feature Engineering | â­â­â­ |
| Rolling Window Operations | â­â­â­ |
| Performance Benchmarking | â­â­â­ |
| Pandas vs NumPy | â­â­â­ |
| Chunking Strategy | â­â­ |
| Vectorization | â­â­â­ |
| Memory Optimization | â­â­ |

### 8.3 Performance Optimization Checklist

à¹€à¸¡à¸·à¹ˆà¸­à¸—à¸³à¸‡à¸²à¸™à¸à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸š:

- [ ] à¹ƒà¸Šà¹‰ dtype à¸—à¸µà¹ˆà¹€à¸«à¸¡à¸²à¸°à¸ªà¸¡ (float32, int8, category)
- [ ] à¹ƒà¸Šà¹‰ Vectorized Operations à¹à¸—à¸™ Loop
- [ ] à¹ƒà¸Šà¹‰ NumPy à¸ªà¸³à¸«à¸£à¸±à¸šà¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“à¸«à¸™à¸±à¸
- [ ] à¹ƒà¸Šà¹‰ Chunking à¸ªà¸³à¸«à¸£à¸±à¸šà¹„à¸Ÿà¸¥à¹Œà¸‚à¸™à¸²à¸”à¹ƒà¸«à¸à¹ˆ
- [ ] à¸«à¸¥à¸µà¸à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ apply() à¹€à¸¡à¸·à¹ˆà¸­à¸¡à¸µ Vectorized alternative
- [ ] Monitor Memory Usage à¸”à¹‰à¸§à¸¢ `df.memory_usage()`

### 8.4 à¹€à¸•à¸£à¸µà¸¢à¸¡à¸à¸£à¹‰à¸­à¸¡à¸ªà¸³à¸«à¸£à¸±à¸š Module 4

à¹ƒà¸™ Module à¸–à¸±à¸”à¹„à¸› à¸„à¸¸à¸“à¸ˆà¸°à¹„à¸”à¹‰à¹€à¸£à¸µà¸¢à¸™à¸£à¸¹à¹‰:
- Cloud Storage Concepts (Data Lake)
- à¸à¸²à¸£à¹ƒà¸Šà¹‰ Cloud SDKs
- Data Partitioning Strategy à¸ªà¸³à¸«à¸£à¸±à¸š Time-Series

---

## ğŸ“ Challenge Questions

à¸à¹ˆà¸­à¸™à¹„à¸› Module 4 à¸¥à¸­à¸‡à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹€à¸«à¸¥à¹ˆà¸²à¸™à¸µà¹‰:

1. **Rolling Window:** à¸–à¹‰à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸¡à¸²à¸—à¸¸à¸ 5 à¸™à¸²à¸—à¸µ à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ Moving Average 1 à¸Šà¸±à¹ˆà¸§à¹‚à¸¡à¸‡ à¸„à¸§à¸£à¹ƒà¸Šà¹‰ window à¹€à¸—à¹ˆà¸²à¹„à¸£?
2. **Performance:** à¸—à¸³à¹„à¸¡ Vectorized Operations à¹€à¸£à¹‡à¸§à¸à¸§à¹ˆà¸² Python Loop?
3. **Chunking:** à¸–à¹‰à¸²à¹„à¸Ÿà¸¥à¹Œ CSV 5 GB à¹à¸•à¹ˆà¸¡à¸µ RAM 8 GB à¸„à¸§à¸£à¹ƒà¸Šà¹‰ Chunk Size à¹€à¸—à¹ˆà¸²à¹„à¸£?
4. **NumPy vs Pandas:** à¸–à¹‰à¸²à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸„à¸³à¸™à¸§à¸“ Mean à¸‚à¸­à¸‡ 10 à¸¥à¹‰à¸²à¸™ records à¸„à¸§à¸£à¹ƒà¸Šà¹‰à¸­à¸°à¹„à¸£?

---

## ğŸ¯ à¸–à¸±à¸”à¹„à¸›: Labs & Practical Exercises

à¸à¸£à¹‰à¸­à¸¡à¸—à¸³ Lab à¹à¸¥à¹‰à¸§à¸«à¸£à¸·à¸­à¸¢à¸±à¸‡?

**ğŸ‘‰ [à¹€à¸£à¸´à¹ˆà¸¡à¸—à¸³ Lab Module 3](./labs/lab-module-3.md)**

à¹ƒà¸™ Lab à¸„à¸¸à¸“à¸ˆà¸°à¹„à¸”à¹‰:
- Lab 3.1: Implement Rolling Average (5min window)
- Lab 3.2: Benchmark NumPy vs Pandas subtraction
- Lab 3.3: Chunking large CSV files
- Lab 3.4: Vectorization performance comparison

---

**[â¬…ï¸ à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆ Module 2](../module-2/module-2.md)** | **[Module 4: Cloud Integration & Storage â¡ï¸](../module-4/module-4.md)** | **[ğŸ  à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸µà¹ˆ Wiki à¸«à¸¥à¸±à¸](../wiki.md)**

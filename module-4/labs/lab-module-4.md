# Lab Module 4: Cloud Integration & Storage

**üéØ ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå:**
- ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Data Partitioning Strategy ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Time-Series Data
- ‡∏™‡∏£‡πâ‡∏≤‡∏á File Naming Convention ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô
- ‡∏ù‡∏∂‡∏Å‡∏à‡∏±‡∏î‡∏£‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ö Folder Structure ‡πÅ‡∏ö‡∏ö Data Lake
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô Python Script ‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏ö‡∏ö Cloud Storage

**‚è±Ô∏è ‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ:** 1.5 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á

---

## üìã Pre-requisites

‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏£‡∏¥‡πà‡∏° Lab ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ:
- ‚úÖ Python 3.8+ ‡∏û‡∏£‡πâ‡∏≠‡∏° Virtual Environment
- ‚úÖ Pandas ‡πÅ‡∏•‡∏∞ NumPy ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡πâ‡∏ß
- ‚úÖ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Module 4 ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á Partitioning ‡πÅ‡∏•‡∏∞ Data Lake

---

## Lab 4.1: Define Time-Series Partitioning Strategy (30 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Partitioning Strategy ‡πÅ‡∏•‡∏∞ File Naming Convention ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• IoT Sensor

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå Requirements

‡∏™‡∏°‡∏°‡∏ï‡∏¥‡πÄ‡∏£‡∏≤‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏ö IoT ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏°‡∏ö‡∏±‡∏ï‡∏¥‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:
- ‡∏°‡∏µ Sensors ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 100 ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á (S001 - S100)
- ‡πÅ‡∏ï‡πà‡∏•‡∏∞ Sensor ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ
- ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô JSON Format
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° Date Range ‡∏ö‡πà‡∏≠‡∏¢‡πÜ
- ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 90 ‡∏ß‡∏±‡∏ô‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢

**‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:** Partitioning Strategy ‡πÉ‡∏î‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î?

<details>
<summary>üí° ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏• (‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π)</summary>

**Strategy ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°: Time-Based Partitioning (year/month/day)**

**‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•:**
1. ‚úÖ Query Pattern: ‡∏ï‡πâ‡∏≠‡∏á Query ‡∏ï‡∏≤‡∏° Date Range ‡∏ö‡πà‡∏≠‡∏¢
2. ‚úÖ Data Management: ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢ (‡∏•‡∏ö‡∏ó‡∏±‡πâ‡∏á Partition)
3. ‚úÖ Data Volume: 100 sensors √ó 12 records/hour √ó 24 hours = 28,800 records/day ‡∏ï‡πà‡∏≠ Partition ‡∏û‡∏≠‡∏î‡∏µ
4. ‚úÖ Scalability: ‡πÄ‡∏û‡∏¥‡πà‡∏° Sensor ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡∏Å‡∏£‡∏∞‡∏ó‡∏ö Partition Structure

**‡πÑ‡∏°‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Sensor-Based Partitioning ‡πÄ‡∏û‡∏£‡∏≤‡∏∞:**
- ‚ùå ‡∏ï‡πâ‡∏≠‡∏á Scan ‡∏´‡∏•‡∏≤‡∏¢ Partitions ‡πÄ‡∏ß‡∏•‡∏≤ Query ‡∏ï‡∏≤‡∏° Date
- ‚ùå ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏¢‡∏≤‡∏Å (‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡∏•‡∏ö‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Sensor Partition)
</details>

#### Step 2: ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Folder Structure

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `partitioning_design.md` ‡πÅ‡∏•‡∏∞‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Structure:

```markdown
# IoT Data Lake - Partitioning Design

## Folder Structure

```
s3://iot-data-lake/
‚îÇ
‚îú‚îÄ raw/
‚îÇ   ‚îî‚îÄ sensors/
‚îÇ       ‚îî‚îÄ year=2025/
‚îÇ           ‚îî‚îÄ month=01/
‚îÇ               ‚îî‚îÄ day=01/
‚îÇ                   ‚îú‚îÄ sensor_S001_20250101_000000.json
‚îÇ                   ‚îú‚îÄ sensor_S001_20250101_000500.json
‚îÇ                   ‚îú‚îÄ sensor_S002_20250101_000000.json
‚îÇ                   ‚îî‚îÄ ...
‚îÇ
‚îú‚îÄ processed/
‚îÇ   ‚îî‚îÄ sensors/
‚îÇ       ‚îî‚îÄ year=2025/
‚îÇ           ‚îî‚îÄ month=01/
‚îÇ               ‚îî‚îÄ day=01/
‚îÇ                   ‚îî‚îÄ sensors_20250101.parquet
‚îÇ
‚îî‚îÄ curated/
    ‚îî‚îÄ daily_summary/
        ‚îî‚îÄ year=2025/
            ‚îî‚îÄ month=01/
                ‚îî‚îÄ summary_20250101.parquet
```

## File Naming Convention

**Format:** `{source}_{entity}_{timestamp}.{extension}`

**Examples:**
- RAW: `sensor_S001_20250101_120530.json`
- PROCESSED: `sensors_20250101.parquet`
- CURATED: `summary_20250101.parquet`

**Components:**
- source: `sensor`, `processed`, `summary`
- entity: `S001`, `S002`, `all`
- timestamp: `YYYYMMDD_HHMMSS` or `YYYYMMDD`
- extension: `.json`, `.parquet`, `.csv`

## Query Examples

### Query 1: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 2025-01-01
```
Path: s3://iot-data-lake/raw/sensors/year=2025/month=01/day=01/
```

### Query 2: ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Sensor S001 ‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô January 2025
```
Path: s3://iot-data-lake/raw/sensors/year=2025/month=01/day=*/
Filter: sensor_S001_*.json
```

### Query 3: ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ 90 ‡∏ß‡∏±‡∏ô
```
Delete Partitions: year=2024/month=10/day=01 and older
```
```

#### Step 3: ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Data Volume

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `calculate_partition_size.py`:

```python
"""
‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô Partition ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ß‡∏±‡∏ô
"""

# Configuration
NUM_SENSORS = 100
RECORDS_PER_HOUR_PER_SENSOR = 12  # ‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ = 12 records/hour
HOURS_PER_DAY = 24
AVG_RECORD_SIZE_BYTES = 500  # JSON record ~500 bytes

# Calculate
records_per_day_per_sensor = RECORDS_PER_HOUR_PER_SENSOR * HOURS_PER_DAY
total_records_per_day = records_per_day_per_sensor * NUM_SENSORS

# Storage size
total_size_bytes = total_records_per_day * AVG_RECORD_SIZE_BYTES
total_size_mb = total_size_bytes / (1024 * 1024)
total_size_gb = total_size_mb / 1024

print("=== IoT Data Volume Calculation ===\n")
print(f"Sensors: {NUM_SENSORS}")
print(f"Records per hour per sensor: {RECORDS_PER_HOUR_PER_SENSOR}")
print(f"Records per day per sensor: {records_per_day_per_sensor}")
print(f"\n--- Daily Partition Size ---")
print(f"Total records per day: {total_records_per_day:,}")
print(f"Total size: {total_size_mb:.2f} MB ({total_size_gb:.4f} GB)")

# Monthly and Yearly estimates
monthly_size_gb = total_size_gb * 30
yearly_size_gb = total_size_gb * 365

print(f"\n--- Estimates ---")
print(f"Monthly storage: {monthly_size_gb:.2f} GB")
print(f"Yearly storage: {yearly_size_gb:.2f} GB")

# Cost estimation (example: S3 Standard ~$0.023/GB/month)
s3_cost_per_gb = 0.023
monthly_cost = monthly_size_gb * s3_cost_per_gb * 30  # 30 days retention
print(f"\n--- Estimated S3 Cost (Standard, 30-day retention) ---")
print(f"Monthly cost: ${monthly_cost:.2f}")
```

‡∏£‡∏±‡∏ô Script:

```bash
python calculate_partition_size.py
```

**Expected Output:**
```
=== IoT Data Volume Calculation ===

Sensors: 100
Records per hour per sensor: 12
Records per day per sensor: 288

--- Daily Partition Size ---
Total records per day: 28,800
Total size: 13.73 MB (0.0134 GB)

--- Estimates ---
Monthly storage: 0.40 GB
Yearly storage: 4.89 GB

--- Estimated S3 Cost (Standard, 30-day retention) ---
Monthly cost: $0.28
```

### ‚úÖ Checkpoint 4.1

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Partitioning Strategy ‡πÑ‡∏î‡πâ (year/month/day)
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á File Naming Convention ‡πÅ‡∏•‡πâ‡∏ß
- [ ] ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Data Volume ‡πÅ‡∏•‡∏∞ Storage Cost ‡πÑ‡∏î‡πâ
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à Trade-offs ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Strategy

---

## Lab 4.2: Implement Local Data Lake Simulator (40 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏™‡∏£‡πâ‡∏≤‡∏á Python Script ‡∏à‡∏≥‡∏•‡∏≠‡∏á Data Lake Structure ‡πÅ‡∏ö‡∏ö Local (‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ Cloud Account)

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: ‡∏™‡∏£‡πâ‡∏≤‡∏á Folder Structure Generator

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `create_datalake_structure.py`:

```python
"""
‡∏™‡∏£‡πâ‡∏≤‡∏á Data Lake Folder Structure ‡πÅ‡∏ö‡∏ö Local
"""
import os
from pathlib import Path
from datetime import datetime

def create_datalake_structure(base_path='./local_datalake'):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Folder Structure ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Data Lake
    """
    # Define structure
    zones = {
        'raw': ['sensors', 'logs', 'events'],
        'processed': ['sensors_cleaned', 'aggregated'],
        'curated': ['daily_summary', 'sensor_stats', 'ml_features']
    }

    print(f"Creating Data Lake structure at: {base_path}\n")

    for zone, folders in zones.items():
        for folder in folders:
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á Partitions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 3 ‡∏ß‡∏±‡∏ô‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
            for day in range(1, 4):
                partition_path = Path(base_path) / zone / folder / \
                                 f'year=2025/month=01/day={day:02d}'
                partition_path.mkdir(parents=True, exist_ok=True)
                print(f"‚úÖ Created: {partition_path}")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á metadata folder
    metadata_path = Path(base_path) / '_metadata'
    metadata_path.mkdir(parents=True, exist_ok=True)
    print(f"‚úÖ Created: {metadata_path}")

    print("\n‚úÖ Data Lake structure created successfully!")

if __name__ == '__main__':
    create_datalake_structure()
```

‡∏£‡∏±‡∏ô Script:

```bash
python create_datalake_structure.py
```

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Folder Structure ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô:

```bash
tree local_datalake -L 4
# ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏ô Windows: dir local_datalake /s
```

**Expected Output:**
```
local_datalake/
‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ sensors/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ year=2025/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ month=01/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ day=01/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ day=02/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ day=03/
‚îÇ   ‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ events/
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ sensors_cleaned/
‚îÇ   ‚îî‚îÄ‚îÄ aggregated/
‚îú‚îÄ‚îÄ curated/
‚îÇ   ‚îú‚îÄ‚îÄ daily_summary/
‚îÇ   ‚îú‚îÄ‚îÄ sensor_stats/
‚îÇ   ‚îî‚îÄ‚îÄ ml_features/
‚îî‚îÄ‚îÄ _metadata/
```

#### Step 2: Generate Sample Sensor Data

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `generate_sensor_data.py`:

```python
"""
Generate Sample IoT Sensor Data with Partitioning
"""
import json
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta

def generate_sensor_record(sensor_id, timestamp):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á Sensor Record 1 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
    """
    return {
        'timestamp': timestamp.isoformat(),
        'sensor_id': sensor_id,
        'temperature': round(25 + np.random.normal(0, 2), 2),
        'humidity': round(60 + np.random.normal(0, 5), 2),
        'pressure': round(1013 + np.random.normal(0, 2), 2),
        'battery_level': round(100 - np.random.uniform(0, 5), 2)
    }

def generate_partitioned_data(base_path='./local_datalake',
                               num_sensors=5,
                               num_days=3):
    """
    Generate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Sensor ‡πÅ‡∏ö‡∏ö Partitioned
    """
    print(f"Generating sensor data for {num_sensors} sensors, {num_days} days\n")

    # Start date
    start_date = datetime(2025, 1, 1, 0, 0, 0)

    for day in range(num_days):
        current_date = start_date + timedelta(days=day)
        partition_path = Path(base_path) / 'raw' / 'sensors' / \
                         f'year={current_date.year}' / \
                         f'month={current_date.month:02d}' / \
                         f'day={current_date.day:02d}'

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á records ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÅ‡∏£‡∏Å (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÄ‡∏ß‡∏•‡∏≤)
        for sensor_id in range(1, num_sensors + 1):
            sensor_name = f'S{sensor_id:03d}'
            records = []

            # Generate 12 records (‡∏ó‡∏∏‡∏Å 5 ‡∏ô‡∏≤‡∏ó‡∏µ ‡πÉ‡∏ô 1 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á)
            for minute in range(0, 60, 5):
                timestamp = current_date + timedelta(minutes=minute)
                record = generate_sensor_record(sensor_name, timestamp)
                records.append(record)

            # Save to JSON file
            filename = f'sensor_{sensor_name}_{current_date.strftime("%Y%m%d")}_000000.json'
            file_path = partition_path / filename

            with open(file_path, 'w') as f:
                json.dump(records, f, indent=2)

            print(f"‚úÖ Created: {file_path.relative_to(base_path)} ({len(records)} records)")

    print(f"\n‚úÖ Data generation complete!")

def list_partition_contents(base_path='./local_datalake'):
    """
    ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏ô Partitions
    """
    print("\n=== Partition Contents ===\n")

    raw_path = Path(base_path) / 'raw' / 'sensors'

    for partition_dir in sorted(raw_path.rglob('day=*')):
        files = list(partition_dir.glob('*.json'))
        print(f"üìÅ {partition_dir.relative_to(base_path)}/")
        print(f"   Files: {len(files)}")

        if files:
            # Show file sizes
            total_size = sum(f.stat().st_size for f in files)
            print(f"   Size: {total_size / 1024:.2f} KB")
        print()

if __name__ == '__main__':
    # Generate data
    generate_partitioned_data(num_sensors=5, num_days=3)

    # List contents
    list_partition_contents()
```

‡∏£‡∏±‡∏ô Script:

```bash
python generate_sensor_data.py
```

#### Step 3: Query Simulator

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `query_partitioned_data.py`:

```python
"""
‡∏à‡∏≥‡∏•‡∏≠‡∏á Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Partitioned Data Lake
"""
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

def query_by_date(base_path='./local_datalake', year=2025, month=1, day=1):
    """
    Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° Date Partition
    """
    partition_path = Path(base_path) / 'raw' / 'sensors' / \
                     f'year={year}' / f'month={month:02d}' / f'day={day:02d}'

    print(f"=== Query: Date {year}-{month:02d}-{day:02d} ===\n")
    print(f"Partition: {partition_path}\n")

    if not partition_path.exists():
        print("‚ùå Partition not found!")
        return None

    # List files
    json_files = list(partition_path.glob('*.json'))
    print(f"Found {len(json_files)} files\n")

    # Load all data
    all_records = []
    for json_file in json_files:
        with open(json_file, 'r') as f:
            records = json.load(f)
            all_records.extend(records)

    # Convert to DataFrame
    df = pd.DataFrame(all_records)
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    print("--- Sample Data ---")
    print(df.head())

    print("\n--- Statistics ---")
    print(f"Total records: {len(df)}")
    print(f"Unique sensors: {df['sensor_id'].nunique()}")
    print(f"Avg temperature: {df['temperature'].mean():.2f}¬∞C")
    print(f"Avg humidity: {df['humidity'].mean():.2f}%")

    return df

def query_by_sensor(base_path='./local_datalake', sensor_id='S001',
                    year=2025, month=1):
    """
    Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Sensor ‡πÉ‡∏î‡πÜ ‡πÉ‡∏ô‡πÄ‡∏î‡∏∑‡∏≠‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
    """
    print(f"=== Query: Sensor {sensor_id} in {year}-{month:02d} ===\n")

    sensors_path = Path(base_path) / 'raw' / 'sensors' / \
                   f'year={year}' / f'month={month:02d}'

    if not sensors_path.exists():
        print("‚ùå Month partition not found!")
        return None

    # Scan all days in the month
    all_records = []
    for day_dir in sorted(sensors_path.glob('day=*')):
        # Find files for the specific sensor
        sensor_files = list(day_dir.glob(f'sensor_{sensor_id}_*.json'))

        for sensor_file in sensor_files:
            with open(sensor_file, 'r') as f:
                records = json.load(f)
                all_records.extend(records)

    if not all_records:
        print(f"‚ùå No data found for sensor {sensor_id}")
        return None

    df = pd.DataFrame(all_records)
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    print("--- Sample Data ---")
    print(df.head())

    print("\n--- Statistics ---")
    print(f"Total records: {len(df)}")
    print(f"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
    print(f"Avg temperature: {df['temperature'].mean():.2f}¬∞C")

    return df

def compare_partition_scan():
    """
    ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏≤‡∏£ Scan ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: With Partitioning vs Without
    """
    print("=== Partitioning Benefits ===\n")

    base_path = Path('./local_datalake/raw/sensors')

    # Count total files
    all_files = list(base_path.rglob('*.json'))
    print(f"Total files in Data Lake: {len(all_files)}")

    # Simulate query for one day
    day1_path = base_path / 'year=2025/month=01/day=01'
    day1_files = list(day1_path.glob('*.json'))

    print(f"\n--- Query: Only 2025-01-01 ---")
    print(f"Files scanned WITH partitioning: {len(day1_files)}")
    print(f"Files scanned WITHOUT partitioning: {len(all_files)}")

    savings = (1 - len(day1_files) / len(all_files)) * 100
    print(f"\n‚úÖ Partition Scan Reduction: {savings:.1f}%")

if __name__ == '__main__':
    # Query 1: By Date
    print("\n" + "="*60)
    df1 = query_by_date(year=2025, month=1, day=1)

    print("\n" + "="*60)
    # Query 2: By Sensor
    df2 = query_by_sensor(sensor_id='S001', year=2025, month=1)

    print("\n" + "="*60)
    # Query 3: Compare scan efficiency
    compare_partition_scan()
```

‡∏£‡∏±‡∏ô Script:

```bash
python query_partitioned_data.py
```

### ‚úÖ Checkpoint 4.2

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á Local Data Lake Structure ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] Generate Partitioned Sensor Data ‡πÑ‡∏î‡πâ
- [ ] Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° Date Partition ‡πÑ‡∏î‡πâ
- [ ] Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° Sensor ID ‡πÑ‡∏î‡πâ
- [ ] ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á Partitioning

---

## Lab 4.3: File Naming & Validation (20 ‡∏ô‡∏≤‡∏ó‡∏µ)

### ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå
‡∏™‡∏£‡πâ‡∏≤‡∏á Utility Functions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö File Naming ‡πÅ‡∏•‡∏∞ Validation

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

#### Step 1: File Naming Helper

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `file_naming_utils.py`:

```python
"""
Utility functions ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö File Naming Convention
"""
from datetime import datetime
from pathlib import Path
import re

class DataLakeFileNaming:
    """
    Helper class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö File Naming Convention
    """

    @staticmethod
    def generate_filename(source, entity, timestamp, extension, version=None):
        """
        Generate filename ‡∏ï‡∏≤‡∏° Convention

        Args:
            source: ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (sensor, api, manual)
            entity: ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (S001, daily_summary)
            timestamp: datetime object ‡∏´‡∏£‡∏∑‡∏≠ string
            extension: .json, .parquet, .csv
            version: v1, v2 (optional)

        Returns:
            Filename string
        """
        if isinstance(timestamp, datetime):
            ts_str = timestamp.strftime('%Y%m%d_%H%M%S')
        else:
            ts_str = timestamp

        parts = [source, entity, ts_str]

        if version:
            parts.append(version)

        filename = '_'.join(parts) + extension

        return filename

    @staticmethod
    def parse_filename(filename):
        """
        Parse filename ‡πÅ‡∏•‡∏∞ Extract components
        """
        # Remove extension
        name_without_ext = Path(filename).stem

        # Split by underscore
        parts = name_without_ext.split('_')

        if len(parts) < 3:
            raise ValueError(f"Invalid filename format: {filename}")

        result = {
            'source': parts[0],
            'entity': parts[1],
            'timestamp': parts[2] if len(parts) > 2 else None,
            'version': parts[3] if len(parts) > 3 else None,
            'extension': Path(filename).suffix
        }

        return result

    @staticmethod
    def validate_filename(filename):
        """
        Validate ‡∏ß‡πà‡∏≤ Filename ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° Convention ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        """
        try:
            parsed = DataLakeFileNaming.parse_filename(filename)

            # Check timestamp format (YYYYMMDD_HHMMSS or YYYYMMDD)
            ts = parsed['timestamp']
            ts_pattern = r'^\d{8}(_\d{6})?$'

            if not re.match(ts_pattern, ts):
                return False, "Invalid timestamp format"

            # Check extension
            valid_extensions = ['.json', '.parquet', '.csv', '.txt']
            if parsed['extension'] not in valid_extensions:
                return False, f"Invalid extension: {parsed['extension']}"

            return True, "Valid filename"

        except Exception as e:
            return False, str(e)

# Test functions
if __name__ == '__main__':
    naming = DataLakeFileNaming()

    print("=== File Naming Convention Tests ===\n")

    # Test 1: Generate filename
    print("--- Test 1: Generate Filename ---")
    filename1 = naming.generate_filename(
        source='sensor',
        entity='S001',
        timestamp=datetime(2025, 1, 1, 12, 30, 45),
        extension='.json'
    )
    print(f"Generated: {filename1}")

    filename2 = naming.generate_filename(
        source='processed',
        entity='daily_summary',
        timestamp='20250101',
        extension='.parquet',
        version='v2'
    )
    print(f"Generated: {filename2}")

    # Test 2: Parse filename
    print("\n--- Test 2: Parse Filename ---")
    parsed = naming.parse_filename(filename1)
    print(f"Parsed: {parsed}")

    # Test 3: Validate filename
    print("\n--- Test 3: Validate Filename ---")

    test_files = [
        'sensor_S001_20250101_120000.json',          # ‚úÖ Valid
        'processed_daily_20250101_v2.parquet',       # ‚úÖ Valid
        'sensor_S001_20250101.csv',                  # ‚úÖ Valid
        'data.json',                                 # ‚ùå Invalid
        'sensor_S001_2025-01-01.json',               # ‚ùå Invalid timestamp
        'sensor_S001_20250101_120000.txt',           # ‚úÖ Valid
    ]

    for filename in test_files:
        valid, message = naming.validate_filename(filename)
        status = "‚úÖ" if valid else "‚ùå"
        print(f"{status} {filename:45s} -> {message}")
```

‡∏£‡∏±‡∏ô Script:

```bash
python file_naming_utils.py
```

#### Step 2: Data Validation

‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå `data_validation.py`:

```python
"""
Data Validation ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô Upload ‡πÑ‡∏õ Data Lake
"""
import pandas as pd
import json
from pathlib import Path

class DataLakeValidator:
    """
    Validate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô Upload ‡πÑ‡∏õ Data Lake
    """

    @staticmethod
    def validate_sensor_data(df):
        """
        Validate Sensor DataFrame
        """
        print("=== Data Validation ===\n")

        checks = {}

        # Check 1: Required columns
        required_cols = ['timestamp', 'sensor_id', 'temperature']
        checks['required_columns'] = all(col in df.columns for col in required_cols)

        if not checks['required_columns']:
            missing = [col for col in required_cols if col not in df.columns]
            print(f"‚ùå Missing columns: {missing}")
        else:
            print(f"‚úÖ Required columns present")

        # Check 2: No duplicates
        checks['no_duplicates'] = not df.duplicated(subset=['timestamp', 'sensor_id']).any()
        dup_count = df.duplicated(subset=['timestamp', 'sensor_id']).sum()

        if checks['no_duplicates']:
            print(f"‚úÖ No duplicate records")
        else:
            print(f"‚ùå Found {dup_count} duplicate records")

        # Check 3: No nulls in critical columns
        critical_nulls = df[['timestamp', 'sensor_id']].isnull().sum().sum()
        checks['no_critical_nulls'] = critical_nulls == 0

        if checks['no_critical_nulls']:
            print(f"‚úÖ No nulls in critical columns")
        else:
            print(f"‚ùå Found {critical_nulls} nulls in critical columns")

        # Check 4: Valid temperature range
        if 'temperature' in df.columns:
            valid_temp = df['temperature'].between(-50, 100).all()
            checks['valid_temperature_range'] = valid_temp

            if valid_temp:
                print(f"‚úÖ Temperature values in valid range (-50 to 100¬∞C)")
            else:
                invalid_count = (~df['temperature'].between(-50, 100)).sum()
                print(f"‚ùå Found {invalid_count} invalid temperature values")

        # Check 5: Timestamp format
        try:
            pd.to_datetime(df['timestamp'])
            checks['valid_timestamp'] = True
            print(f"‚úÖ Timestamp format is valid")
        except:
            checks['valid_timestamp'] = False
            print(f"‚ùå Invalid timestamp format")

        # Summary
        print(f"\n--- Validation Summary ---")
        passed = sum(checks.values())
        total = len(checks)
        print(f"Passed: {passed}/{total} checks")

        all_passed = all(checks.values())

        if all_passed:
            print("\n‚úÖ All validations passed! Data is ready for upload.")
        else:
            print("\n‚ùå Validation failed! Please fix errors before uploading.")

        return all_passed, checks

# Test
if __name__ == '__main__':
    validator = DataLakeValidator()

    # Test 1: Valid data
    print("--- Test 1: Valid Data ---")
    valid_data = {
        'timestamp': ['2025-01-01 00:00:00', '2025-01-01 00:05:00', '2025-01-01 00:10:00'],
        'sensor_id': ['S001', 'S001', 'S002'],
        'temperature': [25.5, 26.0, 24.8],
        'humidity': [60, 62, 58]
    }
    df_valid = pd.DataFrame(valid_data)
    validator.validate_sensor_data(df_valid)

    print("\n" + "="*60)

    # Test 2: Invalid data
    print("\n--- Test 2: Invalid Data ---")
    invalid_data = {
        'timestamp': ['2025-01-01 00:00:00', '2025-01-01 00:05:00', '2025-01-01 00:00:00'],
        'sensor_id': ['S001', 'S001', 'S001'],  # Duplicate
        'temperature': [25.5, 150.0, 24.8],     # Invalid temp
        'humidity': [60, None, 58]              # Null value
    }
    df_invalid = pd.DataFrame(invalid_data)
    validator.validate_sensor_data(df_invalid)
```

‡∏£‡∏±‡∏ô Script:

```bash
python data_validation.py
```

### ‚úÖ Checkpoint 4.3

‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤:
- [ ] ‡∏™‡∏£‡πâ‡∏≤‡∏á File Naming Utility ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à
- [ ] Generate Filename ‡∏ï‡∏≤‡∏° Convention ‡πÑ‡∏î‡πâ
- [ ] Parse ‡πÅ‡∏•‡∏∞ Validate Filename ‡πÑ‡∏î‡πâ
- [ ] Validate Data Quality ‡∏Å‡πà‡∏≠‡∏ô Upload ‡πÑ‡∏î‡πâ

---

## üèÜ Challenge Exercise (‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°)

### Challenge 1: Cloud Upload Simulator (Advanced)

‡∏™‡∏£‡πâ‡∏≤‡∏á Python Class ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏Å‡∏≤‡∏£ Upload ‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏õ‡∏¢‡∏±‡∏á Cloud Storage (‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏ó‡∏µ‡πà Local)

**Requirements:**
- ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Partitioned Upload
- ‡∏°‡∏µ Progress Indicator
- Validate ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô Upload ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
- Generate Metadata File

<details>
<summary>üí° Hint (‡∏Ñ‡∏•‡∏¥‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π)</summary>

```python
class CloudStorageSimulator:
    def __init__(self, base_path):
        self.base_path = Path(base_path)

    def upload_file(self, local_file, destination_key, validate=True):
        """
        Simulate file upload
        """
        # 1. Validate data if needed
        # 2. Create destination directory
        # 3. Copy file
        # 4. Generate metadata
        # 5. Return upload status
        pass

    def list_objects(self, prefix):
        """
        List objects with prefix
        """
        pass

    def download_file(self, key, local_path):
        """
        Simulate download
        """
        pass
```
</details>

### Challenge 2: Partition Lifecycle Manager

‡∏™‡∏£‡πâ‡∏≤‡∏á Script ‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Partition Lifecycle:
- ‡∏•‡∏ö Partitions ‡πÄ‡∏Å‡πà‡∏≤‡∏Å‡∏ß‡πà‡∏≤ N ‡∏ß‡∏±‡∏ô
- ‡∏¢‡πâ‡∏≤‡∏¢ Partitions ‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏õ Archive Zone
- Generate Report ‡πÅ‡∏™‡∏î‡∏á Storage Usage

### Challenge 3: Query Performance Benchmark

‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Query Performance ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á:
- Partitioned vs Non-partitioned Data
- Different Partition Strategies (by time vs by entity)
- Different File Formats (JSON vs Parquet)

---

## üìä Lab Summary

### ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

‚úÖ **Lab 4.1:** ‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö Time-Series Partitioning Strategy (year/month/day)
‚úÖ **Lab 4.2:** ‡∏™‡∏£‡πâ‡∏≤‡∏á Local Data Lake Simulator ‡πÅ‡∏•‡∏∞ Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‚úÖ **Lab 4.3:** ‡∏™‡∏£‡πâ‡∏≤‡∏á File Naming Convention ‡πÅ‡∏•‡∏∞ Data Validation

### Skills Acquired

| Skill | Confidence Level |
|-------|------------------|
| Partitioning Strategy Design | ‚≠ê‚≠ê‚≠ê |
| File Organization | ‚≠ê‚≠ê‚≠ê |
| Data Validation | ‚≠ê‚≠ê‚≠ê |
| Local Data Lake Simulation | ‚≠ê‚≠ê |

### Key Files Created

```
lab_module_4/
‚îú‚îÄ‚îÄ partitioning_design.md
‚îú‚îÄ‚îÄ calculate_partition_size.py
‚îú‚îÄ‚îÄ create_datalake_structure.py
‚îú‚îÄ‚îÄ generate_sensor_data.py
‚îú‚îÄ‚îÄ query_partitioned_data.py
‚îú‚îÄ‚îÄ file_naming_utils.py
‚îú‚îÄ‚îÄ data_validation.py
‚îî‚îÄ‚îÄ local_datalake/
    ‚îú‚îÄ‚îÄ raw/
    ‚îú‚îÄ‚îÄ processed/
    ‚îî‚îÄ‚îÄ curated/
```

---

## üéØ ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

‡∏Ñ‡∏∏‡∏ì‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Module 5 ‡πÅ‡∏•‡πâ‡∏ß!

**üëâ [Module 5: Airflow - Orchestration Basics](../../module-5/module-5.md)**

‡πÉ‡∏ô Module ‡∏ñ‡∏±‡∏î‡πÑ‡∏õ ‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ:
- Apache Airflow Architecture
- DAG (Directed Acyclic Graph)
- Tasks ‡πÅ‡∏•‡∏∞ Dependencies
- Airflow Installation

---

## üí° Real-World Application

‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ‡∏à‡∏£‡∏¥‡∏á Partitioning Strategy ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢:
- **‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢:** Query ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Partition ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ‡∏•‡∏î Data Scan
- **‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß:** Parallel Processing ‡∏´‡∏•‡∏≤‡∏¢ Partitions ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô
- **‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£:** ‡∏•‡∏ö/Archive ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢
- **Compliance:** ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏≤‡∏° Policy (‡πÄ‡∏ä‡πà‡∏ô GDPR: ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á 90 ‡∏ß‡∏±‡∏ô)

---

## üìû Need Help?

‡∏´‡∏≤‡∏Å‡∏ï‡∏¥‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤:
1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Folder Structure ‡∏ß‡πà‡∏≤‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏° Convention
2. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö File Naming ‡∏ß‡πà‡∏≤‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Pattern
3. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Data Validation ‡∏ß‡πà‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏ó‡∏∏‡∏Å Checks

---

**[‚¨ÖÔ∏è ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ Module 4](../module-4.md)** | **[üè† ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡∏´‡∏ô‡πâ‡∏≤‡∏´‡∏•‡∏±‡∏Å](../../wiki.md)**
